\chapter{Experimental Results}
\label{ch:results}

This chapter presents experimental results addressing the three research questions established in Chapter~\ref{ch:introduction}.
We begin by validating training convergence across diverse programming languages (RQ1), then examine scaffold adaptation (RQ2) and SWE-Bench-Verified performance (RQ3), concluding with a discussion of the observed patterns and their implications.

\section{RQ1: Training Convergence Across Languages}
\label{sec:training-convergence}

\textbf{Research Question:}
Does execution-free \ac{GSPO} training converge across diverse programming languages?

\textbf{Approach:}
We monitor training dynamics over 460 gradient steps spanning 2 epochs of the 1{,}000-task curriculum (750 Python + 250 multilingual tasks across nine languages).
Overall convergence is assessed through mean and standard deviation of patch-similarity rewards computed after each episode, with reward progression indicating learning and sustained variance ensuring strong gradient signal.
Language-level convergence is validated by comparing per-language rewards on identical problem instances between epoch 1 (first exposure) and epoch 2 (second exposure).
For group-relative methods like \ac{GSPO}, sustained variance is critical: when all responses receive similar rewards, advantages collapse toward zero and eliminate the policy gradient.

\subsection{Overall Training Dynamics}

Training proceeded for a little over 2 epochs, completing in 2 days of wall-clock time on 3 A100 \acp{GPU} (144 total \ac{GPU}-hours), with \Cref{fig:reward-progression} demonstrating sustained learning throughout.
Mean patch-similarity rewards approximately doubled from 0.05 to peak values exceeding 0.10.
The continued upward trend validates that execution-free rewards provide sufficient signal for policy optimization in the agentic debugging setting.

Crucially, reward standard deviation shows steady increase from 0.02 to 0.07 rather than collapsing toward zero.
In group-relative optimization methods such as \ac{GSPO}, advantages are computed by comparing each response's reward against within-group statistics that approximate the value function.
When all responses receive similar rewards, this approximation becomes degenerate: uniform outcomes provide no signal for distinguishing better from worse actions, causing advantages to approach zero and eliminating the policy gradient.
Moreover, the variance itself enables learning the value function approximation—without diversity in outcomes, the baseline cannot meaningfully estimate expected returns.
The sustained and growing variance indicates the policy generates diverse episode outcomes with meaningfully different rewards, maintaining strong policy gradient signal throughout training.
This stands in contrast to variance collapse, a common failure mode where the policy converges prematurely to a narrow strategy that produces uniform outcomes.

The transient reward decline around step 200 reflects a characteristic challenge in online reinforcement learning.
As detailed in \cref{sec:rq2-harness}, the tool usage patterns that proved effective through step 150 subsequently degraded performance, and it took considerable time for alternative patterns to reemerge in the policy distribution.
These alternative patterns ultimately proved superior, yielding the subsequent reward improvements.
Similar transitions occur throughout training as the policy sifts through different behavioral patterns.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\textwidth]{plotting/figures/plots/temporal/reward_over_time_8dc73bp4.png}
	\caption{Mean patch-similarity reward (left) and standard deviation (right) throughout training, plotted with exponential moving average smoothing (\(\alpha = 0.01\)). Sustained variance growth indicates the policy maintains strong gradient signal by generating diverse episode outcomes, avoiding variance collapse.}
	\label{fig:reward-progression}
\end{figure}

\subsection{Language-Level Convergence}

The overall convergence demonstrated above aggregates performance across all languages in the training curriculum.
To validate that execution-free rewards enable effective learning across diverse programming ecosystems, \Cref{fig:language-reward-epochs} presents per-language reward progression from epoch 1 to epoch 2.
All nine languages demonstrate measurable learning progression despite substantial differences in syntax, semantics, and runtime characteristics, including systems languages (C, Rust), dynamically-typed languages (Python, Ruby, JavaScript, PHP), and statically-typed languages (Java, Go, TypeScript).

Despite substantial variation in absolute reward magnitudes (likely reflecting language verbosity differences and task difficulty), every language shows epoch-over-epoch improvement.
This validates that patch-similarity rewards provide effective learning signals across diverse syntax and semantics without language-specific adaptations, confirming that execution-free training supports unified policy optimization across heterogeneous language ecosystems.

\begin{figure}[httb]
	\centering
	\includegraphics[width=0.85\textwidth]{plotting/figures/plots/analysis/language_reward_epochs_n1000_8dc73bp4.png}
	\caption{Per-language reward progression from epoch 1 to epoch 2 on the same problem instances, demonstrating convergence across nine programming languages.}
	\label{fig:language-reward-epochs}
\end{figure}

\section{RQ2: Nano Agent Scaffold Adaptation}
\label{sec:rq2-harness}

\textbf{Research Question:}
How does \ac{GSPO} training improve Nano agent scaffold adaptation?

\textbf{Approach:}
We analyze operational metrics extracted from training episode logs: tool execution success rates (proportion of syntactically valid tool calls that execute without errors), episode length (number of tool calls per episode), and command usage patterns (frequency distribution of specific shell commands over time).
All metrics are computed over the training trajectory and visualized with exponential moving average smoothing ($\alpha = 0.05$) to reveal trends.
These operational measures characterize how the policy's scaffold interaction behavior evolves during training, complementing the reward progression analysis in \cref{sec:training-convergence}.

\subsection{Episode Length}
\label{subsec:episode-length}

\Cref{fig:episode-length} tracks the number of tool calls per episode throughout training.
Initially, the policy consistently exhausts the maximum episode budget of 30 tool calls, frequently exhibiting ``death spiral'' behavior where it repeatedly invokes the same failing tool without adapting its strategy.
The reward decline around step 200 discussed in \cref{sec:training-convergence} coincides with a spike in apply\_patch usage visible in \cref{fig:command-evolution}.

As training progresses, the policy ceases to exhaust the tool call budget and episode lengths trend consistently downward.
This indicates that the policy learns both to execute successful debugging workflows more efficiently and to terminate episodes when productive actions are exhausted rather than persisting with unproductive patterns.

\begin{figure}[httb]
	\centering
	\includegraphics[width=0.85\textwidth]{plotting/figures/plots/temporal/tool_calls_over_time_ema0.05_8dc73bp4.png}
	\caption{Number of tool calls per episode throughout training, plotted with exponential moving average smoothing (\(\alpha = 0.05\)).}
	\label{fig:episode-length}
\end{figure}

\subsection{Tool Execution Success Rates}
\label{subsec:tool-success}

Tool execution success measures whether the policy generates syntactically valid \ac{JSON} tool calls that execute without errors.
As noted in \cref{subsec:episode-length}, episodes shorten substantially over training, which could artificially inflate success rates if the policy simply avoids aforementioned death-spirals.
However, success rate improvements occur throughout training as rewards increase, including during the early phase when episodes consistently exhaust the 30-call budget, indicating that the policy learns to generate more valid tool calls rather than merely becoming more selective about which tools to invoke.

\Cref{fig:tool-success} shows tool execution success rates for both tools throughout training.
The shell command success rates improve from approximately 45\% early in training to 80\% at the end, though the peak success rate coincides with episodes averaging only 5 tool calls.
The apply\_patch success rates exhibit greater volatility, but still trend upwards.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.85\textwidth]{plotting/figures/plots/temporal/tool_success_rates_ema0.05_8dc73bp4.png}
	\caption{Tool execution success rates throughout training, computed as the mean proportion of successful invocations per tool type across episodes. Plotted with exponential moving average smoothing (\(\alpha = 0.05\)).}
	\label{fig:tool-success}
\end{figure}

\subsection{Command Usage Evolution}
\label{subsec:command-evolution}

While \cref{subsec:episode-length} examined overall shell command and apply\_patch counts, \cref{fig:command-evolution} decomposes shell command usage into the most frequently invoked individual commands to reveal strategic shifts in debugging behavior.
Each command's usage is expressed as a percentage of total tool calls at that training step, allowing us to broadly observe the patterns the policy favors as a function of time.

\begin{figure}[httb]
	\centering
	\includegraphics[width=0.85\textwidth]{plotting/figures/plots/temporal/command_trend_direct_ema0.05_8dc73bp4.png}
	\caption{Evolution of command usage frequencies throughout training, plotted with exponential moving average smoothing (\(\alpha = 0.05\)).}
	\label{fig:command-evolution}
\end{figure}

\section{RQ3: SWE-Bench-Verified Performance}
\label{sec:rq3-swebench}

\textbf{Research Question:}
Does execution-free patch-similarity \ac{RL} training improve SWE-Bench-Verified performance?

\textbf{Approach:}
We evaluate on the full 500-instance SWE-Bench-Verified benchmark using identical evaluation infrastructure (Nano scaffold, inference settings, episode limits) for both baseline and post-training checkpoints.
The baseline is Qwen3-14B operating through the Nano scaffold before any \ac{RL} training.
Primary metrics are completion rate (episodes producing non-empty repository modifications) and resolve rate (test-verified functional correctness).
Both evaluations use deterministic generation to isolate policy learning effects from sampling variance.

\Cref{tab:swebench-verified-results} compares SWE-Bench-Verified performance before and after \ac{GSPO} training.
The baseline model, evaluated using the identical Nano agent before \ac{RL} training, demonstrates substantial difficulties with fundamental agent operation.
Although all 500 instances receive submission attempts, only 186 episodes (37.2\%) produce non-empty patches, while 313 episodes (62.6\%) fail to modify the repository.
This high empty-patch rate reveals that the pre-trained model struggles to execute valid debugging workflows within episode constraints, frequently exhausting token budgets or tool-call limits without applying repository changes.
Among the 186 episodes that produce patches, 36 instances (7.2\% of total) pass all associated test cases.

After \ac{GSPO} training at step 460, episodes producing patches more than double to 388 instances (77.6\%), reducing empty-patch failures by 64\%.
This demonstrates substantial improvement in scaffold adaptation: the trained model reliably executes multi-turn debugging episodes, navigates repositories, and applies patches within operational constraints.

\emph{However}, test-verified success decreases from 36 to 31 resolved instances (7.2\% → 6.2\%).
Given the small absolute numbers and inherent evaluation variance, this difference likely falls within error margins and may reflect statistical noise rather than meaningful degradation.
Nonetheless, the results clearly indicate that training has not yet improved functional correctness despite clear gains in operational reliability.

\begin{table}[H]
\centering
\caption{SWE-Bench-Verified performance comparing pre-training baseline and post-\ac{GSPO} training on 500 instances. The baseline model is Qwen3-14B with the Nano agent before \ac{RL} training. All metrics are presented such that higher values indicate better performance.}
\label{tab:swebench-verified-results}
\begin{tabular}{lcc}
\toprule
\textbf{Metric} & \textbf{Baseline} & \textbf{Ours (step 460)} \\
\midrule
Completed instances & 186 & \textbf{388} \\
Completion rate & 37.2\% & \textbf{77.6\%} \\
Resolved instances & \textbf{36} & 31 \\
Resolve rate & 7.2\% & 6.2\% \\
\bottomrule
\end{tabular}
\end{table}

While test-verified success remains flat, mean patch-similarity rewards on these same SWE-Bench-Verified submissions increase substantially.
\Cref{tab:swebench-rewards} reports performance on the training objective itself: textual similarity between generated patches and ground-truth modifications.

\begin{table}[H]
\centering
\caption{Mean patch-similarity rewards on SWE-Bench-Verified instances.}
\label{tab:swebench-rewards}
\begin{tabular}{lcc}
\toprule
\textbf{Metric} & \textbf{Baseline} & \textbf{Ours (step 460)} \\
\midrule
Mean reward & 0.122 & \textbf{0.189} \\
\bottomrule
\end{tabular}
\end{table}

The 54\% reward improvement demonstrates that the policy is learning according to the training objective (patch similarity), even though this learning has not yet translated to improved test-verified success rates.

\section{Discussion}
\label{sec:discussion}

Training demonstrates clear convergence: rewards double, variance sustains, operational metrics improve substantially, and learning generalizes across nine programming languages.
Yet test-verified success on SWE-Bench-Verified remains flat at approximately 6--7\%.

One plausible explanation involves the initial reward distribution, where 313 of 500 instances (62.6\%) produce empty patches.
This creates a natural learning progression: escaping the zero-reward regime offers abundant signal where any valid patch constitutes measurable progress, while refining patch quality toward higher similarity scores becomes feasible only after the policy reliably produces non-zero outcomes.
Step 460's trajectory aligns with this interpretation.
Completion rates increase from 37\% to 78\%, tool success improves from 45\% to 80\%, and the 54\% reward gain demonstrates operational competence, but this competence has not yet translated to test-verified improvements.
The transient reward decline at step 200, coinciding with increased apply\_patch usage, may reflect temporary overweighting of patch production at the expense of quality.

Extended training may bridge this gap, particularly given that similar methodologies (DeepSWE~\cite{deepSWE2025}) achieve strong results after substantially longer training with greater resources.
Alternative explanations remain possible, including fundamental limitations of patch-similarity rewards for functional correctness, though continued upward reward trends suggest the policy has not exhausted learning from the current objective.

The results validate core feasibility: execution-free \ac{RL} training converges across diverse programming languages and drives measurable scaffold adaptation.
The gap between operational competence and functional success highlights open questions about training duration, reward design, and the relationship between textual similarity and semantic correctness.
