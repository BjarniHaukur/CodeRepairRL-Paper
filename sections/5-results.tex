\chapter{Experimental Results}
\label{ch:results}

This chapter presents experimental results addressing the four research questions established in Chapter~\ref{ch:introduction}.
We begin by validating training convergence (RQ0), then examine harness adaptation (RQ1), SWE-Bench-Verified performance (RQ2), and multilingual generalization (RQ3).

\section{RQ0: Does \ac{GSPO} training converge effectively with execution-free rewards?}
\label{sec:training-convergence}

\textbf{Research Question:}
Does \ac{GSPO} training converge effectively with execution-free rewards?

Training proceeded for approximately 2 epochs over the 1{,}000-task curriculum, completing in 2 days of wall-clock time on 3 A100 \acp{GPU} (144 total \ac{GPU}-hours), with \Cref{fig:reward-progression} demonstrating sustained learning throughout.
Mean patch-similarity rewards approximately doubled from 0.05 to peak values exceeding 0.10.
The continued upward trend validates that execution-free rewards provide sufficient signal for policy optimization in the agentic debugging setting.

Crucially, reward standard deviation shows steady increase from 0.02 to 0.07 rather than collapsing toward zero.
In group-relative optimization methods such as \ac{GSPO}, advantages are computed by comparing each response's reward against within-group statistics that approximate the value function.
When all responses receive similar rewards, this approximation becomes degenerate: uniform outcomes provide no signal for distinguishing better from worse actions, causing advantages to approach zero and eliminating the policy gradient.
Moreover, the variance itself enables learning the value function approximation—without diversity in outcomes, the baseline cannot meaningfully estimate expected returns.
The sustained and growing variance indicates the policy generates diverse episode outcomes with meaningfully different rewards, maintaining strong policy gradient signal throughout training.
This stands in contrast to variance collapse, a common failure mode where the policy converges prematurely to a narrow strategy that produces uniform outcomes.

The transient reward decline around step 200 reflects a characteristic challenge in online reinforcement learning.
As detailed in \cref{sec:rq1-harness}, the tool usage patterns that proved effective through step 150 subsequently degraded performance, and it took considerable time for alternative patterns to reemerge in the policy distribution.
These alternative patterns ultimately proved superior, yielding the subsequent reward improvements.
Similar transitions occur throughout training as the policy sifts through different behavioral patterns.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\textwidth]{plotting/figures/plots/temporal/reward_over_time_8dc73bp4.png}
	\caption{Mean patch-similarity reward (left) and standard deviation (right) throughout training, plotted with exponential moving average smoothing (\(\alpha = 0.01\)). Sustained variance growth indicates the policy maintains strong gradient signal by generating diverse episode outcomes, avoiding variance collapse.}
	\label{fig:reward-progression}
\end{figure}

While this exploration maintains healthy gradient signal, extended training would likely require curriculum mixing with non-agentic or meaningfullly different tasks to preserve broader model capabilities and prevent catastrophic forgetting.

\section{RQ1: Nano Harness Adaptation}
\label{sec:rq1-harness}

\textbf{Research Question:}
How does \ac{GSPO} training improve Nano harness adaptation?

Effective harness adaptation requires the policy to generate valid tool calls, strategically appropriate commands, and applying syntactically correct changes to files.
We measure tool execution success rates, number of tool calls, and command usage patterns to characterize how the policy's interaction behavior evolves during training.
These trends offer insight into the learning that is occurring, as evidenced by concurrent reward progression shown in \cref{sec:training-convergence}.

\subsection{Episode Length}
\label{subsec:episode-length}

\Cref{fig:episode-length} tracks the number of tool calls per episode throughout training.
Initially, the policy consistently exhausts the maximum episode budget of 30 tool calls, frequently exhibiting ``death spiral'' behavior where it repeatedly invokes the same failing tool without adapting its strategy.
The reward decline around step 200 discussed in \cref{sec:training-convergence} coincides with a spike in apply\_patch usage visible in \cref{fig:command-evolution}.
One plausible explanation is that the policy adopted a strategy of broadly affecting the repository to attain tail rewards, but this approach prooved harmful to performance was eventually superseded after many gradient updates.

As training progresses, the policy ceases to exhaust the tool call budget and episode lengths trend consistently downward.
This indicates that the policy learns both to execute successful debugging workflows more efficiently and to terminate episodes when productive actions are exhausted rather than persisting with unproductive patterns.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\textwidth]{plotting/figures/plots/temporal/tool_calls_over_time_ema0.05_8dc73bp4.png}
	\caption{Number of tool calls per episode throughout training, plotted with exponential moving average smoothing (\(\alpha = 0.05\)).}
	\label{fig:episode-length}
\end{figure}

\subsection{Tool Execution Success Rates}
\label{subsec:tool-success}

Tool execution success measures whether the policy generates syntactically valid \ac{JSON} tool calls that execute without errors.
As noted in \cref{subsec:episode-length}, episodes shorten substantially over training, which could artificially inflate success rates if the policy simply avoids aforementioned death-spirals.
However, success rate improvements occur throughout training as rewards increase, including during the early phase when episodes consistently exhaust the 30-call budget, indicating that the policy learns to generate more valid tool calls rather than merely becoming more selective about which tools to invoke.

\Cref{fig:tool-success} shows tool execution success rates for both tools throughout training.
The shell command success rates improve from approximately 45\% early in training to 80\% at the end, though the peak success rate coincides with episodes averaging only 5 tool calls.
The apply\_patch success rates exhibit greater volatility, but still trend upwards.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\textwidth]{plotting/figures/plots/temporal/tool_success_rates_ema0.05_8dc73bp4.png}
	\caption{Tool execution success rates throughout training, computed as the mean proportion of successful invocations per tool type across episodes. Plotted with exponential moving average smoothing (\(\alpha = 0.05\)).}
	\label{fig:tool-success}
\end{figure}

\subsection{Command Usage Evolution}
\label{subsec:command-evolution}

While \cref{subsec:episode-length} examined overall shell command and apply\_patch counts, \cref{fig:command-evolution} decomposes shell command usage into the most frequently invoked individual commands to reveal strategic shifts in debugging behavior.
Each command's usage is expressed as a percentage of total tool calls at that training step, allowing us to broadly observe the patterns the policy favors as a function of time.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\textwidth]{plotting/figures/plots/temporal/command_trend_direct_ema0.05_8dc73bp4.png}
	\caption{Evolution of command usage frequencies throughout training, plotted with exponential moving average smoothing (\(\alpha = 0.05\)).}
	\label{fig:command-evolution}
\end{figure}

\section{RQ2: SWE-Bench-Verified Performance}
\label{sec:rq2-swebench}

\textbf{Research Question:}
Does execution-free patch-similarity \ac{RL} training improve SWE-Bench-Verified performance?

We compare pre-training Qwen3-14B baseline performance with post-\ac{GSPO} training results on SWE-Bench-Verified.

\Cref{tab:swebench-verified-results} compares SWE-Bench-Verified performance before and after \ac{GSPO} training.
The baseline model, evaluated using the identical Nano harness before \ac{RL} training, demonstrates substantial difficulties with fundamental agent operation.
Although all 500 instances receive submission attempts, only 186 episodes (37.2\%) produce non-empty patches, while 313 episodes (62.6\%) fail to modify the repository.
This high empty-patch rate reveals that the pre-trained model struggles to execute valid debugging workflows within episode constraints, frequently exhausting token budgets or tool-call limits without applying repository changes.
Among the 186 episodes that produce patches, 36 instances (7.2\% of total) pass all associated test cases.

After \ac{GSPO} training at step 460, episodes producing patches more than double to 388 instances (77.6\%), reducing empty-patch failures by 64\%.
This demonstrates substantial improvement in harness adaptation: the trained model reliably executes multi-turn debugging episodes, navigates repositories, and applies patches within operational constraints.

\emph{However}, test-verified success decreases from 36 to 31 resolved instances (7.2\% → 6.2\%).
Given the small absolute numbers and inherent evaluation variance, this difference likely falls within error margins and may reflect statistical noise rather than meaningful degradation.
Nonetheless, the results clearly indicate that training has not yet improved functional correctness despite clear gains in operational reliability.

\begin{table}[H]
\centering
\caption{SWE-Bench-Verified performance comparing pre-training baseline and post-\ac{GSPO} training. The baseline model is Qwen3-14B with the Nano agent before \ac{RL} training. Metrics represent test-verified outcomes on the full 500-instance benchmark.}
\label{tab:swebench-verified-results}
\begin{tabular}{lcc}
\toprule
\textbf{Metric} & \textbf{Baseline} & \textbf{Ours (step 460)} \\
\midrule
Total instances & 500 & 500 \\
Submitted instances & 500 & 500 \\
Completed instances & 186 & \textbf{388} \\
Resolved instances & \textbf{36} & 31 \\
Unresolved instances & 150 & 357 \\
Empty patch instances & 313 & 111 \\
Error instances & 0 & 1 \\
\midrule
Resolve rate & 7.2\% & 6.2\% \\
\bottomrule
\end{tabular}
\end{table}

While test-verified success remains flat, mean patch-similarity rewards on these same SWE-Bench-Verified submissions increase substantially.
\Cref{tab:swebench-rewards} reports performance on the training objective itself: textual similarity between generated patches and ground-truth modifications.

\begin{table}[H]
\centering
\caption{Mean patch-similarity rewards on SWE-Bench-Verified instances.}
\label{tab:swebench-rewards}
\begin{tabular}{lcc}
\toprule
\textbf{Metric} & \textbf{Baseline} & \textbf{Ours (step 460)} \\
\midrule
Mean reward & 0.122 & \textbf{0.189} \\
\bottomrule
\end{tabular}
\end{table}

The 54\% reward improvement provides crucial context for interpreting the unchanged resolve rate.
The baseline results characterize the reward distribution at the start of training: heavily concentrated near zero, with 313 instances producing empty patches (zero reward by definition) and many of the 150 unresolved instances generating patches which might bear little resemblance to ground truth.
This initial distribution creates a natural learning hierarchy.
When most trajectories yield near-zero rewards, the highest-impact optimization target involves escaping this low-reward regime entirely—learning to navigate repositories systematically, avoid unproductive tool-use patterns, and achieve the small tail rewards that distinguish \textit{any} progress from complete failure.
Only after the policy reliably produces non-zero outcomes does gradient-based optimization gain sufficient signal to hill-climb toward higher-quality patches.

The observed training trajectory aligns with this hierarchical structure.
Substantial gains in operational metrics documented in \cref{sec:rq1-harness}—completion rates, tool success, and strategic command usage—alongside the 54\% reward increase suggest that step 460 represents successful acquisition of the first capability tier: consistent achievement of non-zero rewards through competent harness interaction.
Extended training may be necessary to translate these operational and textual improvements into functional correctness.

\section{RQ3: Multilingual Performance Improvements}
\label{sec:rq3-multilingual}

\textbf{Research Question:}
Does execution-free \ac{RL} improve performance across programming languages without language-specific engineering?

The mixed 1{,}000-task curriculum described in \cref{sec:data-env} combines 750 Python tasks with 250 multilingual tasks spanning nine programming languages.
To evaluate whether execution-free patch-similarity rewards enable unified training across diverse languages, we compare per-language rewards on the same problem instances between epoch 1 (first exposure) and epoch 2 (second exposure).
This within-task comparison directly measures learning progression across languages while controlling for task difficulty variation.

\Cref{fig:language-reward-epochs} presents per-language reward improvements across the two training epochs.
Despite the uneven data distribution, all languages demonstrate measurable gains, validating that execution-free rewards provide effective learning signals across diverse language ecosystems without language-specific adaptations.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\textwidth]{plotting/figures/plots/analysis/language_reward_epochs_n1000_8dc73bp4.png}
	\caption{Per-language reward progression from epoch 1 to epoch 2 on the same problem instances.}
	\label{fig:language-reward-epochs}
\end{figure}

The absolute reward magnitudes vary substantially across languages.
Java, Ruby, and PHP achieve the highest rewards (0.12--0.18 in epoch 2), while Rust and TypeScript remain lower (0.06--0.08) despite clear learning progression.
These differences likely reflect multiple confounding factors.
Language verbosity introduces a systematic bias where verbose and strict syntax causes any two Java patches to share more boilerplate and structural similarity by default, inflating similarity scores.
Concise languages like Rust and TypeScript generate shorter patches with less redundant structure, potentially yielding lower similarity scores even when semantically equivalent.
Task difficulty variation across languages may also contribute—Rust's memory safety constraints and TypeScript's type system complexities could produce intrinsically harder debugging scenarios—though disentangling difficulty from verbosity effects would require controlled investigation beyond the scope of this work.

The consistent improvements across languages with diverse syntax, semantics, and programming paradigms—from low-level systems languages (C, Rust) to dynamic scripting languages (Python, Ruby, JavaScript) to statically-typed enterprise languages (Java, TypeScript)—demonstrate that patch-similarity rewards provide language-agnostic learning signals.
This validates the core premise of execution-free training: by decoupling reward computation from language-specific test infrastructure, a single methodology trains effectively across heterogeneous language ecosystems.
