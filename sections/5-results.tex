\chapter{Experimental Results and Analysis}
\label{ch:results}

This chapter presents experimental results addressing our three research questions (RQ1/RQ2/RQ3) about online \ac{RL} for automated code repair.
\todoinline{Insert comprehensive experimental results demonstrating improvements over baseline approaches and evaluation of learned debugging capabilities generalization.}

\section{Training Dynamics and Convergence}
\label{sec:training-dynamics}

\subsection{Learning Curve Analysis}
\label{subsec:learning-curve}

Our \ac{GSPO}~\cite{gspo2025} training demonstrates clear and consistent learning dynamics across multiple experimental runs.
\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.9\textwidth]{plotting/figures/plots/temporal/training_loss_ema0.05_tfk08zx2.png}
	\caption{Training loss evolution over time showing convergence dynamics for run tfk08zx2 with exponential moving average smoothing ($\alpha=0.05$).}
	\label{fig:training-loss}
\end{figure}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.9\textwidth]{plotting/figures/plots/temporal/reward_components_ema0.05_tfk08zx2.png}
	\caption{Evolution of reward components throughout training, showing the progression of different reward signals over training steps.}
	\label{fig:reward-components}
\end{figure}

\subsubsection{Reward Progression}

\todoinline{Describe training progression from initial near-zero success rates through rapid improvement phases.
Include analysis of: \begin{itemize}
\item Initial Phase: Tool usage pattern acquisition
\item Intermediate Phase: Strategy development and generalization
\item Convergence Phase: Refinement and breakthrough improvements
\end{itemize} Validate hypothesis about \ac{RL} enhancement of coding capabilities.
}

\subsubsection{Training Stability}

% \begin{figure}[htbp]
% 	\centering
% 	\includegraphics[width=1.0\textwidth]{plotting/figures/plots/comparison/training_dynamics_c8qr1evt_vs_tfk08zx2.png}
% \caption{Comprehensive comparison of training dynamics between \ac{GSPO} (c8qr1evt) and \ac{GRPO} (tfk08zx2) approaches. The comparison shows training loss, mean reward, \ac{KL} divergence, and learning progress across training steps. \ac{GRPO} demonstrates significantly more stable training with faster convergence and better final performance.}
% 	\label{fig:training-comparison}
% \end{figure}

\todoinline{Analyze \ac{GSPO} training stability and show failure cases of our \ac{GRPO} runs?}

\subsection{Computational Efficiency}
\label{subsec:computational-efficiency}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.9\textwidth]{plotting/figures/plots/temporal/kl_divergence_ema0.05_tfk08zx2.png}
\caption{\ac{KL} divergence between policy and reference model throughout training, showing the degree of policy shift during online learning.}
	\label{fig:kl-divergence}
\end{figure}

\todoinline{Report computational efficiency improvements from training-inference duality architecture:
\begin{itemize}
\item Wall-clock training time reduction vs traditional approaches
\item \acp{GPU} utilization metrics across training and inference hardware
\item Sample efficiency improvements in episodes to target performance
\end{itemize}
Analyze \ac{NCCL}-based weight synchronization overhead and contribution to efficiency gains.}

\section{Main Results: \ac{RL} vs.
  Baselines} \label{sec:main-results}

\subsection{SWE-Bench-Verified Performance} \label{subsec:swe-bench-performance}

Table~\cref{tab:main-results} presents our primary experimental results on SWE-Bench-Verified, comparing online \ac{RL} training against multiple baseline approaches.

\todoinline{Add table here}
\subsubsection{Qualitative Improvement Analysis}

Beyond quantitative metrics, agent-trained models demonstrate qualitatively different debugging behaviors:

\textbf{Strategic Exploration}: \ac{RL}-trained agents develop systematic repository exploration strategies, typically examining project structure, documentation, and related test files before attempting fixes.

\textbf{Context Awareness}: Agents learn to gather sufficient context about bug locations, including understanding function signatures, variable scopes, and dependency relationships.

\textbf{Iterative Refinement}: Unlike single-shot generation approaches, agents can discover and correct initial mistakes through multi-step interaction patterns.

\textbf{Tool Usage Efficiency}: Trained agents develop efficient command usage patterns, avoiding redundant operations and focusing on information-gathering commands that maximize debugging insight.

\section{RQ1: Harness Adaptation Across Model Sizes}
\label{sec:rq1-harness}

\textbf{Research Question 1}: How effectively does online \ac{RL} training enable models to adapt to the Nano harness environment, and what specific improvements occur in tool usage patterns?

\subsection{Methodology}
\label{subsec:rq1-methodology}
We analyze harness adaptation through three key metrics measured throughout training: \begin{itemize}
	\item \textbf{Tool success rate}: fraction of tool calls that execute without schema/validation errors
	\item \textbf{Invalid call rate}: fraction of calls rejected by the harness (schema mismatch, unsafe commands)
	\item \textbf{Action efficiency}: commands per successful episode; files viewed per solved task
\end{itemize}

\subsection{Results} \label{subsec:rq1-results}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.9\textwidth]{plotting/figures/plots/temporal/tool_success_rates_ema0.05_tfk08zx2.png}
	\caption{Tool success rates over training steps, showing the model's improving ability to generate valid tool calls that execute successfully.}
	\label{fig:tool-success}
\end{figure}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.9\textwidth]{plotting/figures/plots/temporal/command_trend_direct_ema0.05_tfk08zx2.png}
	\caption{Evolution of command usage patterns throughout training, demonstrating shifts in tool usage strategies.}
	\label{fig:command-trend}
\end{figure}

\todoinline{Insert comprehensive analysis including: \begin{itemize}
\item Quantitative metrics from training logs
\item Statistical significance of improvements
\item Comparison with baseline models
\item Actions-per-episode distribution analysis
\end{itemize}}

\subsection{Key Findings} \label{subsec:rq1-findings} \todoinline{Summarize key findings regarding harness adaptation effectiveness, timeline of improvements, and implications for \ac{RL} training success.
}

\subsection{Model Size Comparison}
\label{subsec:rq1-model-size}
\textbf{Model size analysis}: We compare pre/post GSPO+KL performance across Qwen3-8B/14B/30B and the Llama3.1-8B reference within the same experimental conditions.

\subsubsection{Methodology}
\label{subsubsec:rq1b-methodology}
We evaluate training recipe transferability by applying identical training procedures to multiple base models with consistent experimental conditions: \begin{itemize}
	\item Same training budgets and episode counts
	\item Identical prompts and evaluation protocols
	\item Consistent hyperparameter scaling approaches
	\item Uniform tool success metric collection
\end{itemize}

\subsubsection{Results} \label{subsubsec:rq1b-results} \todoinline{Insert comprehensive comparison table including: \begin{itemize}
\item Performance on SWE-Bench-Verified across different base models
\item Tool success metrics for each model
\item Training convergence characteristics
\item Computational efficiency comparisons
\item Statistical significance of cross-model consistency
\end{itemize}}

\subsubsection{Analysis} \label{subsubsec:rq1b-analysis} \todoinline{Analyze training recipe portability, identify model-specific adaptations required, and discuss implications for scaling to new architectures.
}

\section{RQ2: Multilingual Holdout Evaluation} \label{sec:rq2-multilingual}

\textbf{Research Question 2}: Does the 750/250 curriculum improve performance on a held-out SWE-Bench-Multilingual set without bespoke scaffolding?\todoinline{Confirm holdout sample count and language coverage once runs finish.}

\subsection{Experimental Design}\label{subsec:rq2-design}
We measure multilingual transfer by evaluating GSPO-trained checkpoints on the reserved 50-task SWE-Bench-Multilingual holdout: \begin{itemize}
	\item Training curriculum: 750 SWE-Gym Python + 250 SWE-Bench-Multilingual tasks with holdout samples removed.\todoinline{Verify final task IDs per language.}
	\item Holdout evaluation: 50 multilingual bugs spanning the nine languages present in the curriculum.
	\item Baselines: pre-training checkpoints evaluated on the identical holdout split.
	\item Metrics: average reward per bug, per-language reward deltas, and bootstrap 95\% confidence intervals.
\end{itemize}

\subsection{Holdout Performance} \label{subsec:rq2-performance} \todoinline{Insert table/plot of multilingual holdout rewards before vs. after training, including per-language breakdown and confidence intervals.}

\subsection{Generalization Analysis} \label{subsec:rq2-analysis} \todoinline{Interpret multilingual holdout reward shifts, highlight languages with notable changes, and discuss remaining limitations.}

\section{RQ3: Scaffold Transfer Evaluation} \label{sec:rq3-scaffold}

\textbf{Research Question 3}: To what extent do behaviours learned on Nano transfer to alternative scaffolds without bespoke retraining?\todoinline{List final scaffold lineup (Mini-SWE-Agent, Aider, OpenHands) once evaluations conclude.}

\subsection{Evaluation Protocol}\label{subsec:rq3-protocol}
We adopt the scaffold-transfer protocol outlined in the accompanying conference draft:
\begin{itemize}
\item Train exclusively on Nano using the 1,000-task curriculum described in \S\ref{sec:nano-agent-new}.\todoinline{Verify curriculum parity between thesis and conference experiments.}
\item Evaluate checkpoints zero-shot on Mini-SWE-Agent-, Aider-, and OpenHands-style harnesses over matched SWE-Bench-Verified instances.
\item Record success rate, tool validity, and scaffold-adherence metrics (prompt compliance, command formatting) per harness.
\item Provide a five-demonstration primer per harness to gauge few-shot adaptation gains after zero-shot evaluation.\todoinline{Confirm demonstration format and token budgets.}
\end{itemize}


\subsection{Zero-Shot Transfer}\label{subsec:rq3-zeroshot}
\todoinline{Insert table comparing Nano performance with zero-shot transfer success rates, tool validity, and episode length across harnesses.}
Preliminary dry runs mirror the trends described in the ICLR submission: GSPO-trained checkpoints maintain non-trivial success on Mini-SWE-Agent despite schema differences, whereas untrained baselines collapse to near-zero performance.\todoinline{Replace with measured numbers and cite Appendix once finalized.}

\subsection{Few-Shot Adaptation}\label{subsec:rq3-fewshot}
We measure adaptation speed by supplying five annotated trajectories for each harness and evaluating improvement over ten tasks.\todoinline{Add learning-curve plot once experiments complete.} Early results suggest the GSPO policy adapts within two demonstrations, while the pretrained baseline requires substantially more guidance, supporting the hypothesis that Nano-trained routines persist under interface changes.\todoinline{Quantify adaptation delta (success + tool validity).}

\subsection{Behavioural Analysis}\label{subsec:rq3-behaviour}
\todoinline{Document qualitative differences in command sequencing, error recovery, and prompt adherence during transfer, referencing transcript samples from each scaffold.}


\section{Ablation Studies}
\label{sec:ablations}

\subsection{Component-wise Analysis}
\label{subsec:component-analysis}

To understand the contribution of different system components, we conducted systematic ablation studies:

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.9\textwidth]{plotting/figures/plots/problem_performance_distribution_tfk08zx2.png}
\caption{Distribution of problem-solving performance across different task categories, showing which types of problems benefit most from \ac{RL} training.}
	\label{fig:problem-distribution}
\end{figure}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.9\textwidth]{plotting/figures/plots/problem_clustering_tfk08zx2.png}
	\caption{Clustering analysis of problem types based on model performance patterns, revealing systematic strengths and weaknesses.}
	\label{fig:problem-clustering}
\end{figure}

\todoinline{Insert ablation study results table comparing system configurations: \begin{itemize}
\item Full system performance baseline
\item Impact of removing online updates (evaluation of frozen pre-training checkpoint)
\item Effect of agent scaffold removal
\item Multi-step interaction contribution
\item \ac{GSPO} vs. traditional \ac{PPO} comparison
\item Real-time updates impact analysis
\item Statistical significance of each component
\end{itemize}}

\subsubsection{Critical Component Identification}

The ablation results highlight several critical system components:

\todoinline{Analyze critical component contributions including: \begin{itemize}
\item Multi-step interaction impact and validation of experiential learning hypothesis
\item Agent scaffold integration benefits over direct generation
\item \ac{RL} training advantages over supervised fine-tuning
\item \ac{GSPO} improvements over traditional \ac{PPO} methods
\item Real-time updates contribution to sample efficiency
\item Ranking of component importance and implications
\end{itemize}}

\subsection{Hyperparameter Sensitivity} \label{subsec:hyperparameter}

We evaluated sensitivity to key hyperparameters to understand training robustness:

\subsubsection{Learning Rate Analysis}

\todoinline{Report learning rate sensitivity analysis including: \begin{itemize}
\item Tested learning rate ranges
\item Optimal values for different model sizes
\item Performance robustness across effective ranges
\item Model size-specific optimization requirements
\end{itemize}}

\subsubsection{Trajectory Length Impact}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.9\textwidth]{plotting/figures/plots/individual_trajectories_tfk08zx2.png}
	\caption{Individual trajectory analysis showing reward evolution patterns for successful and unsuccessful episodes during training.}
	\label{fig:trajectories}
\end{figure}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.9\textwidth]{plotting/figures/plots/reward_distribution_evolution_tfk08zx2.png}
	\caption{Evolution of reward distribution throughout training, demonstrating the shift from low-reward episodes early in training to higher-reward episodes as the model learns.}
	\label{fig:reward-evolution}
\end{figure}

\todoinline{Analyze trajectory length impact including: \begin{itemize}
\item Tested trajectory length ranges
\item Performance plateaus and optimal values
\item Context sufficiency for debugging tasks
\item Computational efficiency tradeoffs
\end{itemize}}

\section{Error Analysis and Failure Modes} \label{sec:error-analysis}

\subsection{Systematic Failure Analysis} \label{subsec:failure-analysis}

To understand current limitations and guide future improvements, we analyzed failed debugging attempts across multiple categories:

\todoinline{Insert failure mode analysis table categorizing failed tasks by: \begin{itemize}
\item Problem understanding errors and frequency
\item File identification mistakes
\item Correct location but wrong fix applications
\item Incomplete understanding issues
\item Technical tool usage errors
\item Context length limitation impacts
\item Statistical analysis of failure patterns
\end{itemize}}

\subsubsection{Problem Understanding Challenges}

\todoinline{Analyze most common failure modes and their implications for improvement strategies.
}
This suggests areas for improvement:

\begin{itemize}
	\item \textbf{Enhanced Issue Processing}: Better training on interpreting natural language issue descriptions and mapping them to technical requirements
	\item \textbf{Clarification Strategies}: Learning to ask clarifying questions or seek additional context when issue descriptions are ambiguous
	\item \textbf{Domain Knowledge Integration}: Incorporating more domain-specific knowledge about common bug patterns and software engineering practices
\end{itemize}

\subsubsection{File Localization Accuracy}

\todoinline{Discuss file identification challenges and potential improvements in repository understanding.
}

\begin{itemize}
	\item \textbf{Improved Search Strategies}: Learning more effective patterns for locating relevant code through grep and find operations
	\item \textbf{Dependency Analysis}: Better understanding of code dependencies and import relationships
	\item \textbf{Project Structure Learning}: Enhanced ability to navigate unfamiliar project architectures and coding conventions
\end{itemize}

\subsection{Success Pattern Analysis}
\label{subsec:success-patterns}

Conversely, analyzing successful debugging attempts reveals effective strategies:

\subsubsection{Successful Exploration Patterns}

Successful agents consistently follow effective exploration patterns:

\begin{enumerate}
	\item \textbf{Initial Reconnaissance}: Examine project structure, README files, and high-level organization
	\item \textbf{Issue Analysis}: Carefully parse issue descriptions and identify key terms for searching
	\item \textbf{Systematic Search}: Use grep and find strategically to locate relevant code sections
	\item \textbf{Context Gathering}: Examine related files, tests, and documentation before attempting fixes
	\item \textbf{Targeted Modification}: Apply precise, minimal changes that address the root cause
\end{enumerate}

\subsubsection{Tool Usage Efficiency}

Successful agents develop efficient tool usage patterns:

\begin{itemize}
	\item \textbf{Strategic Grep Usage}: Effective search terms and patterns that quickly locate relevant code
	\item \textbf{Minimal File Examination}: Focus on essential files rather than exhaustive exploration
	\item \textbf{Iterative Refinement}: Start with broad searches and progressively narrow focus
	\item \textbf{Error-Driven Learning}: Adapt strategies based on command outputs and error messages
\end{itemize}

\section{Computational Performance Analysis} \label{sec:performance-analysis}

\subsection{Training Efficiency Metrics} \label{subsec:training-efficiency}

Our infrastructure achievements enable practical large-scale agent training:

\todoinline{Insert training performance metrics table including: \begin{itemize}
\item Episodes per hour for different model configurations
\item Memory usage across model sizes
\item Update latency measurements
\item Training throughput analysis
\item \acp{GPU} utilization efficiency
\item Scalability characteristics
\end{itemize}}

\subsubsection{Scalability Achievements}

\todoinline{Analyze scalability characteristics including: \begin{itemize}
\item \acp{GPU} scaling linearity up to tested limits
\item Memory utilization efficiency achievements
\item Network bandwidth consumption for \ac{NCCL} updates
\item Fault tolerance and graceful degradation capabilities
\item Performance bottlenecks and limitations
\end{itemize}}

\subsection{Cost-Effectiveness Analysis} \label{subsec:cost-effectiveness}

\todoinline{Analyze computational cost-effectiveness including: \begin{itemize}
\item Total training costs for different model sizes
\item Per-episode cost comparisons with commercial \acp{API}
\item Development cost implications for academic research
\item Infrastructure maintenance requirements
\item Budget accessibility and democratization implications
\end{itemize}}

\todoinline{Table of A100 \ac{GPU} hours per run?
}

\section{Summary of Key Findings}
\label{sec:key-findings}

Our comprehensive experimental evaluation yields several important conclusions:

\subsection{Research Question Answers} \label{subsec:research-answers}

\textbf{RQ1 (Harness Adaptation)}: \todoinline{Summarize GSPO improvements in tool success, invalid calls, action efficiency, and success rates per model size using Sections~\ref{sec:rq1-harness} and \ref{subsec:rq1-model-size}.}

\textbf{RQ2 (Multilingual Holdout)}: \todoinline{Report average reward improvements per language with references to Section~\ref{sec:rq2-multilingual}.}

\textbf{RQ3 (Scaffold Transfer)}: \todoinline{Summarize zero-shot and few-shot transfer outcomes across alternative harnesses with references to Section~\ref{sec:rq3-scaffold}.}

\subsection{Broader Implications}
\label{subsec:broader-implications}

These results have several important implications for the field:

\begin{itemize}
	\item \textbf{Design Philosophy}: \todoinline{Discuss how the bitter lesson applies to training software engineering agents, comparing simple tools with extensive learning versus sophisticated engineering approaches.}
	\item \textbf{Future Potential}: \todoinline{Analyze evidence for continued improvement potential through longer training periods and larger models, based on observed training dynamics.}
\end{itemize}

These findings establish online reinforcement learning as a promising direction for advancing automated software engineering capabilities.
