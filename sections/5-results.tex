\chapter{Experimental Results}
\label{ch:results}

This chapter presents experimental results addressing the three research questions established in Chapter~\ref{ch:introduction}.
We begin by examining training convergence through reward progression, then address harness adaptation (RQ1), SWE-Bench-Verified performance (RQ2), and multilingual generalization (RQ3).

\section{Training Convergence}
\label{sec:training-convergence}

Training proceeded for approximately 2 epochs over the 1{,}000-task curriculum, completing in 2 days of wall-clock time on 3 A100 \acp{GPU} (144 total \ac{GPU}-hours), with \Cref{fig:reward-progression} demonstrating sustained learning throughout.
Mean patch-similarity rewards approximately doubled from 0.05 to peak values exceeding 0.10.
The continued upward trend validates that execution-free rewards provide sufficient signal for policy optimization in the agentic debugging setting.

Crucially, reward standard deviation shows steady increase from 0.02 to 0.07 rather than collapsing toward zero.
In group-relative optimization methods such as \ac{GSPO}, advantages are computed by comparing each response's reward against within-group statistics that approximate the value function.
When all responses receive similar rewards, this approximation becomes degenerate: uniform outcomes provide no signal for distinguishing better from worse actions, causing advantages to approach zero and eliminating the policy gradient.
Moreover, the variance itself enables learning the value function approximationâ€”without diversity in outcomes, the baseline cannot meaningfully estimate expected returns.
The sustained and growing variance indicates the policy generates diverse episode outcomes with meaningfully different rewards, maintaining strong policy gradient signal throughout training.
This stands in contrast to variance collapse, a common failure mode where the policy converges prematurely to a narrow strategy that produces uniform outcomes.

The transient reward decline around step 200 reflects a characteristic challenge in online reinforcement learning.
As detailed in \cref{sec:rq1-harness}, the tool usage patterns that proved effective through step 150 subsequently degraded performance, and it took considerable time for alternative patterns to reemerge in the policy distribution.
These alternative patterns ultimately proved superior, yielding the subsequent reward improvements.
Similar transitions occur throughout training as the policy sifts through different behavioral patterns.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\textwidth]{plotting/figures/plots/temporal/reward_over_time_8dc73bp4.png}
	\caption{Mean patch-similarity reward (left) and standard deviation (right) throughout training, plotted with exponential moving average smoothing (\(\alpha = 0.01\)). Sustained variance growth indicates the policy maintains strong gradient signal by generating diverse episode outcomes, avoiding variance collapse.}
	\label{fig:reward-progression}
\end{figure}

While this exploration maintains healthy gradient signal, extended training would likely require curriculum mixing with non-agentic or meaningfullly different tasks to preserve broader model capabilities and prevent catastrophic forgetting.

\section{RQ1: Nano Harness Adaptation}
\label{sec:rq1-harness}

\textbf{Research Question:}
How does \ac{GSPO} training improve Nano harness adaptation?

Effective harness adaptation requires the policy to generate valid tool calls, strategically appropriate commands, and applying syntactically correct changes to files.
We measure tool execution success rates, number of tool calls, and command usage patterns to characterize how the policy's interaction behavior evolves during training.
These trends offer insight into the learning that is occurring, as evidenced by concurrent reward progression shown in \cref{sec:training-convergence}.

\subsection{Episode Length}
\label{subsec:episode-length}

\Cref{fig:episode-length} tracks the number of tool calls per episode throughout training.
Initially, the policy consistently exhausts the maximum episode budget of 30 tool calls, frequently exhibiting ``death spiral'' behavior where it repeatedly invokes the same failing tool without adapting its strategy.
The reward decline around step 200 discussed in \cref{sec:training-convergence} coincides with a spike in apply\_patch usage visible in \cref{fig:command-evolution}.
One plausible explanation is that the policy adopted a strategy of broadly affecting the repository to attain tail rewards, but this approach prooved harmful to performance was eventually superseded after many gradient updates.

As training progresses, the policy ceases to exhaust the tool call budget and episode lengths trend consistently downward.
This indicates that the policy learns both to execute successful debugging workflows more efficiently and to terminate episodes when productive actions are exhausted rather than persisting with unproductive patterns.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\textwidth]{plotting/figures/plots/temporal/tool_calls_over_time_ema0.05_8dc73bp4.png}
	\caption{Number of tool calls per episode throughout training, plotted with exponential moving average smoothing (\(\alpha = 0.05\)).}
	\label{fig:episode-length}
\end{figure}

\subsection{Tool Execution Success Rates}
\label{subsec:tool-success}

Tool execution success measures whether the policy generates syntactically valid \ac{JSON} tool calls that execute without errors.
As noted in \cref{subsec:episode-length}, episodes shorten substantially over training, which could artificially inflate success rates if the policy simply avoids aforementioned death-spirals.
However, success rate improvements occur throughout training as rewards increase, including during the early phase when episodes consistently exhaust the 30-call budget, indicating that the policy learns to generate more valid tool calls rather than merely becoming more selective about which tools to invoke.

\Cref{fig:tool-success} shows tool execution success rates for both tools throughout training.
The shell command success rates improve from approximately 45\% early in training to 80\% at the end, though the peak success rate coincides with episodes averaging only 5 tool calls.
The apply\_patch success rates exhibit greater volatility, but still trend upwards.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\textwidth]{plotting/figures/plots/temporal/tool_success_rates_ema0.05_8dc73bp4.png}
	\caption{Tool execution success rates throughout training, computed as the mean proportion of successful invocations per tool type across episodes. Plotted with exponential moving average smoothing (\(\alpha = 0.05\)).}
	\label{fig:tool-success}
\end{figure}

\subsection{Command Usage Evolution}
\label{subsec:command-evolution}

While \cref{subsec:episode-length} examined overall shell command and apply\_patch counts, \cref{fig:command-evolution} decomposes shell command usage into the most frequently invoked individual commands to reveal strategic shifts in debugging behavior.
Each command's usage is expressed as a percentage of total tool calls at that training step, allowing us to broadly observe the patterns the policy favors as a function of time.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\textwidth]{plotting/figures/plots/temporal/command_trend_direct_ema0.05_8dc73bp4.png}
	\caption{Evolution of command usage frequencies throughout training, plotted with exponential moving average smoothing (\(\alpha = 0.05\)).}
	\label{fig:command-evolution}
\end{figure}

\section{RQ2: SWE-Bench-Verified Performance}
\label{sec:rq2-swebench}

\textbf{Research Question:}
Does execution-free patch-similarity \ac{RL} training improve SWE-Bench-Verified performance?

Following the evaluation protocol detailed in \cref{sec:eval-brief}, we compare pre-training Qwen3-14B baseline performance with post-\ac{GSPO} training results on SWE-Bench-Verified (approximately 500 Python debugging tasks).
We report test-verified success rates with bootstrap 95\% confidence intervals.

\subsection{SWE-Bench-Verified Results} \label{subsec:rq2-performance}

\Cref{tab:swebench-verified-results} presents the baseline performance of Qwen3-14B on SWE-Bench-Verified using the Nano agent before \ac{RL} training.
The baseline model submitted patches for all 500 instances, with 186 episodes completing the full evaluation pipeline.
Of these completed episodes, 36 instances were successfully resolved by passing all associated test cases, representing a 7.2\% resolve rate on total instances.
The high proportion of empty patches (313 instances, 62.6\%) indicates that the baseline model frequently failed to generate valid repository modifications within the episode constraints.

\begin{table}[htbp]
\centering
\caption{SWE-Bench-Verified performance comparing pre-training baseline and post-\ac{GSPO} training. The baseline model is Qwen3-14B with the Nano agent before \ac{RL} training. Metrics represent test-verified outcomes on the full 500-instance benchmark.}
\label{tab:swebench-verified-results}
\begin{tabular}{lcc}
\toprule
\textbf{Metric} & \textbf{Baseline} & \textbf{Ours} \\
\midrule
Total instances & 500 & 500 \\
Submitted instances & 500 & --- \\
Completed instances & 186 & --- \\
Resolved instances & 36 & --- \\
Unresolved instances & 150 & --- \\
Empty patch instances & 313 & --- \\
\midrule
Resolve rate (on total) & 7.2\% & --- \\
Resolve rate (on completed) & 19.4\% & --- \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Analysis} \label{subsec:rq2-analysis} \todoinline{Interpret success rate improvements, discuss error modes and failure cases, and relate patch-similarity training rewards to test-verified success.}

\section{RQ3: Multilingual Generalization}
\label{sec:rq3-multilingual}

\textbf{Research Question:}
Does execution-free \ac{RL} enable effective multilingual training without language-specific engineering?

Using the mixed 1,000-task curriculum described in \cref{sec:data-env} (750 Python + 250 multilingual training tasks), we evaluate \ac{GSPO}-trained checkpoints on the reserved 50-task SWE-Bench-Multilingual holdout spanning nine programming languages.
This evaluation directly tests whether execution-free patch-similarity rewards enable unified training across diverse languages without language-specific infrastructure.
We report per-language reward deltas with bootstrap 95\% confidence intervals, comparing pre-training baselines against post-training performance.

\subsection{Per-Language Results}\label{subsec:rq3-results}
\todoinline{Insert table/plot of multilingual holdout rewards before vs. after training, including per-language breakdown (Rust, Java, PHP, Ruby, JavaScript, TypeScript, Go, C, C++) and bootstrap confidence intervals.}

\subsection{Cross-Language Generalization Analysis}\label{subsec:rq3-analysis}
\todoinline{Interpret per-language reward shifts, highlight languages with notable improvements, discuss languages with limited gains, and analyze whether execution-free patch-similarity rewards generalize across programming paradigms.
Consider language family effects (e.g., C-family vs.
functional languages).}