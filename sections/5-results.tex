\chapter{Experimental Results and Analysis}
\label{ch:results}

This chapter presents experimental results addressing our research questions (RQ1/RQ2/RQ3/RQ4) about online \ac{RL} for automated code repair.
\todoinline{Insert comprehensive experimental results demonstrating improvements over baseline approaches and evaluation of learned debugging capabilities generalization.}

\section{Training Dynamics and Convergence}
\label{sec:training-dynamics}

\subsection{Learning Curve Analysis}
\label{subsec:learning-curve}

Our \ac{GRPO}~\cite{shao2024deepseekmathpushinglimitsmathematical} training demonstrates clear and consistent learning dynamics across multiple experimental runs.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.9\textwidth]{plotting/figures/plots/temporal/training_loss_ema0.05_tfk08zx2.png}
	\caption{Training loss evolution over time showing convergence dynamics for run tfk08zx2 with exponential moving average smoothing ($\alpha=0.05$).}
	\label{fig:training-loss}
\end{figure}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.9\textwidth]{plotting/figures/plots/temporal/reward_components_ema0.05_tfk08zx2.png}
	\caption{Evolution of reward components throughout training, showing the progression of different reward signals over training steps.}
	\label{fig:reward-components}
\end{figure}

\subsubsection{Reward Progression}

\todoinline{Describe training progression from initial near-zero success rates through rapid improvement phases.
Include analysis of: \begin{itemize}
\item Initial Phase: Tool usage pattern acquisition
\item Intermediate Phase: Strategy development and generalization
\item Convergence Phase: Refinement and breakthrough improvements
\end{itemize} Validate hypothesis about RL enhancement of coding capabilities.
}

\subsubsection{Training Stability}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=1.0\textwidth]{plotting/figures/plots/comparison/training_dynamics_c8qr1evt_vs_tfk08zx2.png}
	\caption{Comprehensive comparison of training dynamics between GSPO (c8qr1evt) and GRPO (tfk08zx2) approaches. The comparison shows training loss, mean reward, KL divergence, and learning progress across training steps. GRPO demonstrates significantly more stable training with faster convergence and better final performance.}
	\label{fig:training-comparison}
\end{figure}

\todoinline{Analyze GRPO training stability compared to traditional PPO implementations.
Report gradient variance reduction, convergence smoothness, and standard deviation metrics across training runs.
}

\subsection{Computational Efficiency}
\label{subsec:computational-efficiency}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.9\textwidth]{plotting/figures/plots/temporal/kl_divergence_ema0.05_tfk08zx2.png}
	\caption{KL divergence between policy and reference model throughout training, showing the degree of policy shift during online learning.}
	\label{fig:kl-divergence}
\end{figure}

\todoinline{Report computational efficiency improvements from training-inference duality architecture:
\begin{itemize}
\item Wall-clock training time reduction vs traditional approaches
\item GPU utilization metrics across training and inference hardware
\item Sample efficiency improvements in episodes to target performance
\end{itemize}
Analyze NCCL-based weight synchronization overhead and contribution to efficiency gains.}

\section{Main Results: RL vs.
  Baselines} \label{sec:main-results}

\subsection{SWE-Bench-Verified Performance} \label{subsec:swe-bench-performance}

Table~\cref{tab:main-results} presents our primary experimental results on \ac{SWE-Bench-Verified}, comparing online \ac{RL} training against multiple baseline approaches.

\todoinline{Add table here}

\subsubsection{Statistical Significance}

\todoinline{Report statistical significance analysis using McNemar's test for paired comparisons:
\begin{itemize}
\item 8B Model: \ac{RL} vs \ac{SFT} improvement with confidence intervals
\item 32B Model: \ac{RL} vs \ac{SFT} improvement with confidence intervals
\item Effect sizes (Cohen's d) for both model sizes
\end{itemize}
Provide evidence for substantial and statistically significant RL improvements.}

\subsubsection{Qualitative Improvement Analysis}

Beyond quantitative metrics, agent-trained models demonstrate qualitatively different debugging behaviors:

\textbf{Strategic Exploration}: \ac{RL}-trained agents develop systematic repository exploration strategies, typically examining project structure, documentation, and related test files before attempting fixes.

\textbf{Context Awareness}: Agents learn to gather sufficient context about bug locations, including understanding function signatures, variable scopes, and dependency relationships.

\textbf{Iterative Refinement}: Unlike single-shot generation approaches, agents can discover and correct initial mistakes through multi-step interaction patterns.

\textbf{Tool Usage Efficiency}: Trained agents develop efficient command usage patterns, avoiding redundant operations and focusing on information-gathering commands that maximize debugging insight.

\section{RQ1: Adaptation to the Nano Harness}
\label{sec:rq1-harness}

\textbf{Research Question 1}: How effectively does online RL training enable models to adapt to the Nano harness environment, and what specific improvements occur in tool usage patterns?

\subsection{Methodology}
\label{subsec:rq1-methodology}
We analyze harness adaptation through three key metrics measured throughout training: \begin{itemize}
	\item \textbf{Tool success rate}: fraction of tool calls that execute without schema/validation errors
	\item \textbf{Invalid call rate}: fraction of calls rejected by the harness (schema mismatch, unsafe commands)
	\item \textbf{Action efficiency}: commands per successful episode; files viewed per solved task
\end{itemize}

\subsection{Results} \label{subsec:rq1-results}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.9\textwidth]{plotting/figures/plots/temporal/tool_success_rates_ema0.05_tfk08zx2.png}
	\caption{Tool success rates over training steps, showing the model's improving ability to generate valid tool calls that execute successfully.}
	\label{fig:tool-success}
\end{figure}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.9\textwidth]{plotting/figures/plots/temporal/command_trend_direct_ema0.05_tfk08zx2.png}
	\caption{Evolution of command usage patterns throughout training, demonstrating shifts in tool usage strategies.}
	\label{fig:command-trend}
\end{figure}

\todoinline{Insert comprehensive analysis including: \begin{itemize}
\item Quantitative metrics from training logs
\item Statistical significance of improvements
\item Comparison with baseline models
\item Actions-per-episode distribution analysis
\end{itemize}}

\subsection{Key Findings} \label{subsec:rq1-findings} \todoinline{Summarize key findings regarding harness adaptation effectiveness, timeline of improvements, and implications for RL training success.
}

\section{RQ2: Multiple Base Models}
\label{sec:rq2-multi-base}

\textbf{Research Question 2}: Does the training recipe and Nano harness generalize across different base model architectures and sizes?

\subsection{Methodology}
\label{subsec:rq2-methodology}
We evaluate training recipe transferability by applying identical training procedures to multiple base models with consistent experimental conditions: \begin{itemize}
	\item Same training budgets and episode counts
	\item Identical prompts and evaluation protocols
	\item Consistent hyperparameter scaling approaches
	\item Uniform tool success metric collection
\end{itemize}

\subsection{Model Comparison} \label{subsec:rq2-comparison} \todoinline{Insert comprehensive comparison table including: \begin{itemize}
\item Performance on SWE-Bench-Verified across different base models
\item Tool success metrics for each model
\item Training convergence characteristics
\item Computational efficiency comparisons
\item Statistical significance of cross-model consistency
\end{itemize}}

\subsection{Analysis} \label{subsec:rq2-analysis} \todoinline{Analyze training recipe portability, identify model-specific adaptations required, and discuss implications for scaling to new architectures.
}

\section{RQ3: \ac{SWE-Gym}
  Training vs.
  \ac{SWE-Bench}/\ac{Multi-SWE-Bench}
  Evaluation} \label{sec:rq3-overfit}

\textbf{Research Question 3}: Does training on \ac{SWE-Gym} lead to overfitting, or do learned debugging strategies generalize to unseen evaluation datasets?

\subsection{Experimental Design}
\label{subsec:rq3-design}
We assess generalization by training exclusively on \ac{SWE-Gym} and evaluating on held-out datasets: \begin{itemize}
	\item Training dataset: \ac{SWE-Gym} repository instances
	\item Evaluation datasets: \ac{SWE-Bench} and \ac{Multi-SWE-Bench}
	\item Comparison baselines: Models trained with \ac{SFT} on same data splits
	\item Overfitting indicators: Performance gap analysis between training and evaluation
\end{itemize}

\subsection{Cross-Dataset Performance} \label{subsec:rq3-performance} \todoinline{Insert comprehensive cross-dataset analysis including: \begin{itemize}
\item Performance comparison table across datasets
\item Gap analysis between SWE-Gym training and evaluation performance
\item Comparison with \ac{SFT} baselines on same data splits
\item Statistical analysis of overfitting indicators
\item Domain transfer success metrics
\end{itemize}}

\subsection{Generalization Analysis} \label{subsec:rq3-analysis} \todoinline{Analyze evidence for/against overfitting, discuss learned strategy transferability, and evaluate cross-dataset generalization effectiveness.
}

\section{RQ4: Cross-Harness and Cross-Task Generalization}
\label{sec:rq4-cross}

\textbf{Research Question 4}: How well do models trained on the Nano harness transfer to different interaction paradigms and task domains?

\subsection{Evaluation Domains}
\label{subsec:rq4-domains}
We assess out-of-domain generalization across multiple dimensions: \begin{itemize}
	\item \textbf{Cross-Harness}: Aider and OpenHands-style interaction paradigms
	\item \textbf{Cross-Task}: Terminal-centric tasks (TauBench, TerminalBench)
	\item \textbf{Adaptation Requirements}: Minimal prompt modifications vs. fine-tuning needs
\end{itemize}

\subsection{Evaluation Protocol} \label{subsec:rq4-protocol} \todoinline{Document evaluation protocol including: \begin{itemize}
\item Cross-harness evaluation setup and metrics
\item Terminal-centric task evaluation procedures
\item Prompt adaptation strategies and requirements
\item Baseline comparisons for each domain
\item Statistical significance testing approaches
\end{itemize}}

\subsection{Transfer Performance} \label{subsec:rq4-transfer} \todoinline{Insert results tables and analysis covering: \begin{itemize}
\item Performance on alternative harnesses
\item Terminal task success rates
\item Comparison with domain-specific baselines
\item Analysis of required adaptations
\item Implications for general agent capabilities
\end{itemize}}

\section{Transfer to General Code Generation} \label{sec:generalization}

\subsection{HumanEval Results} \label{subsec:humaneval}

To assess whether debugging-specific training affects general coding capabilities, we evaluate our models on HumanEval, a standard code generation benchmark.

\todoinline{Insert HumanEval performance comparison table with Pass@1 results showing:
\begin{itemize}
\item Base model performance for different sizes
\item \ac{SFT} model improvements vs. base
\item \ac{RL} model improvements vs. base and \ac{SFT}
\item Statistical significance of improvements
\item Cross-model consistency analysis
\end{itemize}}

\subsubsection{Positive Transfer to Code Generation}

\todoinline{Analyze positive transfer to code generation including:
\begin{itemize}
\item Quantitative improvement analysis across model sizes
\item Statistical significance testing results
\item Evidence for/against negative transfer or capability degradation
\item Comparison with domain-specific training approaches
\end{itemize}}

\subsubsection{Explanation for General Improvement}

Several factors likely contribute to improved general coding performance:

\textbf{Enhanced Code Understanding}: \ac{RL} training requires deep understanding of code structure, variable relationships, and control flow, skills that benefit general programming tasks.

\textbf{Improved Error Analysis}: Learning to interpret and respond to error messages during debugging transfers to writing more robust code from scratch.

\textbf{Strategic Thinking}: Multi-step reasoning and planning skills developed through agent training enhance problem-solving capabilities in general programming contexts.

\section{Ablation Studies}
\label{sec:ablations}

\subsection{Component-wise Analysis}
\label{subsec:component-analysis}

To understand the contribution of different system components, we conducted systematic ablation studies:

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.9\textwidth]{plotting/figures/plots/problem_performance_distribution_tfk08zx2.png}
	\caption{Distribution of problem-solving performance across different task categories, showing which types of problems benefit most from RL training.}
	\label{fig:problem-distribution}
\end{figure}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.9\textwidth]{plotting/figures/plots/problem_clustering_tfk08zx2.png}
	\caption{Clustering analysis of problem types based on model performance patterns, revealing systematic strengths and weaknesses.}
	\label{fig:problem-clustering}
\end{figure}

\todoinline{Insert ablation study results table comparing system configurations: \begin{itemize}
\item Full system performance baseline
\item Impact of removing \ac{RL} training (\ac{SFT} only)
\item Effect of agent scaffold removal
\item Multi-step interaction contribution
\item GRPO vs. traditional PPO comparison
\item Real-time updates impact analysis
\item Statistical significance of each component
\end{itemize}}

\subsubsection{Critical Component Identification}

The ablation results highlight several critical system components:

\todoinline{Analyze critical component contributions including: \begin{itemize}
\item Multi-step interaction impact and validation of experiential learning hypothesis
\item Agent scaffold integration benefits over direct generation
\item RL training advantages over supervised fine-tuning
\item GRPO improvements over traditional PPO methods
\item Real-time updates contribution to sample efficiency
\item Ranking of component importance and implications
\end{itemize}}

\subsection{Hyperparameter Sensitivity} \label{subsec:hyperparameter}

We evaluated sensitivity to key hyperparameters to understand training robustness:

\subsubsection{Learning Rate Analysis}

\todoinline{Report learning rate sensitivity analysis including: \begin{itemize}
\item Tested learning rate ranges
\item Optimal values for different model sizes
\item Performance robustness across effective ranges
\item Model size-specific optimization requirements
\end{itemize}}

\subsubsection{Trajectory Length Impact}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.9\textwidth]{plotting/figures/plots/individual_trajectories_tfk08zx2.png}
	\caption{Individual trajectory analysis showing reward evolution patterns for successful and unsuccessful episodes during training.}
	\label{fig:trajectories}
\end{figure}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.9\textwidth]{plotting/figures/plots/reward_distribution_evolution_tfk08zx2.png}
	\caption{Evolution of reward distribution throughout training, demonstrating the shift from low-reward episodes early in training to higher-reward episodes as the model learns.}
	\label{fig:reward-evolution}
\end{figure}

\todoinline{Analyze trajectory length impact including: \begin{itemize}
\item Tested trajectory length ranges
\item Performance plateaus and optimal values
\item Context sufficiency for debugging tasks
\item Computational efficiency tradeoffs
\end{itemize}}

\section{Error Analysis and Failure Modes} \label{sec:error-analysis}

\subsection{Systematic Failure Analysis} \label{subsec:failure-analysis}

To understand current limitations and guide future improvements, we analyzed failed debugging attempts across multiple categories:

\todoinline{Insert failure mode analysis table categorizing failed tasks by: \begin{itemize}
\item Problem understanding errors and frequency
\item File identification mistakes
\item Correct location but wrong fix applications
\item Incomplete understanding issues
\item Technical tool usage errors
\item Context length limitation impacts
\item Statistical analysis of failure patterns
\end{itemize}}

\subsubsection{Problem Understanding Challenges}

\todoinline{Analyze most common failure modes and their implications for improvement strategies.
}
This suggests areas for improvement:

\begin{itemize}
	\item \textbf{Enhanced Issue Processing}: Better training on interpreting natural language issue descriptions and mapping them to technical requirements
	\item \textbf{Clarification Strategies}: Learning to ask clarifying questions or seek additional context when issue descriptions are ambiguous
	\item \textbf{Domain Knowledge Integration}: Incorporating more domain-specific knowledge about common bug patterns and software engineering practices
\end{itemize}

\subsubsection{File Localization Accuracy}

\todoinline{Discuss file identification challenges and potential improvements in repository understanding.
}

\begin{itemize}
	\item \textbf{Improved Search Strategies}: Learning more effective patterns for locating relevant code through grep and find operations
	\item \textbf{Dependency Analysis}: Better understanding of code dependencies and import relationships
	\item \textbf{Project Structure Learning}: Enhanced ability to navigate unfamiliar project architectures and coding conventions
\end{itemize}

\subsection{Success Pattern Analysis}
\label{subsec:success-patterns}

Conversely, analyzing successful debugging attempts reveals effective strategies:

\subsubsection{Successful Exploration Patterns}

Successful agents consistently follow effective exploration patterns:

\begin{enumerate}
	\item \textbf{Initial Reconnaissance}: Examine project structure, README files, and high-level organization
	\item \textbf{Issue Analysis}: Carefully parse issue descriptions and identify key terms for searching
	\item \textbf{Systematic Search}: Use grep and find strategically to locate relevant code sections
	\item \textbf{Context Gathering}: Examine related files, tests, and documentation before attempting fixes
	\item \textbf{Targeted Modification}: Apply precise, minimal changes that address the root cause
\end{enumerate}

\subsubsection{Tool Usage Efficiency}

Successful agents develop efficient tool usage patterns:

\begin{itemize}
	\item \textbf{Strategic Grep Usage}: Effective search terms and patterns that quickly locate relevant code
	\item \textbf{Minimal File Examination}: Focus on essential files rather than exhaustive exploration
	\item \textbf{Iterative Refinement}: Start with broad searches and progressively narrow focus
	\item \textbf{Error-Driven Learning}: Adapt strategies based on command outputs and error messages
\end{itemize}

\section{Computational Performance Analysis} \label{sec:performance-analysis}

\subsection{Training Efficiency Metrics} \label{subsec:training-efficiency}

Our infrastructure achievements enable practical large-scale agent training:

\todoinline{Insert training performance metrics table including: \begin{itemize}
\item Episodes per hour for different model configurations
\item Memory usage across model sizes
\item Update latency measurements
\item Training throughput analysis
\item GPU utilization efficiency
\item Scalability characteristics
\end{itemize}}

\subsubsection{Scalability Achievements}

\todoinline{Analyze scalability characteristics including: \begin{itemize}
\item GPU scaling linearity up to tested limits
\item Memory utilization efficiency achievements
\item Network bandwidth consumption for NCCL updates
\item Fault tolerance and graceful degradation capabilities
\item Performance bottlenecks and limitations
\end{itemize}}

\subsection{Cost-Effectiveness Analysis} \label{subsec:cost-effectiveness}

\todoinline{Analyze computational cost-effectiveness including: \begin{itemize}
\item Total training costs for different model sizes
\item Per-episode cost comparisons with commercial APIs
\item Development cost implications for academic research
\item Infrastructure maintenance requirements
\item Budget accessibility and democratization implications
\end{itemize}}

\todoinline{Table of A100 GPU hours per run?
}

\section{Summary of Key Findings}
\label{sec:key-findings}

Our comprehensive experimental evaluation yields several important conclusions:

\subsection{Research Question Answers} \label{subsec:research-answers}

\textbf{RQ1 (Harness Adaptation)}: \todoinline{Summarize RL training improvements in Nano harness adaptation, including concrete metrics for tool success rates, invalid call reductions, and action efficiency improvements.
Reference Section~\ref{sec:rq1-harness}.
}

\textbf{RQ2 (Multiple Base Models)}: \todoinline{Summarize training recipe portability across base models and consistent gains over \ac{SFT}.
Include specific performance deltas and statistical significance.
Reference Section~\ref{sec:rq2-multi-base}.
}

\textbf{RQ3 (Cross-Dataset Generalization)}: \todoinline{Summarize SWE-Gym training performance on SWE-Bench and Multi-SWE-Bench, including quantitative results and confidence intervals.
Discuss overfitting evidence.
Reference Section~\ref{sec:rq3-overfit}.
}

\textbf{RQ4 (Cross-Harness/Task Generalization)}: \todoinline{Summarize zero-shot transfer results to alternative harnesses and terminal-centric tasks.
Include specific performance metrics and generalization analysis.
Reference Section~\ref{sec:rq4-cross}.
}

\subsection{Broader Implications}
\label{subsec:broader-implications}

These results have several important implications for the field:

\begin{itemize}
	\item \textbf{Design Philosophy}: \todoinline{Discuss how the bitter lesson applies to training software engineering agents, comparing simple tools with extensive learning versus sophisticated engineering approaches.}
	\item \textbf{Future Potential}: \todoinline{Analyze evidence for continued improvement potential through longer training periods and larger models, based on observed training dynamics.}
\end{itemize}

These findings establish online reinforcement learning as a promising direction for advancing automated software engineering capabilities.
