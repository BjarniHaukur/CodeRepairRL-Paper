\chapter{Experimental Results}
\label{ch:results}

This chapter presents experimental results addressing the three research questions established in Chapter~\ref{ch:introduction}.
Following the evaluation protocol detailed in \cref{sec:eval-brief}, we report harness adaptation metrics (RQ1), test-verified success rates on SWE-Bench-Verified (RQ2), and language-agnostic training effectiveness on the SWE-Bench-Multilingual holdout (RQ3), supplemented by patch-similarity rates and training dynamics analysis.

The detailed analyses in this chapter focus primarily on run 2m8geyey (Qwen3-14B-Multilingual-GSPO-Clipping), which trained for 212 steps before experiencing a hardware crash.
Despite its premature termination, this run exhibits particularly instructive characteristics including exceptionally high \ac{KL} divergence reflecting aggressive policy exploration, a dramatic late-stage breakthrough in patch application success reaching 90.6\%, and clear evidence of strategic debugging workflow development.
The training dynamics and harness adaptation patterns observed in this run illuminate both the promise and challenges of \ac{RL}-based debugging agent training.

\todoinline{Insert comprehensive experimental results tables and statistical analyses once training runs complete, demonstrating improvements over baseline approaches.}

\section{RQ1: Nano Harness Adaptation}
\label{sec:rq1-harness}

\textbf{Research Question 1}: How does \ac{GSPO} training improve Nano harness adaptation?

Following the evaluation protocol detailed in \cref{sec:eval-brief}, we measured harness adaptation through tool-call success rates, invalid-call reduction, and action efficiency, supplemented by qualitative analysis of command usage evolution over training.

\subsection{Training Dynamics} \label{subsec:training-dynamics}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.9\textwidth]{plotting/figures/plots/temporal/reward_components_ema0.05_2m8geyey.png}
	\caption{Evolution of the Unified Diff Similarity reward component throughout training.}
	\label{fig:reward-components}
\end{figure}

\subsection{Harness Adaptation Results} \label{subsec:rq1-results}

The evaluation of harness adaptation reveals how effectively the model learns to interact with the Nano debugging environment through valid tool calls and strategic command selection.
Run 2m8geyey demonstrates particularly striking patterns in both tool execution success and command usage evolution, culminating in a dramatic late-stage breakthrough that validates the \ac{RL} training approach.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.9\textwidth]{plotting/figures/plots/temporal/tool_success_rates_ema0.05_2m8geyey.png}
	\caption{Tool success rates throughout training.}
	\label{fig:tool-success}
\end{figure}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.9\textwidth]{plotting/figures/plots/temporal/command_trend_direct_ema0.05_2m8geyey.png}
	\caption{Evolution of command usage frequencies throughout training.}
	\label{fig:command-trend}
\end{figure}

\subsection{Command Evolution and Specialization} \label{subsec:command-evolution}
\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.9\textwidth]{plotting/figures/plots/sankey/early_sankey.png}
	\caption{Evolution of command usage frequencies throughout training.}
	\label{fig:early-sankey}
\end{figure}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.9\textwidth]{plotting/figures/plots/sankey/late_sankey.png}
	\caption{Evolution of command usage frequencies throughout training.}
	\label{fig:late-sankey}
\end{figure}

\subsection{Key Findings} \label{subsec:rq1-findings}

\section{RQ2: Execution-Free Patch-Similarity \ac{RL}
  Performance} \label{sec:rq2-swebench}

\textbf{Research Question 2}: Does execution-free patch-similarity \ac{RL} training improve SWE-Bench-Verified performance?

Following the evaluation protocol detailed in \cref{sec:eval-brief}, we compare pre-training Qwen3-14B baseline performance with post-\ac{GSPO} training results on SWE-Bench-Verified (approximately 500 Python debugging tasks).
We report test-verified success rates with bootstrap 95\% confidence intervals.

\subsection{SWE-Bench-Verified Results} \label{subsec:rq2-performance}

\Cref{tab:swebench-verified-results} presents the baseline performance of Qwen3-14B on SWE-Bench-Verified using the Nano agent before \ac{RL} training.
The baseline model submitted patches for all 500 instances, with 186 attempts completing the full evaluation pipeline.
Of these completed attempts, 36 instances were successfully resolved by passing all associated test cases, representing a 7.2\% resolve rate on total instances.
The high proportion of empty patches (313 instances, 62.6\%) indicates that the baseline model frequently failed to generate valid repository modifications within the episode constraints.

\begin{table}[htbp]
\centering
\caption{SWE-Bench-Verified performance comparing pre-training baseline and post-\ac{GSPO} training. The baseline model is Qwen3-14B with the Nano agent before \ac{RL} training. Metrics represent test-verified outcomes on the full 500-instance benchmark.}
\label{tab:swebench-verified-results}
\begin{tabular}{lcc}
\toprule
\textbf{Metric} & \textbf{Baseline} & \textbf{Ours} \\
\midrule
Total instances & 500 & 500 \\
Submitted instances & 500 & --- \\
Completed instances & 186 & --- \\
Resolved instances & 36 & --- \\
Unresolved instances & 150 & --- \\
Empty patch instances & 313 & --- \\
\midrule
Resolve rate (on total) & 7.2\% & --- \\
Resolve rate (on completed) & 19.4\% & --- \\
\bottomrule
\end{tabular}
\end{table}

\todoinline{Fill in trained model results once final evaluation completes.
Include bootstrap confidence intervals for resolve rates.
}

\subsection{Analysis} \label{subsec:rq2-analysis} \todoinline{Interpret success rate improvements, discuss error modes and failure cases, and relate patch-similarity training rewards to test-verified success.}

\section{RQ3: Language-Agnostic Training Effectiveness} \label{sec:rq3-multilingual}

\textbf{Research Question 3}: Does execution-free \ac{RL} enable effective multilingual training without language-specific engineering?

Using the mixed 1,000-task curriculum described in \cref{sec:data-env} (750 Python + 250 multilingual training tasks), we evaluate \ac{GSPO}-trained checkpoints on the reserved 50-task SWE-Bench-Multilingual holdout spanning nine programming languages.
This evaluation directly tests whether execution-free patch-similarity rewards enable unified training across diverse languages without language-specific infrastructure.
We report per-language reward deltas with bootstrap 95\% confidence intervals, comparing pre-training baselines against post-training performance.

\subsection{Per-Language Results}\label{subsec:rq3-results}
\todoinline{Insert table/plot of multilingual holdout rewards before vs. after training, including per-language breakdown (Rust, Java, PHP, Ruby, JavaScript, TypeScript, Go, C, C++) and bootstrap confidence intervals.}

\subsection{Cross-Language Generalization Analysis}\label{subsec:rq3-analysis}
\todoinline{Interpret per-language reward shifts, highlight languages with notable improvements, discuss languages with limited gains, and analyze whether execution-free patch-similarity rewards generalize across programming paradigms.
Consider language family effects (e.g., C-family vs.
functional languages).}