\chapter{Infrastructure: Live Weight Synchronization for Online RL}
\label{ch:work}

\Cref{ch:method} established the methodology: execution-free patch-similarity rewards, the Nano agent specification, \ac{GSPO} training, and evaluation protocol.
This chapter documents the practical realization of that methodology, addressing two primary challenges: enabling online \ac{RL} training with live policy updates to deployed inference servers, and making multi-turn agent training tractable within academic compute constraints through systematic optimization.

We detail the training environment implementation, including live adapter synchronization via \ac{NCCL}, trainer extensions for variable-turn episodes, and orchestration under SLURM.
Compute-efficiency optimizations compose parameter-efficient adaptation, memory management, gradient checkpointing, mixed-precision training, and fused kernels to enable 14B parameter training on modest \ac{GPU} clusters.
Complete implementation artifacts and configurations appear in \cref{app:reproducibility}.

\section{Online RL Infrastructure}
\label{sec:online-rl-infrastructure}

Recent \ac{RL} training libraries have begun supporting out-of-process generation, where episode collection runs on separate inference servers rather than within the trainer process itself.
For most use cases, vLLM's synchronous v1 engine suffices: the trainer requests a batch of prompts, waits for completion, and processes the returned sequences.
This approach works well for single-turn generation but proves inefficient for multi-turn coding agents, where existing methods execute each turn synchronously across the entire batch.

We adopt a different architecture that leverages vLLM's asynchronous engine and its OpenAI-compatible \acs{API}.
The async engine implements continuous batching, tool call parsing, and KV-cache management necessary for multi-turn agent interactions.
From the trainer's perspective, we provide a method that synchronously returns a batch of completed episode trajectories.
Internally, however, individual agents progress through their multi-turn interactions asynchronously, substantially improving throughput.
The batch remains synchronous at the episode level to satisfy group-relative policy optimization requirements.

The OpenAI-compatible \acs{API} interface decouples the \ac{AI} application logic from trainer abstractions.
The agent runs identically during training and deployment—the same server configuration, tool definitions, and interaction logic in both contexts.
The trainer operates only on issue descriptions and completed trajectories, requiring no agent-specific code.

\Cref{fig:training-sequence-diagram} illustrates the training loop.
After the trainer collects N completed trajectories and updates the policy, modified weights synchronize to the vLLM server via \ac{NCCL} without restarting the inference service.

\begin{figure}[H]
\centering
\includegraphics[width=1.0\textwidth]{plotting/figures/training_sequence_diagram.png}
\caption{Training loop architecture showing asynchronous episode collection through the vLLM server's \ac{API} and subsequent weight synchronization via \ac{NCCL}.}
\label{fig:training-sequence-diagram}
\end{figure}

Standard vLLM loads weights at server initialization and keeps them frozen thereafter.
Our key technical contribution extends the async engine to accept \ac{NCCL}-based weight updates from the distributed trainer, enabling continuous policy improvement without service restarts.

\section{Live Weight Synchronization}
\label{sec:live-weight-sync}

Weight synchronization requires extensions on both sides of the training-inference boundary.
We inject a worker into vLLM's async engine that manages an \ac{NCCL} communicator and exposes parameter update and kv-cache resetting operations, while the trainer instantiates a corresponding client.
At system initialization, the trainer launches on designated training \acsp{GPU} and waits for the \ac{NCCL} handshake, while the vLLM server launches on separate inference \acsp{GPU} and opens the communicator, establishing a persistent point-to-point channel.
This architecture supports flexible deployment topologies, accommodating co-located processes on a single node or distributed configurations across networked infrastructure.
Training commences once the communication channel initializes.

After each policy update, the trainer gathers updated parameters sequentially, layer by layer, from the distributed training nodes.
For \ac{LoRA}-adapted models, adapters are merged into their base layers before transmission, rendering the synchronization mechanism agnostic to whether training uses full weights or parameter-efficient adaptation—the inference server receives standard weight tensors in both cases.
This layer-wise gathering bounds peak memory consumption during the collection phase while the sequential transmission introduces negligible latency relative to network transfer time.

The gathered weights broadcast to the inference server, where the worker process updates model parameters layer by layer.
After parameter updates, we invalidate all cached key-value states, as cached projections were computed using previous weights.
The worker resets the prefix cache completely, ensuring subsequent requests execute with consistent weights and cache state.

The system coordinates synchronization timing such that no episode requests arrive during the weight update and cache reset window.
This design eliminates race conditions between parameter updates and ongoing inference without requiring explicit locking mechanisms or request queuing—the training loop naturally gates new episode dispatch on synchronization completion.

\section{Compute Optimizations} \label{sec:compute-optimizations}

A central objective of this work was making online \ac{RL} for coding agents tractable within academic compute budgets.
A naive implementation of our training pipeline quickly exhausts available \ac{VRAM}: a 14B parameter model consumes 28GB in BF16 precision for weights alone, before accounting for optimizer states (another 28GB for AdamW), gradients, or the substantial activation memory accumulated across long multi-turn episodes.
Training on three A100 \acp{GPU} rather than industrial clusters with dozens of accelerators required addressing multiple simultaneous memory and computation bottlenecks.

Aside from standard optimizations like BF16 mixed-precision training and FlashAttention kernels, we describe key techniques that address distinct bottlenecks and compose synergistically.

\subsection{Limit Aware Agent Design for Improved Reward Signal}
Nano imposes explicit episode limits that serve dual purposes: bounding \ac{VRAM} utilization and improving sample efficiency.
Token limits prevent out-of-memory failures as conversation histories grow, while tool-call budgets and wall-clock timeouts prevent runaway episodes from monopolizing training resources.
Rather than aggressively truncating episodes to fit memory constraints—which risks discarding tokens that influenced the final reward—we set budgets that accommodate realistic debugging workflows.
The agent receives warnings when approaching limits, encouraging submission attempts that ensure episodes yield gradient information even for unsolved tasks.

\subsection{\ac{LoRA} for Parameter-Efficient Adaptation}

We restrict updates to low-rank adapter matrices (rank 32, $\alpha=64$) applied to attention and \ac{MLP} projections, vastly reducing the amount of trainable parameters.
This choice yields compounding benefits: optimizer states shrink proportionally, reference policy access for KL computation requires only unmounting adapters rather than loading duplicate weights, and weight synchronization transmits compact adapter deltas rather than the full 28GB model.
The restricted parameter space also improves training stability by avoiding simultaneous updates to billions of pre-trained parameters.

\subsection{Custom \ac{GSPO}
Triton Kernels for Loss Computation}

The \ac{GSPO} loss involves multiple sequential operations—log-probability computation, masking, importance weighting, advantage normalization, and sequence-level clipping—that materialize intermediate tensors when implemented as separate PyTorch operations.
Following design principles from Liger-Kernel, we developed fused Triton kernels that combine these operations into single \ac{GPU} kernels, eliminating intermediate materialization.
Liger-Kernel reports ~60\% memory reduction and ~20\% throughput gains for core training primitives, while TRL documents ~40\% peak memory reduction during \ac{GRPO} training with fused kernels.

\subsection{DeepSpeed ZeRO for Optimizer State Sharding}

We employ DeepSpeed ZeRO-2 partitioning, which shards optimizer states and gradients across training \acp{GPU} while replicating model parameters.
This configuration addresses the primary memory bottleneck: AdamW optimizer states (first and second moments) would otherwise double the 28GB parameter memory footprint, exceeding per-device \ac{VRAM} budgets.
We choose ZeRO-2 over ZeRO-3 to avoid parameter-gathering latency during forward and backward passes, as 14B model parameters fit comfortably when replicated across two A100s.

\subsection{Gradient Checkpointing for Activation Memory}

Long multi-turn episodes accumulate substantial activation memory during backward passes.
We employ gradient checkpointing at transformer layer boundaries, discarding intermediate activations during forward passes and recomputing them on demand during backpropagation.
This typically reduces peak activation memory by 40-60\% while adding approximately 20-33\% compute overhead.

\subsection{Gradient Accumulation for Effective Batch Size}

Computing group-relative advantages requires that an entire group fit in memory simultaneously; in our configuration we set $G{=}8$.
With two training \acp{GPU}, we set a per-device batch size of 4 episodes, fully utilizing available \ac{VRAM} given the long context lengths of multi-turn debugging episodes.
To achieve higher effective batch sizes without exceeding memory constraints, we accumulate gradients across 4 mini-batches of 8 episodes before applying optimizer updates.
This configuration ensures that 32 episodes contribute to each policy update, trading increased wall-clock time per training step for improved gradient stability through larger effective batch sizes.
