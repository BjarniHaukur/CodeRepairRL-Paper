\chapter{The Work}
\label{ch:work}

This chapter provides a practical account of the work performed.
Where Chapter~\cref{ch:method} formalizes the methodology, here we describe concretely what was implemented, how components were integrated, and which engineering choices enabled a stable, scalable training environment for a single minimalist agent.
All implementation artifacts—system prompts, trainer configurations, hyperparameters, data processing scripts, SLURM templates, and per-episode logs—are released to enable full replication; specific configurations are documented in the Appendix.

We implemented an end-to-end training environment that couples a minimalist terminal agent with an OpenAI-compatible serving layer and a scalable trainer.
The Nano harness exposes only two tools (\texttt{shell}, \texttt{apply\_patch}) and enforces strict truncation (2{,}000 characters per tool call) and episode budgets over tools, tokens, and time.
The serving layer is a vLLM-based \ac{API} server with function-calling extensions and deterministic \ac{JSON} error handling.
Live adapter synchronization propagates \ac{LoRA} updates to inference without downtime, while the trainer realizes \ac{GSPO} with masked loss and on-policy collection for variable-turn episodes.
Experiments are orchestrated by SLURM with per-episode ephemeral repositories and workspace isolation; comprehensive logging stores tool traces, canonical diffs, rewards, token counts, timings, and RNG seeds.

\section{Scope and Objectives}

Our objective was to realize an online, repository-grounded training environment in which a single terminal-based agent learns via execution-free, outcome rewards.
Achieving this required cohesive changes across serving, training, and orchestration so that multi-turn \ac{RL}, tool I/O, and live policy updates operate concurrently and reliably.

\section{Training Environment Implementation}
\label{sec:training-environment-implementation}

\subsection{Live Adapter Synchronization to Deployed API Servers}

A key technical requirement for our compute-efficient approach is real-time weight synchronization to deployed OpenAI-compatible \ac{API} servers using \ac{NCCL}.
This capability addresses a fundamental mismatch between traditional batched inference servers and the asynchronous, multi-turn nature of coding agents.

\subsubsection{The Async Multi-Turn Challenge}
Standard inference servers optimize for batched generation where all requests in a batch complete synchronously—the batch finishes when the slowest request finishes.
This batching strategy proves incompatible with multi-turn coding agents where episodes vary dramatically in both turn count and per-turn generation length.
An agent might complete debugging in five turns with brief commands, while another requires twenty turns with extensive repository exploration.
Within a single turn, different agents generate tool calls of vastly different token lengths.
Traditional batched serving would force all agents to wait for the longest generation to complete before proceeding, drastically reducing throughput and wasting compute resources.

Our implementation leverages vLLM's OpenAI-compatible \ac{API} server with continuous batching and asynchronous request handling, enabling each agent episode to progress independently.
Requests complete as soon as generation finishes, tool calls execute immediately without waiting for other agents, and episodes advance at their natural pace without synchronization barriers.
This asynchronous behavior within turns is essential for practical online \ac{RL} with multi-turn agents, as it maintains high throughput despite heterogeneous episode structures.

\subsubsection{Weight Synchronization Architecture}
The key technical challenge becomes synchronizing updated policy weights to these running \ac{API} servers without interrupting ongoing inference operations.
Training nodes compute \ac{GSPO} updates on \ac{LoRA} adapter parameters following each batch of collected trajectories.
Rather than checkpointing and restarting servers—which would discard valuable KV-cache state and introduce multi-second interruptions—we broadcast weight deltas directly to live servers via \ac{NCCL} collective communication primitives.

We implement differential updates by gathering only the modified adapter weights—typically 100-500MB for rank-32 adapters on 14B models—and broadcasting them to all inference servers.
This selective transmission dramatically reduces network traffic compared to full model synchronization, which would require transferring 50GB+ per update.

A critical optimization addresses memory consumption during parameter gathering under ZeRO-3, where naive all-gather operations would materialize the full model simultaneously, adding 50+ GiB of peak \ac{VRAM}.
We instead gather adapter modules sequentially one layer at a time, reducing peak memory overhead to approximately 100 MiB while introducing negligible latency.

Asynchronous broadcast operations allow training to proceed while synchronization completes, preventing pipeline stalls.
The system maintains separate \ac{NCCL} communication groups: one for gradient synchronization within the distributed training cluster and another for policy broadcasting to inference servers, enabling concurrent operation without interference.

\subsubsection{Performance and Generality}
Weight broadcasts complete in 150-300ms for typical adapter configurations, representing a small fraction of episode generation time.
The differential update strategy reduces communication overhead by approximately 95\% compared to full weight synchronization.
Custom \ac{CUDA} memory management prevents conflicts during concurrent training and inference operations, with automatic recovery from transient network failures through retry mechanisms.

\subsubsection{Broader Applicability Beyond Coding Agents}
The architecture's reliance on standard OpenAI-compatible \ac{API} interfaces means it naturally extends beyond code repair to other interactive applications where outcomes can be rewarded.
The training pipeline remains application-agnostic, requiring only a reward function to evaluate completed interactions.
This design choice simplified our development workflow—we could prototype agent behaviors using standard vLLM serving infrastructure, then enable online training by connecting to the \ac{GSPO} trainer without service interruption—and suggests potential utility for training other interactive systems such as task-specific assistants, document processors, or dialogue agents where conventional \ac{API} interfaces already exist.

\subsection{Trainer Extensions}

The trainer integrates masked loss (see \cref{app:masked-loss}), sequence-level importance ratios computed after masking, and variable-turn rollout collection that preserves turn boundaries while remaining batch-friendly.
We reconciled \ac{LoRA}-only updates under ZeRO partitioning with gradient accumulation and checkpointing.
Our modifications to TRL's trainer and vLLM integration implement these extensions while maintaining compatibility with the standard \ac{API}; implementation details are documented in \cref{app:trl-modifications,app:vllm-config}.

\subsection{Orchestration and Isolation}

Jobs run under SLURM with per-episode ephemeral repositories, workspace-scoped filesystems, cgroups, and rbash.
Optionally, Apptainer/Singularity images pin dependencies.
SLURM submission templates and environment configurations are documented in \cref{app:slurm-config}.

\section{Training Runs and Protocol}

With the training environment infrastructure established, we now describe the concrete experimental protocol and parameter choices for the runs conducted.

We conducted runs on Python-only SWE-Gym (\(\sim\)2{,}400 tasks) and on a compact 1k curriculum (750 SWE-Gym Python + 250 SWE-Bench-Multilingual across nine languages; 50 multilingual tasks held out for validation), repeating epochs over the latter to maximize sample reuse.
Episodes are bounded by 30 tool calls, 12{,}288 generated tokens, and 60\,s wall-clock per task.
Training uses temperature 1.0 without top-k/top-p; evaluation uses temperature 0.2 with top-p 0.9.
Optimization follows \ac{GSPO} with group size eight and ratio clipping \(\varepsilon=0.2\), AdamW on \ac{LoRA} parameters, gradient accumulation and checkpointing, and DeepSpeed ZeRO-2.
All primary experiments use Qwen3-14B; limited comparisons with Qwen3-8B, Qwen3-30B-A3B, Qwen3-32B, and Llama3.1-8B appear in Appendix~\ref{app:model-comparison}.

\todoinline{Add concrete counts: updates, epochs over the 1k curriculum, compute hours, and cluster details.}

\section{Compute-Efficient Infrastructure Implementation}

A central objective of this work was demonstrating that online \ac{RL} for coding agents remains tractable within academic compute budgets.
We achieved this through systematic application of memory and computational optimizations that, when composed carefully, enable training of 14B parameter models on modest \ac{GPU} clusters.

\subsection{Parameter-Efficient Adaptation with \ac{LoRA}}
Rather than fine-tuning complete model weights, we restrict updates to low-rank adapter matrices applied to attention and \ac{MLP} projections.
With rank 32 and $\alpha=64$, adapter parameters comprise less than 1\% of the base model, dramatically reducing optimizer state memory requirements.
This architectural choice provides several compounding benefits beyond raw parameter reduction.
Adapters enable clean separation between the frozen base policy and trainable modifications, simplifying reference policy access during importance weighting without duplicating model memory.
The restricted parameter space also accelerates convergence on downstream tasks, as optimization focuses on a compact representation rather than attempting to shift billions of frozen pre-training parameters.
Adapter-only updates integrate naturally with our differential weight synchronization system, as only the small adapter state requires transmission to inference servers rather than the full model.

\subsection{Memory Management Through DeepSpeed ZeRO}
Training multi-billion parameter models requires distributing optimizer states, gradients, and parameters across multiple \acp{GPU}.
We employ DeepSpeed ZeRO-2 partitioning for Qwen3-14B training.
ZeRO-2 shards optimizer states and gradients while replicating model parameters across devices, balancing communication overhead against memory savings.
This configuration enables 14B parameter training where parameter memory remains manageable but optimizer states—particularly for AdamW with its first and second moment estimates—would otherwise exceed available \ac{VRAM}.
The choice of ZeRO-2 over ZeRO-3 avoids the additional parameter-gathering latency required when sharding model weights, as 14B model parameters fit comfortably within aggregate \ac{GPU} memory when replicated across two devices.

\subsection{Gradient Checkpointing for Activation Memory}
Multi-turn agent episodes generate long sequence lengths that accumulate substantial activation memory during backward passes.
Gradient checkpointing trades computation for memory by selectively discarding intermediate activations during the forward pass and recomputing them on demand during backpropagation.
We checkpoint activations at transformer layer boundaries, reducing peak activation memory by approximately 50\% while incurring modest recomputation overhead.
This memory savings proves essential for accommodating longer episodes within fixed memory budgets, enabling training on realistic debugging scenarios that span dozens of turns rather than constraining experiments to artificially short interactions.

\subsection{Mixed-Precision Training with BF16}
All training employs Brain Float 16 (BF16) arithmetic for forward and backward passes while maintaining FP32 precision for optimizer state accumulation.
BF16 provides memory and throughput advantages over FP32 while offering better numerical stability than FP16 for large language model training, as its wider exponent range prevents the underflow issues that plague FP16 in deep networks.
The precision choice integrates cleanly with other optimizations: DeepSpeed ZeRO handles mixed-precision sharding automatically, gradient checkpointing operates transparently at BF16 precision, and \ac{LoRA} adapters train stably without requiring loss scaling or other numerical interventions.

\subsection{Custom Triton Kernels for \ac{GSPO}
Loss Computation}

The \ac{GSPO} loss involves multiple sequential operations: log-probability computation, masking, importance weighting, advantage normalization, and clipping.
Implementing these as separate PyTorch operations materializes intermediate tensors at each stage, consuming memory bandwidth and triggering numerous kernel launches.
We implemented fused Triton kernels that combine these operations into single GPU kernels, eliminating intermediate materialization and reducing memory traffic by approximately 60\% during the loss computation phase.
Triton's Python-like syntax enabled rapid iteration compared to hand-written CUDA while achieving performance within 5-10\% of optimized CUDA implementations.
These kernels have since been adopted in several open-source \ac{RL} training frameworks.

\subsection{Kernel Optimizations for Attention and Serving}
The vLLM inference server leverages optimized attention kernels and memory management specifically designed for language model serving.
PagedAttention manages KV-cache memory efficiently, reducing fragmentation and enabling higher batch sizes for concurrent agent episodes.
Continuous batching dynamically assembles requests into batches as they arrive, maintaining high \ac{GPU} utilization despite variable episode lengths and completion times.
While FlashAttention provides kernel-level speedups for attention computation, our primary gains come from algorithmic improvements in cache management and batching rather than raw arithmetic throughput.

\subsection{Resource Requirements and Scalability}
The composed optimizations yield manageable resource requirements for Qwen3-14B training.
Training operates on 2 A100 \acp{GPU} with ZeRO-2 and \ac{LoRA}, while inference serving requires 1 A100 \ac{GPU}.
Both training and inference configurations require power-of-2 \ac{GPU} counts for efficient tensor parallelism and collective communication operations.
These modest requirements—3 \acp{GPU} total for a complete training pipeline—remain well within reach of academic research groups, demonstrating that sophisticated online \ac{RL} training need not demand industrial-scale compute infrastructure when careful optimization and architectural choices reduce overhead systematically.
Limited experiments with other model sizes are documented in Appendix~\ref{app:model-comparison}.

\section{Performance Characterization and Achieved Metrics}

We characterize the training environment along multiple axes to demonstrate the practical viability of online \ac{RL} for coding agents within academic compute constraints.

\subsection{Training Throughput and Efficiency}

Our infrastructure achieves practical training throughput across model scales.
\todoinline{Insert training performance metrics table including episodes per hour for different model configurations, memory usage across model sizes, update latency measurements, training throughput analysis, \acp{GPU} utilization efficiency, and scalability characteristics.}

The system demonstrates efficient resource utilization through careful orchestration of concurrent training and inference workloads.
Episodes complete at rates that enable meaningful training progress within reasonable wall-clock times, while memory consumption remains bounded within available \ac{VRAM} budgets through the optimizations detailed in \S\ref{sec:training-environment-implementation}.

\subsection{Synchronization Overhead and Latency}

Live adapter synchronization introduces minimal overhead relative to overall training time.
Weight broadcasts complete within the 150-300ms range established in \S\ref{sec:training-environment-implementation}, representing a small fraction of episode generation time.
\ac{NCCL} bandwidth consumption remains well below network capacity limits even during synchronized updates across multiple inference servers.
\todoinline{Insert measured sync latencies with percentiles (p50/p95) and network bandwidth utilization.}

\subsection{System Utilization and Efficiency}

The composed infrastructure maintains high \ac{GPU} utilization throughout training through continuous batching and overlapped computation-communication patterns.
\todoinline{Document memory utilization efficiency, fault tolerance characteristics, and performance bottlenecks identified during extended training runs.}

\subsection{Cost-Effectiveness for Academic Research}

Total training costs remain accessible for academic research groups.
\todoinline{Insert cost analysis table with total A100 \ac{GPU}-hours per run, per-episode cost comparisons, development cost implications, and infrastructure maintenance requirements.}
These requirements demonstrate that sophisticated online \ac{RL} training need not demand industrial-scale compute budgets when systematic optimization and careful architectural choices reduce overhead efficiently.

