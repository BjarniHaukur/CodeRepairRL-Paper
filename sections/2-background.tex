\chapter{Theoretical Background and Related Work}

This chapter provides the theoretical foundation for online reinforcement learning applied to automated code repair.
We review key concepts in reinforcement learning for language models, automated program repair, and agent-based software engineering approaches.

\section{Automated Code Repair before LLMs}
\label{sec:apr-pre-llm}
\todoinline{GENProg, Defect4j, etc.}
The evolution of \ac{APR} can be traced through three distinct paradigms.
Early search-based approaches like GenProg~\cite{genProg2012} and PAR~\cite{PAR2013} employed genetic programming and pattern-based transformations to search for valid patches within a predefined space of mutations.
These systems demonstrated the feasibility of automated repair but suffered from low precision and limited generalization capabilities.

The transition to learning-based approaches began with Prophet~\cite{prophet2016} and DeepFix~\cite{deepfix2017}, which introduced \ac{ML} to rank patch candidates and fix compilation errors respectively.
The emergence of neural sequence-to-sequence models marked a fundamental shift, with works like SequenceR~\cite{sequencer2019} and CoCoNuT~\cite{coconut2020} treating program repair as a neural machine translation task.

\section{Automated Code Repair with LLMs}

Large language models have transformed automated program repair, moving from rule-based and search-based approaches to neural methods capable of understanding complex code patterns and generating sophisticated patches.

The advent of large language models has transformed the landscape entirely.
Modern systems leveraging models like Codex~\cite{codex2021}, CodeT5~\cite{codet5}, and more recently CodeLlama~\cite{codellama2023} and DeepSeek-Coder~\cite{deepseek2024} have achieved unprecedented performance on established benchmarks.
However, even state-of-the-art models achieve only approximately 20\% success rates on realistic benchmarks like \ac{SWE-Bench}~\cite{sweBench2024}, highlighting the substantial gap between current capabilities and human-level performance.

Key evaluation datasets have evolved alongside these approaches.
CodeXGLUE~\cite{codexglue2021} provides a comprehensive multi-task benchmark including bug fixing across multiple languages.
Most recently, \ac{SWE-Bench}~\cite{sweBench2024} introduced repository-level tasks requiring understanding of entire codebases, representing a significant leap in evaluation complexity.

Current \ac{LLM}-based approaches exhibit several fundamental limitations that constrain their effectiveness:

Recent specialized models like RepairLLaMA~\cite{repairllama2024} have attempted to address these limitations through continued pre-training on bug-fix datasets, achieving modest improvements on benchmarks.
% \textbf{Lack of environmental interaction}: Models cannot execute code, run tests, or observe runtime behavior, limiting their ability to understand dynamic program properties and verify correctness.% \textbf{Single-step generation}: Most models generate patches in a single forward pass without the ability to explore the codebase, test hypotheses, or iteratively refine solutions based on feedback.% \textbf{Limited context windows}: Even with recent advances extending context lengths to 128K+ tokens, models struggle to maintain coherent understanding across large codebases with complex dependencies.% \textbf{Poor multi-file coordination}: Real-world bugs often require coordinated changes across multiple files.% Current models frequently produce inconsistent edits that fail to maintain invariants across file boundaries.
However, these approaches still operate within the fundamental constraint of single-pass generation without environmental feedback.

Current approaches primarily rely on supervised fine-tuning on bug-fix datasets, where models learn to map broken code snippets to corrected versions.
However, this paradigm has fundamental limitations when applied to real-world debugging scenarios.

The fundamental mismatch between training and deployment paradigms represents a critical challenge in automated program repair.
During training, models learn from static datasets of bug-fix pairs, typically formatted as:

\begin{verbatim}
<buggy_code>
def calculate_average(numbers):
    return sum(numbers) / len(numbers) + 1  # Bug: +1
</buggy_code>

<fixed_code>
def calculate_average(numbers):
    return sum(numbers) / len(numbers)
</fixed_code>
\end{verbatim}

This training regime assumes that all necessary information for generating a fix is contained within the immediate code context.
However, real-world debugging is inherently dynamic and exploratory:

\textbf{Hypothesis formation and testing}: Developers form hypotheses about bug causes, test them through code execution or inspection, and iteratively refine their understanding.
This exploratory process cannot be captured in static input-output pairs.

\textbf{Dynamic information gathering}: Debugging often requires examining stack traces, variable values at runtime, test outputs, and log files—information absent from static training data but crucial for understanding bug manifestations.

\textbf{Repository-level reasoning}: Real bugs exist within complex codebases where understanding requires tracing dependencies, examining calling contexts, and maintaining consistency across module boundaries.
The average \ac{SWE-Bench} task involves understanding code spread across 6.3 files with complex interdependencies.

\textbf{Iterative refinement}: Professional debugging rarely produces correct fixes on the first attempt.
Developers test partial solutions, observe failures, and refine their approach—a fundamentally different process from single-shot generation.

Sequence-to-sequence models, even when scaled to billions of parameters, struggle with these requirements for several reasons:

\textbf{Lack of causal reasoning}: Models trained on correlational patterns in code changes cannot reliably perform the counterfactual reasoning required to understand why code fails and how changes will affect behavior.

\textbf{Limited compositional generalization}: While models can memorize common bug patterns, they struggle to compose learned primitives in novel ways required for previously unseen bugs.

\textbf{Absence of verification mechanisms}: Without the ability to execute code and observe outcomes, models cannot verify their proposed solutions or learn from failed attempts.

This mismatch manifests concretely in evaluation results.
On \ac{SWE-Bench}, even the most advanced models achieve less than 25\% success rates, while human developers solving the same tasks achieve over 90\% success rates when given appropriate time and tools.
The gap highlights not just a quantitative difference in performance, but a qualitative difference in problem-solving approach.

Addressing this mismatch requires moving beyond supervised learning on static datasets toward online learning paradigms where models can explore, test, and refine solutions in realistic development environments—precisely the motivation for online reinforcement learning explored in this thesis.

\section{Reinforcement Learning for Language Models}

Reinforcement learning has emerged as a powerful technique for improving language model performance beyond what supervised learning alone can achieve, particularly for tasks requiring sequential decision-making and optimization of complex objectives.
Unlike supervised learning, which relies on fixed input-output pairs, \ac{RL} enables models to learn through interaction with dynamic environments, receiving rewards based on the quality of their generated outputs.

\todoinline{Clarify what online learning is, in particular for LLMs}

\subsection{Policy Gradient Foundations}

\todoinline{Would it be nice to fully model this process as an MDP?}

The foundation of \ac{RL} for language models lies in treating text generation as a Markov Decision Process (\ac{MDP}).
In this formulation, the language model serves as a policy $\pi_\theta(a_t|s_t)$ that selects actions (tokens) $a_t$ given states (context sequences) $s_t$, parameterized by model weights $\theta$.

The objective is to maximize the expected cumulative reward: \begin{equation}
J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}[R(\tau)]
\end{equation} where $\tau = (s_0, a_0, s_1, a_1, \ldots, s_T, a_T)$ represents a trajectory (complete sequence generation) and $R(\tau)$ is the total reward for that trajectory.

The policy gradient theorem provides the foundation for optimization: \begin{equation}
\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}[\sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t|s_t) R(\tau)]
\end{equation}

However, this basic REINFORCE estimator suffers from high variance, making training unstable and sample-inefficient.
This limitation becomes particularly problematic for language models, where sequence lengths can be substantial and reward signals are often sparse.

\subsection{Proximal Policy Optimization (PPO)}

Proximal Policy Optimization~\cite{schulman2017proximalpolicyoptimizationalgorithms} has emerged as the dominant algorithm for fine-tuning large language models due to its ability to stabilize training while maintaining sample efficiency.
\ac{PPO} addresses the fundamental challenge of policy optimization: making meaningful progress without taking overly large steps that destabilize learning.

\subsubsection{The Clipping Mechanism}

\ac{PPO} introduces a clipped objective function that prevents destructive policy updates: \begin{equation}
L^{CLIP}(\theta) = \mathbb{E}_t\left[\min\left(r_t(\theta)\hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t\right)\right]
\end{equation} where: \begin{align}
r_t(\theta) &= \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)} \\
\hat{A}_t &= R_t - V(s_t)
\end{align}

The ratio $r_t(\theta)$ measures how much the current policy differs from the previous policy, while $\hat{A}_t$ represents the advantage estimate computed using a value function $V(s_t)$.
The clipping parameter $\epsilon$ (typically 0.2) constrains policy updates to prevent catastrophic changes.

\subsubsection{Value Function Training}

\ac{PPO} employs a separate value network $V_\phi(s)$ trained to predict expected returns, enabling more accurate advantage estimation: \begin{equation}
L^{VF}(\phi) = \mathbb{E}_t\left[(V_\phi(s_t) - R_t)^2\right]
\end{equation}

The complete \ac{PPO} objective combines policy and value losses: \begin{equation}
L(\theta, \phi) = L^{CLIP}(\theta) - c_1 L^{VF}(\phi) + c_2 S[\pi_\theta](s_t)
\end{equation} where $S[\pi_\theta]$ is an entropy bonus encouraging exploration, and $c_1, c_2$ are weighting coefficients.

\subsubsection{Success in Language Model Fine-tuning}

\ac{PPO}'s success in language model applications, particularly in \ac{RLHF}, stems from several key properties:

\textbf{Stable Learning}: The clipping mechanism prevents the policy from changing too rapidly, which is crucial when fine-tuning large pre-trained models where dramatic changes can destroy learned representations.

\textbf{Sample Efficiency}: By reusing data for multiple gradient steps and employing importance sampling correction, \ac{PPO} achieves better sample efficiency than simpler policy gradient methods.

\textbf{Scalability}: \ac{PPO}'s architecture separates policy and value training, enabling distributed training across multiple \acp{GPU} with different computational loads for each component.

However, \ac{PPO} also introduces significant computational overhead through the separate value network training and the need for multiple gradient updates per batch of experience.

\subsection{Group Relative Policy Optimization (GRPO)}

\ac{GRPO}~\cite{shao2024deepseekmathpushinglimitsmathematical} is fundamentally \ac{PPO} with a crucial simplification: instead of training a separate value network to estimate advantages, \ac{GRPO} computes advantages directly from relative performance within sampled action groups.
This elegant modification preserves \ac{PPO}'s theoretical guarantees while dramatically reducing computational overhead.

\subsubsection{The Core Simplification}

The key insight behind \ac{GRPO} is that for each state $s$, rather than estimating $V(s)$ with a separate network, we can sample multiple actions $a_1, \ldots, a_G$ from the current policy $\pi_{\theta_t}$ and use their reward distribution to compute relative advantages.

For a given state $s$, \ac{GRPO} samples $G$ actions from the policy and computes the group-relative advantage as: \begin{equation}
A^{\pi_{\theta_t}}(s, a_j) = \frac{r(s, a_j) - \mu}{\sigma}
\end{equation} where $\mu$ and $\sigma$ are the mean and standard deviation of the rewards $r(s, a_1), \ldots, r(s, a_G)$.
This is simply the standard score (z-score) of the rewards, providing a normalized measure of relative performance.

Mathematically, this can be expressed as: \begin{align}
\mu &= \frac{1}{G}\sum_{i=1}^G r(s, a_i) \\
\sigma &= \sqrt{\frac{1}{G}\sum_{i=1}^G (r(s, a_i) - \mu)^2} \\
A^{\pi_{\theta_t}}(s, a_j) &= \frac{r(s, a_j) - \mu}{\sigma}
\end{align}

\subsubsection{PPO Objective with Group-Relative Advantages}

\ac{GRPO} then maximizes the standard \ac{PPO} objective, but using these group-relative advantages instead of value-network-based estimates.
The objective becomes: \begin{equation}
\max_\theta \frac{1}{G}\sum_{i=1}^G \mathbb{E}_{(s,a_1,\ldots,a_G) \sim \pi_{\theta_t}}\left[
\begin{cases}
\min\left(\frac{\pi_\theta(a_i|s)}{\pi_{\theta_t}(a_i|s)}, 1+\epsilon\right) A^{\pi_{\theta_t}}(s, a_i) & \text{if } A^{\pi_{\theta_t}}(s, a_i) > 0 \\
\max\left(\frac{\pi_\theta(a_i|s)}{\pi_{\theta_t}(a_i|s)}, 1-\epsilon\right) A^{\pi_{\theta_t}}(s, a_i) & \text{if } A^{\pi_{\theta_t}}(s, a_i) < 0
\end{cases}
\right]
\end{equation}

This formulation preserves \ac{PPO}'s asymmetric clipping behavior: when advantages are positive (indicating good actions), we clip the importance ratio from above at $(1+\epsilon)$ to prevent over-optimization.
When advantages are negative (indicating poor actions), we clip from below at $(1-\epsilon)$ to avoid excessive penalization.

\subsubsection{Intuitive Understanding}

The intuition behind \ac{GRPO} is elegantly simple: each policy update makes the model more likely to produce actions that performed relatively better than other actions tried at the same state, and less likely to produce actions that performed relatively worse.
This creates a natural competitive dynamic where actions are evaluated against their peers rather than against an absolute baseline.

Consider a concrete example: if for a given coding problem, the model generates five different debugging approaches with rewards $[0.1, 0.8, 0.3, 0.9, 0.2]$, \ac{GRPO} will:
\begin{itemize}
	\item Strongly reinforce the action with reward $0.9$ (highest z-score)
	\item Moderately reinforce the action with reward $0.8$ (second highest z-score)
	\item Slightly penalize actions with rewards $0.3, 0.2, 0.1$ (below-average performance)
\end{itemize}

This relative ranking approach is particularly powerful for code repair where absolute reward values may vary significantly across different types of bugs, but relative solution quality within each problem remains meaningful.

\subsubsection{Relationship to PPO}

It's crucial to understand that \ac{GRPO} is not a fundamentally different algorithm from \ac{PPO}—it is \ac{PPO} with a specific choice of advantage estimation.
The clipping mechanism, importance sampling, and optimization dynamics remain identical.
The only change is replacing:
\begin{equation}
\hat{A}_t^{PPO} = R_t - V_\phi(s_t)
\end{equation}
with:
\begin{equation}
\hat{A}_t^{GRPO} = \frac{r_t - \mu_{\text{group}}}{\sigma_{\text{group}}}
\end{equation}

This substitution eliminates the need for:
\begin{itemize}
	\item Training a separate value network $V_\phi$
	\item Computing value loss $L^{VF}(\phi)$
	\item Managing value network hyperparameters
	\item Coordinating policy and value network training schedules
\end{itemize}

\subsubsection{Computational and Practical Advantages}

The computational benefits of \ac{GRPO} are substantial:

\textbf{Memory Efficiency}: Eliminating the value network reduces GPU memory requirements by approximately 50\%, enabling larger batch sizes or model sizes within the same hardware constraints.

% \textbf{Training Simplicity}: The training loop becomes significantly simpler, reducing implementation complexity and potential sources of bugs.
% There are no value network updates to coordinate or balance against policy updates.
% 
% \textbf{Hyperparameter Robustness}: With fewer moving parts, \ac{GRPO} exhibits reduced sensitivity to hyperparameter choices, making it more reliable across different tasks and model architectures.

% \textbf{Batch Processing Efficiency}: \ac{GRPO} can naturally handle variable batch sizes and sequence lengths without the complications introduced by value network training, which often requires careful batch construction.

\subsubsection{Advantages for Code Repair}

% \ac{GRPO}'s design makes it particularly well-suited for code repair applications:

% \textbf{Natural Handling of Sparse Rewards}: Code repair often produces binary success/failure outcomes or sparse quality metrics.
% \ac{GRPO}'s relative comparison approach handles this naturally, as the standard score normalization adapts to the reward distribution within each group.

% \textbf{Problem Diversity}: Different coding problems require vastly different solution approaches and have different inherent difficulty levels.
% \ac{GRPO}'s group-relative baseline automatically adjusts to each problem's context, whereas a global value function would struggle to capture this diversity.

% \textbf{Exploration Encouragement}: By comparing actions against their immediate peers rather than a global baseline, \ac{GRPO} encourages exploration of diverse solution strategies, which is crucial for learning robust debugging skills.

\textbf{Computational Scaling}: Code repair training requires processing thousands of agent interactions across diverse repositories and bug types.
\ac{GRPO}'s computational efficiency makes this scale of training practically feasible.

% The mathematical elegance of \ac{GRPO} lies in its ability to preserve all of \ac{PPO}'s theoretical guarantees while dramatically simplifying the implementation.
% For code repair, where relative solution quality matters more than absolute reward prediction, this approach provides an optimal balance of performance, simplicity, and computational efficiency.

\subsubsection{Variance Collapse: A Fundamental Challenge}

Despite \ac{GRPO}'s computational advantages, it shares with other policy gradient methods a fundamental challenge known as variance collapse or mode collapse~\cite{varianceCollapse2023}.
This phenomenon occurs when the policy gradient optimization inadvertently incentivizes the model to reduce output variance, leading to increasingly deterministic and less exploratory behavior over training iterations.

The mechanism behind variance collapse in \ac{GRPO} can be understood through the optimization dynamics.
When computing group-relative advantages, actions with consistently high rewards relative to their peers receive positive reinforcement, while those with lower relative rewards are penalized.
Over time, this creates a positive feedback loop:

\begin{enumerate}
	\item High-performing actions become increasingly likely
	\item The policy concentrates probability mass on these "safe" actions
	\item Exploration of alternative strategies diminishes
	\item The standard deviation $\sigma$ in the group decreases
	\item Smaller $\sigma$ amplifies advantage magnitudes, accelerating concentration
\end{enumerate}

This collapse is particularly problematic for code repair applications where:

\textbf{Multiple valid solutions exist}: Most bugs can be fixed through various approaches—refactoring the logic, adding error handling, or modifying data structures.
Variance collapse biases the model toward a single approach, potentially missing simpler or more elegant solutions.

\textbf{Exploration enables learning}: Discovering effective debugging strategies requires experimenting with different investigation paths, tool usage patterns, and fix attempts.
Premature convergence prevents the model from discovering these diverse strategies.

\textbf{Robustness requires diversity}: Models that learn only narrow solution patterns fail when encountering bugs requiring different approaches, leading to brittleness in deployment.

Several mitigation strategies have been proposed in the literature:

\textbf{\ac{KL} Divergence Constraints}: A standard stabilization augments the clipped objective with a penalty that constrains divergence from a \emph{frozen} reference policy $\pi_{\mathrm{ref}}$.
In the \ac{GRPO} setting, the optimization can be written as \begin{equation}
    J_{\mathrm{GRPO}}(\theta) = \mathbb{E}_{q \sim P(Q),\; \{o_i\}_{i=1}^{G} \sim \pi_{\theta_{\mathrm{old}}}(\cdot|q)}\left[ \frac{1}{G} \sum_{i=1}^{G} \min\!\left( \frac{\pi_{\theta}(o_i|q)}{\pi_{\theta_{\mathrm{old}}}(o_i|q)} A_i,\; \text{clip}\!\left(\frac{\pi_{\theta}(o_i|q)}{\pi_{\theta_{\mathrm{old}}}(o_i|q)}, 1-\epsilon, 1+\epsilon\right) A_i \right) \right]
    - \beta\, \mathrm{KL}(\pi_{\theta}\,\|\,\pi_{\mathrm{ref}}),
\end{equation} where $\epsilon$ and $\beta$ are hyperparameters and $A_i$ is the group-relative advantage computed from group rewards.
A practical surrogate for the KL term at the sampled output is \begin{equation}
    \mathrm{KL}(\pi_{\theta}\,\|\,\pi_{\mathrm{ref}}) \approx \frac{\pi_{\mathrm{ref}}(o_i|q)}{\pi_{\theta}(o_i|q)} - \log\!\left( \frac{\pi_{\mathrm{ref}}(o_i|q)}{\pi_{\theta}(o_i|q)} \right) - 1,
\end{equation} and, for token-level implementation from logits, with pre-softmax logits $z_{\theta}(s_t)$ and $z_{\mathrm{ref}}(s_t)$, \begin{equation}
    \mathrm{KL}(\pi_{\theta}\,\|\,\pi_{\mathrm{ref}}) = \mathbb{E}_t\left[\mathrm{KL}\left(\sigma(z_{\theta}(s_t))\,\|\,\sigma(z_{\mathrm{ref}}(s_t))\right)\right].
\end{equation} \todoinline{Standardize notation with the \ac{GRPO} section (e.g., $q$, $\{o_i\}_{i=1}^{G}$, $\pi_{\theta_{\mathrm{old}}}$, $\epsilon$, $\beta$, $\pi_{\mathrm{ref}}$) across the document.
}

\subsection{Group Sequence Policy Optimization (GSPO)}

While \ac{GRPO} offers computational efficiency over traditional \ac{PPO}, recent work from the Qwen team has identified critical stability issues that become particularly pronounced in multi-turn tool-use scenarios~\cite{zheng2025groupsequencepolicyoptimization}.
\ac{GSPO} addresses these fundamental limitations through a theoretically grounded reformulation of the importance sampling mechanism.

\subsubsection{The Stability Problem in GRPO}

The core instability in \ac{GRPO} stems from its token-level importance ratios.
When training models for multi-turn interactions with coding agents, this becomes especially problematic due to numerical discrepancies between logits generated during fast inference (using engines like vLLM) and those computed during training.
These discrepancies compound across long sequences, creating high-variance training noise that progressively destabilizes the model.

For our specific use case—training models to use tools iteratively across multiple turns—this instability manifests as:
\begin{itemize}
	\item Catastrophic forgetting of tool-use patterns mid-training
	\item Divergence between inference and training behavior
	\item Irreversible model collapse when sequences exceed typical lengths
\end{itemize}

\subsubsection{Sequence-Level Importance Sampling}

\ac{GSPO}'s key innovation lies in defining importance ratios at the sequence level rather than token level.
For a response $y$ to query $x$, the sequence-level importance ratio is: \begin{equation}
w(y|x; \theta) = \frac{\pi_\theta(y|x)}{\pi_{\theta_{old}}(y|x)} = \prod_{t=1}^{|y|} \frac{\pi_\theta(y_t|x, y_{<t})}{\pi_{\theta_{old}}(y_t|x, y_{<t})}
\end{equation}

This aligns with the fundamental principle of importance sampling and provides more stable gradient estimates, particularly crucial when responses involve multiple tool invocations with complex dependencies.

\subsubsection{The GSPO Objective}

\ac{GSPO} optimizes the following objective: \begin{equation}
J_{GSPO}(\theta) = \mathbb{E}_{x \sim D, \{y_i\}_{i=1}^G \sim \pi_{\theta_{old}}(\cdot|x)} \left[ \frac{1}{G} \sum_{i=1}^G \min\left( w(y_i|x; \theta) A_i, \text{clip}(w(y_i|x; \theta), 1-\epsilon, 1+\epsilon) A_i \right) \right]
\end{equation}

where advantages $A_i$ are computed as normalized rewards across the group: \begin{equation}
A_i = \frac{r(x, y_i) - \mu_r}{\sigma_r}
\end{equation}

The critical difference is that clipping, rewarding, and optimization all operate at the sequence level, maintaining consistency throughout the learning process.

\subsubsection{Advantages for Multi-Turn Tool Use}

For our coding agent training scenario, \ac{GSPO} provides several crucial benefits:

\textbf{Numerical Stability}: Sequence-level operations are more robust to the logit discrepancies between vLLM inference and training computation, preventing the accumulation of numerical errors that plague token-level methods.

\textbf{Coherent Tool Sequences}: By treating entire tool-use sequences as atomic units, \ac{GSPO} better preserves the logical flow of multi-step debugging processes, avoiding the fragmentation that can occur with token-level optimization.

\textbf{Simplified Infrastructure}: \ac{GSPO}'s design eliminates several complex stabilization strategies required by \ac{GRPO}, reducing the engineering overhead of our training pipeline.

\textbf{MoE Compatibility}: The Qwen team demonstrated that \ac{GSPO} inherently stabilizes Mixture-of-Experts training without additional interventions, opening possibilities for scaling to larger, more efficient architectures.

Empirical results from the Qwen3 models show that \ac{GSPO} not only stabilizes training but also achieves superior final performance compared to \ac{GRPO}, validating its theoretical advantages in practice.
For our work, adopting \ac{GSPO} represents a critical engineering decision that enables stable, long-duration training of coding agents through thousands of multi-turn interactions.

\section{Low-Rank Adaptation (LoRA)}

Parameter-efficient fine-tuning has become essential for adapting large language models to specific tasks without the computational overhead of full fine-tuning.
Low-Rank Adaptation (\ac{LoRA}) represents one of the most successful approaches in this domain.

\subsection{Mathematical Foundation}

\ac{LoRA} is based on the hypothesis that weight updates during fine-tuning have low intrinsic rank.
For a pre-trained weight matrix $W_0 \in \mathbb{R}^{d \times k}$, \ac{LoRA} represents the update $\Delta W$ as:

\begin{equation}
\Delta W = BA
\end{equation}

where $B \in \mathbb{R}^{d \times r}$ and $A \in \mathbb{R}^{r \times k}$ with rank $r \ll \min(d,k)$.
The adapted weight becomes:

\begin{equation}
W = W_0 + \Delta W = W_0 + BA
\end{equation}

During training, $W_0$ remains frozen while only $A$ and $B$ are updated.
This dramatically reduces the number of trainable parameters from $dk$ to $r(d+k)$.

\subsection{Implementation Details}

\subsubsection{Initialization Strategy}

\ac{LoRA} uses a specific initialization scheme to ensure training stability:
\begin{itemize}
	\item Matrix $A$ is initialized using random Gaussian values
	\item Matrix $B$ is initialized to zero, ensuring $\Delta W = BA = 0$ at the start
	\item This guarantees that the adapted model initially behaves identically to the pre-trained model
\end{itemize}

\subsubsection{Scaling Factor}

The \ac{LoRA} update is typically scaled by a factor $\alpha/r$ where $\alpha$ is a hyperparameter:

\begin{equation}
W = W_0 + \frac{\alpha}{r}BA
\end{equation}

This scaling allows for consistent learning rates across different rank values and provides a simple way to control adaptation strength.

\subsection{Computational Advantages}

\ac{LoRA} offers substantial practical benefits for \ac{RL} training:

\textbf{Memory Efficiency}: With typical rank values $r = 8$ to $64$, \ac{LoRA} reduces trainable parameters by 99\%+ for large models, dramatically lowering \ac{GPU} memory requirements.

\textbf{Training Speed}: Fewer parameters mean faster gradient computation and reduced communication overhead in distributed training setups.

\textbf{Storage Efficiency}: \ac{LoRA} adapters are small (typically <100MB vs multi-GB full models), enabling efficient storage and distribution of task-specific adaptations.

\textbf{Modularity}: Multiple \ac{LoRA} adapters can be trained for different tasks and dynamically loaded, enabling flexible model deployment.

\subsection{Integration with Reinforcement Learning}

\ac{LoRA}'s benefits become particularly pronounced in \ac{RL} settings where multiple model instances must be maintained:

\textbf{Policy-Reference Separation}: \ac{RL} algorithms like \ac{PPO} require keeping both current and reference policies.
With \ac{LoRA}, the reference policy can share the frozen base weights while only the adapter differs.

\subsection{Theoretical Considerations}

Recent research has examined \ac{LoRA}'s representational capacity and limitations:

\textbf{Expressiveness}: While \ac{LoRA} cannot represent arbitrary weight updates, empirical evidence suggests that many fine-tuning scenarios do indeed have low-rank structure, making \ac{LoRA}'s constraints reasonable.

\textbf{Task Transfer}: \ac{LoRA} adapters learned for related tasks can serve as initialization for new tasks, potentially accelerating learning through transfer.

\textbf{Rank Selection}: Choosing appropriate rank values requires balancing expressiveness against efficiency.
Higher ranks provide more flexibility but reduce computational savings.

The combination of \ac{LoRA}'s efficiency with online \ac{RL} creates opportunities for more extensive experimentation and deployment of coding agents across diverse software engineering contexts.

\section{Tool-Calling in Large Language Models}

The ability for language models to interact with external tools and \acp{API} represents a fundamental shift from purely generative models to agentic systems capable of performing complex, multi-step tasks in real environments.

\subsection{Stuctured output Calling}

Modern \acp{LLM} implement tool calling through structured output generation, where models learn to produce specially formatted function calls that can be parsed and executed by external systems.
This capability emerged from training models on datasets containing examples of tool usage patterns, enabling them to understand when and how to invoke external functions.

The dominant paradigm uses \acs{JSON}-formatted function calls embedded within model outputs:

\begin{verbatim}
{
  "function_name": "apply_patch",
  "arguments": {
    "file_path": "src/utils.py",
    "old_content": "def buggy_function():",
    "new_content": "def fixed_function():"
  }
}
\end{verbatim}

This approach offers several advantages: (1) structured parsing that reduces ambiguity, (2) type safety through schema validation, and (3) compatibility with existing \ac{API} frameworks.

\subsubsection{Training for Tool Use}

Tool-calling capabilities are typically acquired through multi-stage training:

\textbf{Function Schema Learning}: Models learn to understand function signatures, parameter types, and expected behaviors through exposure to \ac{API} documentation and usage examples.

\textbf{Execution Context Modeling}: Training includes examples of function calls with their subsequent outputs, enabling models to predict the effects of tool usage and plan multi-step interactions.

\textbf{Error Handling}: Models learn to interpret tool execution results, handle failures gracefully, and adapt their strategies based on feedback.

\subsection{Implications for Agent Training}

Tool calling fundamentally changes the \ac{RL} training dynamics by introducing:

\textbf{Discrete Action Spaces}: Unlike continuous text generation, tool calls represent discrete actions with clear semantic meanings, simplifying reward attribution and policy learning.

\textbf{Environmental Feedback}: Tool execution provides immediate, structured feedback that complements language-based responses, enabling richer training signals.

\textbf{Compositional Reasoning}: Models must learn to combine multiple tool calls into coherent strategies, developing higher-level planning capabilities beyond single-step generation.

The integration of tool calling with \ac{RL} training enables models to learn not just what tools to use, but when and how to use them effectively within complex problem-solving workflows.

\section{Sampling Strategies}:
\todoinline{Is it worthwhile to drag LLM sampling parameters into the mix and tie them to "exploration vs exploitation"?}

\section{Agent-Based Programming Environments}

The integration of language models with dynamic programming environments has opened new possibilities for automated software engineering, enabling models to perform complex, multi-step reasoning tasks.

\subsection{Coding Agent Frameworks}

\todoinline{Make this more about: "Hey these agents exist and are obviously impactful." Then explain that those like Aider are not training compliant (modifies rollout histories)}

% Modern coding agents combine language models with tool access, allowing them to navigate codebases, execute commands, and iteratively refine solutions.

% Several sophisticated coding agent frameworks have emerged, each representing different philosophies in agent design and human-\ac{AI} collaboration:

% \textbf{GitHub Copilot Workspace}~\cite{copilotWorkspace2024} extends the original Copilot paradigm beyond code completion to full task planning and execution.
% It employs a multi-agent architecture where specialized agents handle different aspects of software development—understanding requirements, generating implementations, and managing version control.
% On proprietary evaluations, it achieves approximately 30\% task completion rates for well-specified programming tasks.

% \textbf{Cursor}~\cite{cursor2024} pioneered the \ac{IDE}-integrated agent approach, providing context-aware code generation within the development environment.
% Its key innovation lies in automatic context retrieval, using embedding-based search to identify relevant code sections across large codebases.
% This approach achieves strong performance on code completion tasks but has not been formally evaluated on repair benchmarks.

% \textbf{Aider}~\cite{aider2024} represents the heavyweight scaffolding approach, incorporating sophisticated features:
% \begin{itemize}
% 	\item \textit{Repository mapping}: Automatic generation of codebase summaries and dependency graphs
% 	\item \textit{Multi-file coordination}: Tracking and maintaining consistency across file edits
% 	\item \textit{Conversation memory}: Maintaining context across multiple interaction turns
% 	\item \textit{Git integration}: Automatic commit generation and version management
% \end{itemize}

% \textbf{OpenHands (formerly OpenDevin)}~\cite{openhands2024} and \textbf{SWE-Agent}~\cite{sweAgent2024} represent the state-of-the-art in autonomous software engineering agents. These systems provide:
% \begin{itemize}
% 	\item Full shell access within sandboxed environments
% 	\item Web browsing capabilities for documentation lookup
% 	\item Persistent workspace management across sessions
% 	\item Sophisticated error recovery and retry mechanisms
% \end{itemize}

\textbf{Fitting model to scaffold vs Fitting scaffold to model}
\todoinline{sloppy}
\todoinline{Claude Code should illustrate the power of not only making your models good, but making them excel in the environment in which they are deployed (probably trained internally with something highly similar to the Claude Code we use.)}
\todoinline{This is a recurring theme in the industry, Claude -> Claude Code, o3 -> codex, Gemini -> Gemini \ac{CLI}, Qwen3 coder -> Qwen Coder, etc.}

Most existing systems operate exclusively at inference time—they provide scaffolding and tools to pre-trained language models but do not integrate these capabilities into the training process.
This creates a fundamental limitation:

\textbf{Static tool usage patterns}: Models must adapt to tools they were never trained to use, leading to suboptimal usage patterns and frequent errors in tool invocation.

\textbf{Lack of co-adaptation}: The model cannot learn to leverage specific tool capabilities or develop strategies tailored to the available action space.

\textbf{No reinforcement of successful patterns}: When an agent successfully completes a task using a particular tool sequence, this success does not improve future performance—each task starts from scratch.

\textbf{Misaligned representations}: Pre-trained models develop representations optimized for text generation, not for planning tool-based interactions in structured environments.

Integrating agent scaffolding directly into the reinforcement learning training loop.
This enables:

\begin{itemize}
	\item \textbf{Learned tool usage}: Models develop strategies for when and how to use available tools through trial and error
	\item \textbf{Co-evolution}: Model parameters and action policies evolve together, creating tight integration
	\item \textbf{Experience-based improvement}: Successful debugging patterns are reinforced, building expertise over time
	\item \textbf{Emergent strategies}: Models can discover novel tool combinations and workflows not anticipated by designers
\end{itemize}

This training-time integration represents the core innovation of our approach, potentially bridging the gap between current agent capabilities and human-level performance on complex software engineering tasks.

\section{Evaluation Benchmarks}

Rigorous evaluation of automated code repair systems requires diverse, realistic benchmarks that capture the complexity of real-world debugging scenarios.

\subsection{SWE-Bench Family}

The \ac{SWE-Bench} benchmark series has emerged as the gold standard for evaluating coding agents on realistic software engineering tasks.

\subsection{TauBench}

\subsection{}

% \subsubsection{Evolution of SWE-Bench}

% \textbf{Original \ac{SWE-Bench} (2023)}~\cite{jimenez2024swebenchlanguagemodelsresolve}: The foundational dataset introduced 2,294 task instances drawn from 12 popular Python repositories. Each instance consists of:
% \begin{itemize}
% 	\item A GitHub issue describing a bug or feature request
% 	\item The repository state at issue creation time
% 	\item The developer-written patch that resolved the issue
% 	\item Test cases that fail before and pass after the patch
% \end{itemize}

% This design captures the full complexity of real software development: understanding natural language descriptions, navigating large codebases, and implementing solutions that satisfy existing tests.

% \textbf{\ac{SWE-Bench-Verified} (2024)}~\cite{sweBenchVerified2024}: Addressing quality concerns in the original dataset, this refined version includes 500 carefully validated instances that:
% \begin{itemize}
% 	\item Eliminate ambiguous issue descriptions
% 	\item Ensure deterministic test outcomes
% 	\item Remove trivial string replacements
% 	\item Verify patch minimality and correctness
% 	\item Balance difficulty across different problem types
% \end{itemize}

% Human annotators achieve 97\% success on \ac{SWE-Bench-Verified} compared to 73\% on the original, confirming the removal of problematic instances while maintaining challenging, realistic tasks.

% \textbf{\ac{SWE-Bench}
% 	Extensions}: While extensions to other programming languages have been proposed, our work focuses exclusively on the original Python-based benchmark suite, which provides the most mature and extensively validated evaluation framework.

% This diversity demonstrates the complexity and realism of the evaluation framework.

% \subsubsection{Task Format and Complexity}

% Each \ac{SWE-Bench} instance presents a naturalistic debugging scenario:

% \begin{verbatim}
% ISSUE DESCRIPTION:
% Title: DataFrame.apply() fails with axis=1 when columns have mixed types

% When calling df.apply(func, axis=1) on a DataFrame with both numeric 
% and string columns, the function receives Series with incorrect dtypes.

% Example to reproduce:
% ```python
% df = pd.DataFrame({'A': [1, 2], 'B': ['x', 'y']})
% df.apply(lambda row: row['A'] + len(row['B']), axis=1)
% # Raises TypeError
% ```

% REPOSITORY STATE:
% - 847 Python files totaling 284,000 lines
% - Complex module dependencies
% - 15,000+ test cases
% \end{verbatim}

% The evaluation system places the model in this repository state and measures whether it can produce a patch functionally equivalent to the developer's solution.
% This requires:
% \begin{itemize}
% 	\item Understanding the issue from natural language description
% 	\item Reproducing the bug through code execution
% 	\item Navigating the codebase to locate relevant modules
% 	\item Understanding existing implementation patterns
% 	\item Implementing a fix that maintains backward compatibility
% 	\item Ensuring the fix passes all existing tests
% \end{itemize}

% \subsubsection{Evaluation Methodology}

% \ac{SWE-Bench} employs rigorous evaluation protocols:

% \textbf{Functional verification}: Patches are evaluated by running the full test suite, not through textual comparison.
% This allows semantically equivalent but syntactically different solutions.

% \textbf{Isolated execution}: Each evaluation runs in a fresh Docker container to prevent cross-contamination and ensure reproducibility.

% \textbf{Time limits}: Solutions must complete within 5 minutes, reflecting real-world constraints on automated tools.

% \textbf{Minimal patches}: Credit is given only for patches that don't introduce unnecessary changes, encouraging precise solutions.

% \subsubsection{Why Success Rates Remain Low}

% Despite rapid progress in \ac{LLM} capabilities, even state-of-the-art systems achieve only ~20-25\% success rates on \ac{SWE-Bench}.
% This persistent challenge stems from several factors:

% \textbf{Repository complexity}: The average task requires understanding code distributed across 6.3 files, with deep call chains and complex dependencies.
% Current models struggle to maintain coherent understanding across such scales.

% \textbf{Ambiguity in natural language}: Issue descriptions often assume domain knowledge, use project-specific terminology, or describe symptoms rather than root causes.
% Models must infer substantial context.

% \textbf{Execution feedback requirement}: Unlike code generation tasks solvable through pattern matching, debugging requires iterative hypothesis testing through code execution—a capability most models lack.

% \textbf{Test suite complexity}: Solutions must satisfy not just the reported issue but maintain compatibility with thousands of existing tests, requiring deep understanding of system invariants.

% \textbf{Long-tail distribution}: Many bugs involve rare edge cases or unique project-specific patterns absent from training data, testing true generalization rather than memorization.

% These challenges make \ac{SWE-Bench} an ideal testbed for online training, where models can learn through environmental interaction rather than attempting single-shot solutions to complex, multi-faceted problems.

\section{Related Work}

This section synthesizes prior research most directly relevant to our approach, highlighting gaps that this thesis addresses.

This section synthesizes the most directly relevant prior work across three critical dimensions, highlighting the significant gaps our research addresses.

\subsubsection{Reinforcement Learning for Code Tasks}

\textbf{CodeRL}~\cite{le2022coderlmasteringcodegeneration} pioneered applying \ac{RL} to code generation, using unit test execution as rewards.
However, it focused on single-function synthesis rather than repository-scale debugging.

\textbf{PPOCoder}~\cite{shojaee2023executionbasedcodegenerationusing} extended CodeRL with proximal policy optimization but maintained the limitation of single-file, single-step generation.
The work demonstrated improved sample efficiency but did not address the fundamental mismatch between training and real-world debugging workflows.

% \textbf{RLTF}~\cite{liu2023rltfreinforcementlearningunit} (Reinforcement Learning from Unit Test Feedback) introduced more sophisticated reward shaping using test coverage and execution traces.
% While innovative, it still operated on isolated functions without repository context or tool usage.

\textbf{SWE-RL}~\cite{wei2025swerladvancingllmreasoning} introduced a static, single-turn formulation—where a model given full file contexts produces a patch after a reasoning step.
\todoinline{elaborate}

\textbf{DeepSWE}~\cite{deepswe2025} is highly similar to our work and validates our approach.
I.e. they do GRPO on multi-turn, terminal using coding agents with an async inference engine setup.
They use 64 H100s for training, 8 H100s for inference and trained for 6 days on R2EGym, in total ~$10368$ H100 hours.
In contrast, we train for ~2 days on 6 A100s ~$288$ A100 hours, and by normalizing by throughput between these two we can safely assume that $1\text{H100 hour} \approx 2.2\text{A100 hours}$ so: we use $~80$x less compute.
They used test based rewards.
Our novelty is being way more compute efficient and using test-free rewards, meaning we can trivially extend our method to any programming language.
\todoinline{important to make this much better}

\section{Limitations of existing work}:
\textbf{Focus on generation rather than repair}: Existing approaches predominantly emphasize code generation, yet creating new code differs fundamentally from the process of debugging and repairing existing systems.
This distinction is crucial, as effective automated program repair requires models to reason about and modify complex, pre-existing codebases rather than synthesizing code from scratch.

\textbf{Isolated task formulation}: Many prior works formulate tasks at the level of single functions, thereby neglecting the intricate interdependencies and architectural complexity inherent in real-world software systems.
This simplification limits the ability of models to generalize to practical debugging scenarios, where understanding interactions across multiple files and modules is essential.

\textbf{Lack of environmental interaction}: Previous methods typically do not incorporate exploration, tool usage, or iterative refinement into their training regimes.
As a result, models are deprived of opportunities to learn through active engagement with their environment, which is a key aspect of effective debugging and repair in realistic settings.

\textbf{Dependence on test-based rewards}: While relying on test-based rewards can, in principle, provide robust supervision, this strategy becomes increasingly impractical as the number of supported programming languages expands.
Constructing and maintaining comprehensive test suites for each language introduces significant operational challenges, thereby limiting the scalability and applicability of these approaches.

\section{Summary and Research Positioning}

This chapter has established the theoretical and empirical foundations for online \ac{RL} applied to \ac{APR}.
By examining the evolution from rule-based repair systems to modern \ac{LLM}-based approaches, we identified fundamental limitations in current paradigms: the mismatch between static training and the dynamic nature of debugging.

Our review of reinforcement learning techniques, particularly the elegant simplification offered by \ac{GRPO}, demonstrates how computational efficiency can be achieved without sacrificing theoretical guarantees.
The examination of existing coding agents reveals a critical gap—all current systems operate at inference time only, missing the opportunity for models to learn optimal tool usage through experience.

The Nano agent architecture presents an alternative approach to agent design.
By providing only essential tools and relying on learning, we demonstrate that effective debugging capabilities can emerge from simple interfaces.

This thesis makes three fundamental contributions to the field:

\textbf{One of the First open-source online RL implementation for \acp{LLM}}: We provide the research community with complete infrastructure for training language models through interactive environmental experience.
This democratizes access to techniques we believe are already employed by leading industry labs but have remained proprietary.
Our implementation enables researchers to explore online learning across diverse domains beyond code repair.

\textbf{Minimalist agent approach}: We demonstrate that a simple agent design with basic tools can achieve strong performance when combined with reinforcement learning, showing that sophisticated debugging behaviors can emerge from minimal interfaces.

\textbf{Bridge between research and practice}: By demonstrating that models can learn effective debugging strategies through \ac{RL} in realistic environments, we bridge the gap between academic research on program repair and practical tools used by developers.
Our approach shows that the future of coding assistance lies not in ever-more-sophisticated prompt engineering, but in models that learn from experience.

As we proceed to describe our method in detail, keep in mind that our goal is not merely to improve performance metrics on benchmarks, but to demonstrate a fundamentally different, nascent training approach that enables \acp{LLM} to learn not from labeled data points, but from experience.
