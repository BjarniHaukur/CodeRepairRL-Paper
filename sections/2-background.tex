\chapter{Theoretical Background and Related Work}
\label{ch:background}

This chapter establishes the theoretical and empirical background for online \ac{RL} applied to \ac{APR} with tool-using \acp{LLM}.
We first situate \ac{APR} historically, from search- and learning-based methods before \acp{LLM} to contemporary, repository-grounded systems, and introduce a simple taxonomy spanning manual, agentless, and agentic workflows.
We then formalize agentic repair as a history-based \ac{MDP} driven by structured tool calls, and review optimization methods for training such policies, including \ac{PPO}, \ac{GRPO}, and \ac{GSPO}, as well as practical issues such as variance collapse.
Complementary techniques for scalable training (parameter-efficient adaptation via \ac{LoRA} and sampling choices that balance exploration and exploitation) are summarized alongside evaluation resources such as \ac{SWE-Bench} and \ac{TauBench}.
We close with related work and limitations that motivate the minimalist agent and online training recipe developed in the following chapters.

\section{Automated Program Repair before LLMs}\label{sec:apr-pre-llm}

Prior to \acp{LLM}, empirical research in \ac{APR} centered on benchmarks that provide localized bugs with executable tests, typically single files or short patch sets.
Standard datasets include Defects4J for Java~\cite{defects4J2014}, QuixBugs for small algorithmic programs in Java and Python~\cite{quixBugs2017}, and the recent GitBug-Java, which introduces up-to-date, reproducible bugs~\cite{gitbugJava2024}.
These datasets set the scope of most methods: the system observes a compact fault context and must propose a patch that satisfies the test suite.

Within this setting, three families of approaches emerged.
Generate-and-validate search explores a mutation space and accepts candidates that pass tests, with GenProg as a canonical example and PAR illustrating pattern-guided patching~\cite{genProg2012,par2013}.
Semantics-based repair uses symbolic execution and constraint solving to synthesize expressions at instrumented locations, exemplified by SemFix~\cite{semFix2013}.
Early learning-based methods incorporate statistical guidance to prioritize candidates; Prophet learns a patch ranking model from past fixes~\cite{prophet2016}.
Together, these lines established feasibility and clarified limitations of test adequacy as a correctness proxy, motivating the move to richer, repository-grounded settings reviewed next.

\section{Automated Program Repair with LLMs}
\label{sec:apr-with-llms}

Codex marked an inflection point where large pretrained models on code enabled practical repair and synthesis, shifting \ac{APR} from rules or search to learned edit priors~\cite{chen2021evaluatinglargelanguagemodels2021}.
Since then, frontier labs have converged on repository-grounded evaluation (e.g., \ac{SWE-Bench} Verified) and increasingly ship first-party \emph{agent harnesses} such as OpenAI's Codex \ac{CLI}, Anthropic's Claude Code, and Google's Gemini \ac{CLI} that orchestrate tool calls over live repositories~\cite{sweBench2024,sweBenchVerified2024,openaiCodexCLIRepo,claudeCode,geminiCLIRepo}.

We therefore organize \ac{LLM}-assisted repair by the degree of autonomy at inference: (i) \emph{scaffold-free} models operating without an execution harness, (ii) \emph{agentless} scaffolds that curate repository context without centralized autonomy, and (iii) \emph{agentic} systems that plan and execute tools end-to-end.
This taxonomy clarifies which capabilities are evaluated and how results transfer across settings.

\subsection{Scaffold-free}\label{subsec:scaffold-free}
Scaffold-free approaches frame repair as translation from buggy code to fixed code.
The model receives a localized and self-contained context, typically a snippet or a single file with a narrow edit window, and proposes a patch in one interaction turn.
There is no repository traversal, no tool use during generation, and no execution signal in the loop.
Any tests are run outside the model after the fact.

Within this regime we focus on post-training methods that adapt an existing \ac{LLM} for repair.
A representative example is \emph{RepairLLaMA}, which fine-tunes a general code model on curated bug-fix pairs so that it proposes minimal and targeted edits without relying on an execution harness~\cite{repairllama2023}.
A parallel thread brings \ac{RL} into the same scaffold-free setting at the function or snippet level.
\emph{CodeRL} optimizes code generation with unit test execution as feedback while operating on localized contexts \cite{le2022coderlmasteringcodegeneration}.

These setups are attractive because data and pipelines are simple, adaptation is straightforward, and inference is fast.
It provides an effective baseline for measuring translation quality in localized contexts.
Its limitations follow from the design: without an execution loop the model cannot test hypotheses, observe runtime behavior, or coordinate edits across files.
Repository state and build systems remain external to generation.

\subsection{Agentless}\label{subsec:agentless}
Moving beyond scaffold-free translation, \emph{agentless} systems typically operate over a live repository while keeping execution outside the model.
A thin scaffold retrieves relevant files, builds diffs, and structures the prompt; the model returns patch blocks or edit intents; a human or scripted harness runs tests and shell commands and applies the edits.
This preserves repository grounding and repeatability without autonomous tool use.

We treat agentless systems as interfaces rather than agents: they make the contributions of context curation and patch formatting explicit and easy to ablate.
In practice, components inside the scaffold (retrievers, rankers, patch validators) can be trained, and logs from these pipelines support offline learning or imitation without exposing the model to a full tool loop.
Representative analyses show that a simple three-stage pipeline (localize, repair, validate) can match or exceed more complex setups when the scaffold is well-engineered~\cite{xia2024agentlessdemystifyingllmbasedsoftware}.
Related work begins to introduce reinforcement signals in repository settings while still short of a fully autonomous terminal loop, e.g., SWE-RL for repository-level reasoning~\cite{wei2025swerladvancingllmreasoning}.

The limitation is structural.
Because execution remains outside the model, the scaffold decides what to surface, when to run tests, and how to apply or revert edits.
The model does not learn the decision-making that drives repair (what to try next, when to backtrack, when to submit or abandon a patch) because those choices live in the interface rather than in the policy.
The reward signal therefore entangles scaffold choices with model output, and the resulting histories are not suitable for end-to-end training.
Agentless setups are excellent for reproducible evaluation and for collecting logs, but they are a poor fit when the goal is to train a model to use tools by itself.

\todoinline{State that these methods often use LLMs as "functions" in these systems instead of end-to-end processes, meaning conversation histories are modified / restarted frequently which makes end-to-end learning hard / infeasible}

\subsection{Agentic}\label{subsec:agentic}

Agentic systems operate directly over a live repository and let the model initiate actions.
The model can open and edit files, run tests and shell commands, read raw outputs, and iterate until a patch is ready.
Public research harnesses make this concrete: \emph{SWE-Agent} formalizes an agent-computer loop for software tasks, \emph{OpenHands} provides a general terminal-based platform for coding agents, and \emph{mini-swe-agent} shows that a small, disciplined tool set in a tight loop is effective~\cite{sweAgent2024,openhands2024, miniSWEAgentRepo}.
Our Nano agent (\cref{sec:nano-agent}) adopts a similar minimalist philosophy, providing only essential terminal commands and file patching capabilities, allowing effective debugging behaviors to emerge through \ac{RL}.

Operationally, these systems rely on structured tool calls, typically via \acs{JSON}.
The model emits a well-formed function call with arguments, an external executor runs it, and the resulting output is returned to the model.
\begin{verbatim}
{
  "function_name": "apply_patch",
  "arguments": {
    "file_path": "src/utils.py",
    "old_content": "def buggy_function():",
    "new_content": "def fixed_function():"
  }
}
\end{verbatim}

This approach transforms \acp{LLM} from passive text generators into systems capable of interacting with their environment by invoking tools and observing their results.

This shift has also shaped how models are packaged and evaluated: recent open-weight releases pair model checkpoints with first-party harnesses and report agent-style evaluations, while disclosing only partial training details.
We summarize representative examples.

\paragraph{Qwen3\textendash Coder (Alibaba/Qwen team).}
Qwen3\textendash Coder is positioned as an \emph{agentic} coding model and ships with a first-party command-line harness (\emph{Qwen Code}) that drives repository-grounded autonomous workflows~\cite{qwen3CoderBlog2025,qwenCodeCLI}.
The release materials emphasize agent behavior in realistic development loops and demonstrate how the paired harness surfaces those capabilities.
At the same time, the post-training recipe—data generation, reward design, and optimization details—is described only at a high level; weights and the harness are available, but a reproducible end-to-end training pipeline is not.

\paragraph{Kimi\textendash K2 (Moonshot AI).}
Kimi-K2 details an \emph{agentic} data synthesis pipeline paired with a joint reinforcement learning stage that mixes real and synthetic environment interactions.
\cite{kimiK2_2025}
The stated goal is to shape multi-step decision making—tool selection, iterative trial-and-error, and revision—rather than single-pass text generation.
The report frames this as necessary for repository-grounded coding and other multi-step tasks.
As with Qwen3, model artifacts and high-level methodology are public, but the full training code and complete RL environment setup are not.

\paragraph{GLM\textendash 4.5 (Zhipu AI).}
GLM-4.5 presents a hybrid reasoning MoE with dedicated ``thinking'' and tool-use modes, and explicitly reports a comprehensive post-training phase combining expert iteration and reinforcement learning.
\cite{glm45_2025}
The release includes open weights and inference code as well as pointers for SFT/RL fine-tuning with common frameworks; however, a complete, reproducible agentic RL pipeline with environment traces is not provided.

Taken together, these open-weight efforts converge on the same direction: agent-facing post-training that couples tool use, long-context coding, and multi-turn control.
They document RL or agent-based post-training and evaluate on agentic tasks, but stop short of releasing fully reproducible training stacks (task generators, executors, rewarders, roll-out logging, and optimization code).
In contrast, recent work such as \emph{DeepSWE} publicly reports an RL-trained coding agent within a terminal environment and releases open-weight models alongside practical training details, offering a rare end-to-end reference point for open agentic RL on software tasks.
\cite{deepSWE2025}

In practical terms, agentic systems close the loop between hypothesis and verification: the model requests an action, observes the real outcome, and can adjust.
This makes online \ac{RL} feasible and aligns the learning signal with the deployed behavior.
By contrast with agentless setups, the model is exposed to the full decision process of repair rather than a curated slice, so the resulting histories are suitable for end-to-end training.

Having established how agentic systems enable end-to-end learning, we now turn to the theoretical foundations of training such systems through reinforcement learning.

\section{\acl{RL} for Language Models}
\label{sec:rl-language-models}

\ac{RL} has emerged as a powerful technique for improving language model performance beyond what supervised learning alone can achieve, particularly for tasks requiring sequential decision-making and optimization of complex objectives.
Unlike supervised learning, which relies on fixed input-output pairs, \ac{RL} enables models to learn through interaction with dynamic environments, receiving rewards based on the quality of their generated outputs.

\subsection{History-Based \ac{MDP}
Formulation} \label{subsec:mdp-history}

We model interactive code repair as a history-based \ac{MDP} where time indexes executed tool calls rather than individual tokens.
The environment state (repository, filesystem) is never exposed directly; it influences the process only through tool outputs that are appended to the conversation transcript.

\paragraph{\ac{MDP} tuple}
Let $\mathcal{M} = (\mathcal{H}, \mathcal{Y}, \mathcal{P}, r, \gamma)$ be the \ac{MDP}.
Here $\mathcal{H}$ is the set of text histories, and $\mathcal{Y}$ is the set of assistant-authored chunks that contain at least one well-formed tool call.
The transition kernel $\mathcal{P}$ maps a current history and an assistant chunk to a distribution over next histories after executing the tool call and appending its output.
We use $\gamma=1$ for finite episodes with terminal returns.

\paragraph{State, action, observation}
At step $t$ the state is the transcript prefix $h_t \in \mathcal{H}$, which contains the initial instruction and all prior assistant messages and tool outputs.
The policy $\pi_\theta(\cdot \mid h_t)$ produces an assistant chunk $y_t \in \mathcal{Y}$.
A deterministic parser $\psi:\mathcal{Y}\to\mathcal{C}$ extracts the first tool call $c_t = \psi(y_t)$, where $\mathcal{C}$ is the space of executable tool calls.
The environment executes $c_t$ and returns textual output $o_t \in \text{Text}$.
The next state is the history with both the assistant chunk and the tool output appended: $h_{t+1} = \text{append}(h_t, y_t, o_t)$.

\paragraph{Trajectory and termination}
A trajectory of length $T$ is the finite sequence $\tau = \{(h_t, y_t, c_t, o_t)\}_{t=0}^{T-1}$ with terminal transcript $h_T$.
Episodes terminate on success or when resource budgets are exhausted.

\paragraph{Return}
Intermediate rewards are zero.
A scalar terminal return is computed from the terminal transcript, $R(\tau) \equiv R(h_T) \in \mathbb{R}$, and the objective is to maximize the expected return over trajectories induced by the policy: \begin{equation}
J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}[ R(\tau) ]
\end{equation}

\begin{table}[ht]
\centering
\caption{Notation for history-based MDP formulation}
\label{tab:mdp-notation}
\begin{tabular}{ll}
\toprule
Symbol & Meaning \\
\midrule
$\mathcal{H}$ & Space of text histories (conversation transcripts) \\
$\mathcal{Y}$ & Space of assistant chunks with at least one tool call \\
$\mathcal{C}$ & Space of executable tool commands \\
$h_t$ & Transcript prefix before step $t$ \\
$y_t$ & Assistant chunk produced at step $t$ \\
$\psi$ & Parser extracting the first tool call from $y_t$ \\
$c_t$ & The first extracted tool call, $c_t=\psi(y_t)$ \\
$o_t$ & Textual output returned by executing $c_t$ \\
$R(\tau)$ & Terminal return computed from final transcript $h_T$ \\
$\pi_\theta$ & Policy mapping $h_t$ to distribution over $y_t$ \\
$G$ & Group size for batched advantage estimation in GRPO \\
$A_t$ & Advantage at time $t$, measuring relative action quality \\
\bottomrule
\end{tabular}
\end{table}

The repository and all workspace details remain part of the hidden environment state.
They appear only through tool outputs $o_t$ that are appended to the history, so no separate symbol for the repository is required in this formulation.
This abstraction elegantly captures the partial observability inherent in tool-augmented language models while maintaining mathematical rigor.

\todoinline{Clarify what online learning is, in particular for LLMs}

\subsection{Policy Gradient Foundations}
\label{subsec:policy-gradient}

Building on the history-based MDP formulation, the policy gradient theorem provides the foundation for optimization:

\begin{equation}
J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}[R(\tau)]
\end{equation}

where the expectation is over trajectories $\tau = \{(h_t, y_t, c_t, o_t)\}_{t=0}^{T-1}$ sampled from the policy $\pi_\theta$.
Since rewards are terminal, the objective simplifies to maximizing the expected return $R(h_T)$ of the final transcript.

The policy gradient theorem gives: \begin{equation}
\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}[\sum_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(y_t|h_t) R(\tau)]
\end{equation}

However, this basic REINFORCE estimator suffers from high variance, making training unstable and sample-inefficient.
This limitation becomes particularly problematic for language models, where sequence lengths can be substantial and reward signals are often sparse.

\subsection{Proximal Policy Optimization (PPO)}
\label{subsec:ppo}

Proximal Policy Optimization~\cite{schulman2017proximalpolicyoptimizationalgorithms} has emerged as the dominant algorithm for fine-tuning large language models due to its ability to stabilize training while maintaining sample efficiency.
\ac{PPO} addresses the fundamental challenge of policy optimization: making meaningful progress without taking overly large steps that destabilize learning.

\subsubsection{The Clipping Mechanism}

\ac{PPO} introduces a clipped objective function that prevents destructive policy updates: \begin{equation}
L^{CLIP}(\theta) = \mathbb{E}_t\left[\min\left(r_t(\theta)\hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t\right)\right]
\end{equation} where: \begin{align}
r_t(\theta) &= \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)} \\
\hat{A}_t &= R_t - V(s_t)
\end{align}

The ratio $r_t(\theta)$ measures how much the current policy differs from the previous policy, while $\hat{A}_t$ represents the advantage estimate computed using a value function $V(s_t)$.
The clipping parameter $\epsilon$ (typically 0.2) constrains policy updates to prevent catastrophic changes.

\subsubsection{Value Function Training}

\ac{PPO} employs a separate value network $V_\phi(s)$ trained to predict expected returns, enabling more accurate advantage estimation: \begin{equation}
L^{VF}(\phi) = \mathbb{E}_t\left[(V_\phi(s_t) - R_t)^2\right]
\end{equation}

The complete \ac{PPO} objective combines policy and value losses: \begin{equation}
L(\theta, \phi) = L^{CLIP}(\theta) - c_1 L^{VF}(\phi) + c_2 S[\pi_\theta](s_t)
\end{equation} where $S[\pi_\theta]$ is an entropy bonus encouraging exploration, and $c_1, c_2$ are weighting coefficients.

\subsubsection{Success in Language Model Fine-tuning}

\ac{PPO}'s success in language model applications, particularly in \ac{RLHF}, stems from several key properties:

\textbf{Stable Learning}: The clipping mechanism prevents the policy from changing too rapidly, which is crucial when fine-tuning large pre-trained models where dramatic changes can destroy learned representations.

\textbf{Sample Efficiency}: By reusing data for multiple gradient steps and employing importance sampling correction, \ac{PPO} achieves better sample efficiency than simpler policy gradient methods.

\textbf{Scalability}: \ac{PPO}'s architecture separates policy and value training, enabling distributed training across multiple \acp{GPU} with different computational loads for each component.

However, \ac{PPO} also introduces significant computational overhead through the separate value network training and the need for multiple gradient updates per batch of experience.

\subsection{Group Relative Policy Optimization (GRPO)}
\label{subsec:grpo}

\ac{GRPO}~\cite{shao2024deepseekmathpushinglimitsmathematical} is fundamentally \ac{PPO} with a crucial simplification: instead of training a separate value network to estimate advantages, \ac{GRPO} computes advantages directly from relative performance within sampled action groups.
This elegant modification preserves \ac{PPO}'s theoretical guarantees while dramatically reducing computational overhead.

\subsubsection{The Core Simplification}

The key insight behind \ac{GRPO} is that for each history $h$, rather than estimating $V(h)$ with a separate network, we can sample multiple assistant responses $y_1, \ldots, y_G$ from the current policy $\pi_{\theta_{\text{old}}}$ and use their reward distribution to compute relative advantages.

For a given history $h$, \ac{GRPO} samples $G$ responses from the policy and computes the group-relative advantage as: \begin{equation}
A_j = \frac{r_j - \mu}{\sigma}
\end{equation} where $\mu$ and $\sigma$ are the mean and standard deviation of the rewards $r_1, \ldots, r_G$ obtained by executing each response.
This is simply the standard score (z-score) of the rewards, providing a normalized measure of relative performance.

Mathematically, this can be expressed as: \begin{align}
\mu &= \frac{1}{G}\sum_{i=1}^G r_i \\
\sigma &= \sqrt{\frac{1}{G}\sum_{i=1}^G (r_i - \mu)^2} \\
A_j &= \frac{r_j - \mu}{\sigma}
\end{align}

\subsubsection{PPO Objective with Group-Relative Advantages}

\ac{GRPO} then maximizes the standard \ac{PPO} objective, but using these group-relative advantages instead of value-network-based estimates.
The objective becomes: \begin{equation}
\max_\theta \frac{1}{G}\sum_{i=1}^G \mathbb{E}_{h \sim \mathcal{H}, \{y_1,\ldots,y_G\} \sim \pi_{\theta_{\text{old}}}(\cdot|h)}\left[
\begin{cases}
\min\left(\frac{\pi_\theta(y_i|h)}{\pi_{\theta_{\text{old}}}(y_i|h)}, 1+\epsilon\right) A_i & \text{if } A_i > 0 \\
\max\left(\frac{\pi_\theta(y_i|h)}{\pi_{\theta_{\text{old}}}(y_i|h)}, 1-\epsilon\right) A_i & \text{if } A_i < 0
\end{cases}
\right]
\end{equation}

This formulation preserves \ac{PPO}'s asymmetric clipping behavior: when advantages are positive (indicating good actions), we clip the importance ratio from above at $(1+\epsilon)$ to prevent over-optimization.
When advantages are negative (indicating poor actions), we clip from below at $(1-\epsilon)$ to avoid excessive penalization.

\subsubsection{Intuitive Understanding}

The intuition behind \ac{GRPO} is elegantly simple: each policy update makes the model more likely to produce actions that performed relatively better than other actions tried at the same state, and less likely to produce actions that performed relatively worse.
This creates a natural competitive dynamic where actions are evaluated against their peers rather than against an absolute baseline.

Consider a concrete example: if for a given coding problem, the model generates five different debugging approaches with rewards $[0.1, 0.8, 0.3, 0.9, 0.2]$, \ac{GRPO} will: \begin{itemize}
	\item Strongly reinforce the action with reward $0.9$ (highest z-score)
	\item Moderately reinforce the action with reward $0.8$ (second highest z-score)
	\item Slightly penalize actions with rewards $0.3, 0.2, 0.1$ (below-average performance)
\end{itemize}

This relative ranking approach is particularly powerful for code repair where absolute reward values may vary significantly across different types of bugs, but relative solution quality within each problem remains meaningful.

\subsubsection{Relationship to PPO}

It's crucial to understand that \ac{GRPO} is not a fundamentally different algorithm from \ac{PPO}. Rather, it is \ac{PPO} with a specific choice of advantage estimation.
The clipping mechanism, importance sampling, and optimization dynamics remain identical.
The only change is replacing: \begin{equation}
\hat{A}_t^{PPO} = R_t - V_\phi(s_t)
\end{equation} with: \begin{equation}
\hat{A}_t^{GRPO} = \frac{r_t - \mu_{\text{group}}}{\sigma_{\text{group}}}
\end{equation}

This substitution eliminates the need for: \begin{itemize}
	\item Training a separate value network $V_\phi$
	\item Computing value loss $L^{VF}(\phi)$
	\item Managing value network hyperparameters
	\item Coordinating policy and value network training schedules
\end{itemize}

\subsubsection{Computational and Practical Advantages}

The computational benefits of \ac{GRPO} are substantial:

\textbf{Memory Efficiency}: Eliminating the value network reduces GPU memory requirements by approximately 50\%, enabling larger batch sizes or model sizes within the same hardware constraints.

% \textbf{Training Simplicity}: The training loop becomes significantly simpler, reducing implementation complexity and potential sources of bugs.
% There are no value network updates to coordinate or balance against policy updates.
% 
% \textbf{Hyperparameter Robustness}: With fewer moving parts, \ac{GRPO} exhibits reduced sensitivity to hyperparameter choices, making it more reliable across different tasks and model architectures.

% \textbf{Batch Processing Efficiency}: \ac{GRPO} can naturally handle variable batch sizes and sequence lengths without the complications introduced by value network training, which often requires careful batch construction.

\subsubsection{Advantages for Code Repair}

% \ac{GRPO}'s design makes it particularly well-suited for code repair applications:

% \textbf{Natural Handling of Sparse Rewards}: Code repair often produces binary success/failure outcomes or sparse quality metrics.
% \ac{GRPO}'s relative comparison approach handles this naturally, as the standard score normalization adapts to the reward distribution within each group.

% \textbf{Problem Diversity}: Different coding problems require vastly different solution approaches and have different inherent difficulty levels.
% \ac{GRPO}'s group-relative baseline automatically adjusts to each problem's context, whereas a global value function would struggle to capture this diversity.

% \textbf{Exploration Encouragement}: By comparing actions against their immediate peers rather than a global baseline, \ac{GRPO} encourages exploration of diverse solution strategies, which is crucial for learning robust debugging skills.

\textbf{Computational Scaling}: Code repair training requires processing thousands of agent interactions across diverse repositories and bug types.
\ac{GRPO}'s computational efficiency makes this scale of training practically feasible.

% The mathematical elegance of \ac{GRPO} lies in its ability to preserve all of \ac{PPO}'s theoretical guarantees while dramatically simplifying the implementation.
% For code repair, where relative solution quality matters more than absolute reward prediction, this approach provides an optimal balance of performance, simplicity, and computational efficiency.

\subsubsection{Variance Collapse: A Fundamental Challenge}

Despite \ac{GRPO}'s computational advantages, it shares with other policy gradient methods a fundamental challenge known as variance collapse or mode collapse~\cite{varianceCollapse2023}.
This phenomenon occurs when the policy gradient optimization inadvertently incentivizes the model to reduce output variance, leading to increasingly deterministic and less exploratory behavior over training iterations.

The mechanism behind variance collapse in \ac{GRPO} can be understood through the optimization dynamics.
When computing group-relative advantages, actions with consistently high rewards relative to their peers receive positive reinforcement, while those with lower relative rewards are penalized.
Over time, this creates a positive feedback loop:

\begin{enumerate}
	\item High-performing actions become increasingly likely
	\item The policy concentrates probability mass on these "safe" actions
	\item Exploration of alternative strategies diminishes
	\item The standard deviation $\sigma$ in the group decreases
	\item Smaller $\sigma$ amplifies advantage magnitudes, accelerating concentration
\end{enumerate}

This collapse is particularly problematic for code repair applications where:

\textbf{Multiple valid solutions exist}: Most bugs can be fixed through various approaches—refactoring the logic, adding error handling, or modifying data structures.
Variance collapse biases the model toward a single approach, potentially missing simpler or more elegant solutions.

\textbf{Exploration enables learning}: Discovering effective debugging strategies requires experimenting with different investigation paths, tool usage patterns, and fix attempts.
Premature convergence prevents the model from discovering these diverse strategies.

\textbf{Robustness requires diversity}: Models that learn only narrow solution patterns fail when encountering bugs requiring different approaches, leading to brittleness in deployment.

Several mitigation strategies have been proposed in the literature:

\textbf{\ac{KL} Divergence Constraints}: A standard stabilization augments the clipped objective with a penalty that constrains divergence from a \emph{frozen} reference policy $\pi_{\text{ref}}$.
In the \ac{GRPO} setting, the optimization can be written as \begin{equation}
    J_{\text{GRPO}}(\theta) = \mathbb{E}_{h \sim \mathcal{H},\; \{y_i\}_{i=1}^{G} \sim \pi_{\theta_{\text{old}}}(\cdot|h)}\left[ \frac{1}{G} \sum_{i=1}^{G} \min\!\left( \frac{\pi_{\theta}(y_i|h)}{\pi_{\theta_{\text{old}}}(y_i|h)} A_i,\; \text{clip}\!\left(\frac{\pi_{\theta}(y_i|h)}{\pi_{\theta_{\text{old}}}(y_i|h)}, 1-\epsilon, 1+\epsilon\right) A_i \right) \right]
    - \beta\, \text{KL}(\pi_{\theta}\,\|\,\pi_{\text{ref}}),
\end{equation} where $\epsilon$ and $\beta$ are hyperparameters and $A_i$ is the group-relative advantage computed from group rewards.
A practical surrogate for the KL term at the sampled output is \begin{equation}
    \text{KL}(\pi_{\theta}\,\|\,\pi_{\text{ref}}) \approx \frac{\pi_{\text{ref}}(y_i|h)}{\pi_{\theta}(y_i|h)} - \log\!\left( \frac{\pi_{\text{ref}}(y_i|h)}{\pi_{\theta}(y_i|h)} \right) - 1,
\end{equation} and, for token-level implementation from logits, with pre-softmax logits $z_{\theta}(h_t)$ and $z_{\text{ref}}(h_t)$, \begin{equation}
    \text{KL}(\pi_{\theta}\,\|\,\pi_{\text{ref}}) = \mathbb{E}_t\left[\text{KL}\left(\sigma(z_{\theta}(h_t))\,\|\,\sigma(z_{\text{ref}}(h_t))\right)\right].
\end{equation}

\subsection{Group Sequence Policy Optimization (GSPO)} \label{subsec:gspo}

While \ac{GRPO} offers computational efficiency over traditional \ac{PPO}, recent work from the Qwen team has identified critical stability issues that become particularly pronounced in multi-turn tool use scenarios~\cite{zheng2025groupsequencepolicyoptimization}.
\ac{GSPO} addresses these fundamental limitations through a theoretically grounded reformulation of the importance sampling mechanism.

\subsubsection{The Stability Problem in GRPO}

The core instability in \ac{GRPO} stems from its token-level importance ratios.
When training models for multi-turn interactions with coding agents, this becomes especially problematic due to numerical discrepancies between logits generated during fast inference (using engines like vLLM) and those computed during training.
These discrepancies compound across long sequences, creating high-variance training noise that progressively destabilizes the model.

For our specific use case—training models to use tools iteratively across multiple turns—this instability manifests as: \begin{itemize}
	\item Catastrophic forgetting of tool-use patterns mid-training
	\item Divergence between inference and training behavior
	\item Irreversible model collapse when sequences exceed typical lengths
\end{itemize}

\subsubsection{Sequence-Level Importance Sampling}

\ac{GSPO}'s key innovation lies in defining importance ratios at the sequence level rather than token level.
For a response $y$ to query $x$, the sequence-level importance ratio is: \begin{equation}
w(y|x; \theta) = \frac{\pi_\theta(y|x)}{\pi_{\theta_{\text{old}}}(y|x)} = \prod_{t=1}^{|y|} \frac{\pi_\theta(y_t|x, y_{<t})}{\pi_{\theta_{\text{old}}}(y_t|x, y_{<t})}
\end{equation}

This aligns with the fundamental principle of importance sampling and provides more stable gradient estimates, particularly crucial when responses involve multiple tool invocations with complex dependencies.

\subsubsection{The GSPO Objective}

\ac{GSPO} optimizes the following objective: \begin{equation}
J_{\text{GSPO}}(\theta) = \mathbb{E}_{x \sim D, \{y_i\}_{i=1}^G \sim \pi_{\theta_{\text{old}}}(\cdot|x)} \left[ \frac{1}{G} \sum_{i=1}^G \min\left( w(y_i|x; \theta) A_i, \text{clip}(w(y_i|x; \theta), 1-\epsilon, 1+\epsilon) A_i \right) \right]
\end{equation}

where advantages $A_i$ are computed as normalized rewards across the group: \begin{equation}
A_i = \frac{r(x, y_i) - \mu_r}{\sigma_r}
\end{equation}

The critical difference is that clipping, rewarding, and optimization all operate at the sequence level, maintaining consistency throughout the learning process.

\subsubsection{Advantages for Multi-Turn Tool Use}

For our coding agent training scenario, \ac{GSPO} provides several crucial benefits:

\textbf{Numerical Stability}: Sequence-level operations are more robust to the logit discrepancies between vLLM inference and training computation, preventing the accumulation of numerical errors that plague token-level methods.

\textbf{Coherent Tool Sequences}: By treating entire tool-use sequences as atomic units, \ac{GSPO} better preserves the logical flow of multi-step debugging processes, avoiding the fragmentation that can occur with token-level optimization.

\textbf{Simplified Infrastructure}: \ac{GSPO}'s design eliminates several complex stabilization strategies required by \ac{GRPO}, reducing the engineering overhead of our training pipeline.

\textbf{MoE Compatibility}: The Qwen team demonstrated that \ac{GSPO} inherently stabilizes Mixture-of-Experts training without additional interventions, opening possibilities for scaling to larger, more efficient architectures.

Empirical results from the Qwen3 models show that \ac{GSPO} not only stabilizes training but also achieves superior final performance compared to \ac{GRPO}, validating its theoretical advantages in practice.
For our work, adopting \ac{GSPO} represents a critical engineering decision that enables stable, long-duration training of coding agents through thousands of multi-turn interactions.

While the choice of optimization algorithm addresses training stability, the computational demands of fine-tuning billion-parameter models necessitate parameter-efficient approaches.

\section{Low-Rank Adaptation (LoRA)}
\label{sec:lora}

Parameter-efficient fine-tuning has become essential for adapting large language models to specific tasks without the computational overhead of full fine-tuning.
Low-Rank Adaptation (\ac{LoRA}) represents one of the most successful approaches in this domain.

\subsection{Mathematical Foundation}
\label{subsec:lora-math}

\ac{LoRA} is based on the hypothesis that weight updates during fine-tuning have low intrinsic rank.
For a pre-trained weight matrix $W_0 \in \mathbb{R}^{d \times k}$, \ac{LoRA} represents the update $\Delta W$ as:

\begin{equation}
\Delta W = BA
\end{equation}

where $B \in \mathbb{R}^{d \times r}$ and $A \in \mathbb{R}^{r \times k}$ with rank $r \ll \min(d,k)$.
The adapted weight becomes:

\begin{equation}
W = W_0 + \Delta W = W_0 + BA
\end{equation}

During training, $W_0$ remains frozen while only $A$ and $B$ are updated.
This dramatically reduces the number of trainable parameters from $dk$ to $r(d+k)$.

\subsection{Implementation Details}
\label{subsec:lora-implementation}

\subsubsection{Initialization Strategy}

\ac{LoRA} uses a specific initialization scheme to ensure training stability:
\begin{itemize}
	\item Matrix $A$ is initialized using random Gaussian values
	\item Matrix $B$ is initialized to zero, ensuring $\Delta W = BA = 0$ at the start
	\item This guarantees that the adapted model initially behaves identically to the pre-trained model
\end{itemize}

\subsubsection{Scaling Factor}

The \ac{LoRA} update is typically scaled by a factor $\alpha/r$ where $\alpha$ is a hyperparameter:

\begin{equation}
W = W_0 + \frac{\alpha}{r}BA
\end{equation}

This scaling allows for consistent learning rates across different rank values and provides a simple way to control adaptation strength.

\subsection{Computational Advantages}
\label{subsec:lora-advantages}

\ac{LoRA} offers substantial practical benefits for \ac{RL} training:

\textbf{Memory Efficiency}: With typical rank values $r = 8$ to $64$, \ac{LoRA} reduces trainable parameters by 99\%+ for large models, dramatically lowering \ac{GPU} memory requirements.

\textbf{Training Speed}: Fewer parameters mean faster gradient computation and reduced communication overhead in distributed training setups.

\textbf{Storage Efficiency}: \ac{LoRA} adapters are small (typically <100MB vs multi-GB full models), enabling efficient storage and distribution of task-specific adaptations.

\textbf{Modularity}: Multiple \ac{LoRA} adapters can be trained for different tasks and dynamically loaded, enabling flexible model deployment.

\subsection{Integration with Reinforcement Learning}
\label{subsec:lora-rl}

\ac{LoRA}'s benefits become particularly pronounced in \ac{RL} settings where multiple model instances must be maintained:

\textbf{Policy-Reference Separation}: \ac{RL} algorithms like \ac{PPO} require keeping both current and reference policies.
With \ac{LoRA}, the reference policy can share the frozen base weights while only the adapter differs.

\subsection{Theoretical Considerations}
\label{subsec:lora-theory}

Recent research has examined \ac{LoRA}'s representational capacity and limitations:

\textbf{Expressiveness}: While \ac{LoRA} cannot represent arbitrary weight updates, empirical evidence suggests that many fine-tuning scenarios do indeed have low-rank structure, making \ac{LoRA}'s constraints reasonable.

\textbf{Task Transfer}: \ac{LoRA} adapters learned for related tasks can serve as initialization for new tasks, potentially accelerating learning through transfer.

\textbf{Rank Selection}: Choosing appropriate rank values requires balancing expressiveness against efficiency.
Higher ranks provide more flexibility but reduce computational savings.

The combination of \ac{LoRA}'s efficiency with online \ac{RL} creates opportunities for more extensive experimentation and deployment of coding agents across diverse software engineering contexts.

Beyond parameter-efficient training, the choice of sampling strategy during generation plays a crucial role in the exploration-exploitation trade-off fundamental to RL.

\section{Sampling Strategies}
\label{sec:sampling-strategies}

The temperature parameter in language model sampling directly controls the exploration-exploitation trade-off during inference.
Higher temperatures increase randomness and exploration of diverse solutions, while lower temperatures favor exploitation of high-probability actions.
Critically, deterministic sampling (temperature=0) prevents any exploration, making it unsuitable for reinforcement learning where the agent must discover new strategies through trial and error.
During \ac{RL} training, we typically use temperature values between 0.7 and 1.0 to balance exploration of novel debugging approaches with exploitation of learned patterns (see \cref{app:sampling-params} for detailed sampling parameters).

With the theoretical and practical foundations established, we now examine the benchmarks used to evaluate code repair systems in realistic settings.

\section{Evaluation Benchmarks}
\label{sec:evaluation-benchmarks}

Rigorous evaluation of automated code repair systems requires diverse, realistic benchmarks that capture the complexity of real-world debugging scenarios.

\subsection{SWE-Bench Family}
\label{subsec:swe-bench}

The \ac{SWE-Bench} benchmark series has emerged as the gold standard for evaluating coding agents on realistic software engineering tasks.

\subsection{TauBench}
\label{subsec:taubench}

% \subsubsection{Evolution of SWE-Bench}

% \textbf{Original \ac{SWE-Bench} (2023)}~\cite{jimenez2024swebenchlanguagemodelsresolve}: The foundational dataset introduced 2,294 task instances drawn from 12 popular Python repositories. Each instance consists of:
% \begin{itemize}
% 	\item A GitHub issue describing a bug or feature request
% 	\item The repository state at issue creation time
% 	\item The developer-written patch that resolved the issue
% 	\item Test cases that fail before and pass after the patch
% \end{itemize}

% This design captures the full complexity of real software development: understanding natural language descriptions, navigating large codebases, and implementing solutions that satisfy existing tests.

% \textbf{\ac{SWE-Bench-Verified} (2024)}~\cite{sweBenchVerified2024}: Addressing quality concerns in the original dataset, this refined version includes 500 carefully validated instances that:
% \begin{itemize}
% 	\item Eliminate ambiguous issue descriptions
% 	\item Ensure deterministic test outcomes
% 	\item Remove trivial string replacements
% 	\item Verify patch minimality and correctness
% 	\item Balance difficulty across different problem types
% \end{itemize}

% Human annotators achieve 97\% success on \ac{SWE-Bench-Verified} compared to 73\% on the original, confirming the removal of problematic instances while maintaining challenging, realistic tasks.

% \textbf{\ac{SWE-Bench}
% 	Extensions}: While extensions to other programming languages have been proposed, our work focuses exclusively on the original Python-based benchmark suite, which provides the most mature and extensively validated evaluation framework.

% This diversity demonstrates the complexity and realism of the evaluation framework.

% \subsubsection{Task Format and Complexity}

% Each \ac{SWE-Bench} instance presents a naturalistic debugging scenario:

% \begin{verbatim}
% ISSUE DESCRIPTION:
% Title: DataFrame.apply() fails with axis=1 when columns have mixed types

% When calling df.apply(func, axis=1) on a DataFrame with both numeric 
% and string columns, the function receives Series with incorrect dtypes.

% Example to reproduce:
% ```python
% df = pd.DataFrame({'A': [1, 2], 'B': ['x', 'y']})
% df.apply(lambda row: row['A'] + len(row['B']), axis=1)
% # Raises TypeError
% ```

% REPOSITORY STATE:
% - 847 Python files totaling 284,000 lines
% - Complex module dependencies
% - 15,000+ test cases
% \end{verbatim}

% The evaluation system places the model in this repository state and measures whether it can produce a patch functionally equivalent to the developer's solution.
% This requires:
% \begin{itemize}
% 	\item Understanding the issue from natural language description
% 	\item Reproducing the bug through code execution
% 	\item Navigating the codebase to locate relevant modules
% 	\item Understanding existing implementation patterns
% 	\item Implementing a fix that maintains backward compatibility
% 	\item Ensuring the fix passes all existing tests
% \end{itemize}

% \subsubsection{Evaluation Methodology}

% \ac{SWE-Bench} employs rigorous evaluation protocols:

% \textbf{Functional verification}: Patches are evaluated by running the full test suite, not through textual comparison.
% This allows semantically equivalent but syntactically different solutions.

% \textbf{Isolated execution}: Each evaluation runs in a fresh Docker container to prevent cross-contamination and ensure reproducibility.

% \textbf{Time limits}: Solutions must complete within 5 minutes, reflecting real-world constraints on automated tools.

% \textbf{Minimal patches}: Credit is given only for patches that don't introduce unnecessary changes, encouraging precise solutions.

% \subsubsection{Why Success Rates Remain Low}

% Despite rapid progress in \ac{LLM} capabilities, even state-of-the-art systems achieve only ~20-25\% success rates on \ac{SWE-Bench}.
% This persistent challenge stems from several factors:

% \textbf{Repository complexity}: The average task requires understanding code distributed across 6.3 files, with deep call chains and complex dependencies.
% Current models struggle to maintain coherent understanding across such scales.

% \textbf{Ambiguity in natural language}: Issue descriptions often assume domain knowledge, use project-specific terminology, or describe symptoms rather than root causes.
% Models must infer substantial context.

% \textbf{Execution feedback requirement}: Unlike code generation tasks solvable through pattern matching, debugging requires iterative hypothesis testing through code execution—a capability most models lack.

% \textbf{Test suite complexity}: Solutions must satisfy not just the reported issue but maintain compatibility with thousands of existing tests, requiring deep understanding of system invariants.

% \textbf{Long-tail distribution}: Many bugs involve rare edge cases or unique project-specific patterns absent from training data, testing true generalization rather than memorization.

% These challenges make \ac{SWE-Bench} an ideal testbed for online training, where models can learn through environmental interaction rather than attempting single-shot solutions to complex, multi-faceted problems.

\section{Training Infrastructure for RL with LLMs}
\label{sec:training-infrastructure}

Training language models with reinforcement learning on interactive coding tasks presents unique infrastructure challenges that fundamentally differ from both traditional supervised learning and classical RL settings.
The scale of modern LLMs (billions of parameters), the sequential nature of text generation (hundreds to thousands of tokens per action), and the need for real-time environment interaction create a complex engineering problem that has received insufficient attention in the literature.

\subsection{The Dual Computation Problem}
\label{subsec:dual-computation}

Online RL for LLMs requires maintaining two distinct computational workloads simultaneously: model inference for generating actions and gradient computation for policy updates.
In traditional RL settings with small networks, these can occur on the same device.
However, for LLMs, each workload has distinct requirements:

\textbf{Inference workload}: Requires low latency ($<100$ms per token), high throughput (multiple concurrent episodes), optimized memory layouts (KV caching), and specialized kernels (FlashAttention, PagedAttention).
Modern inference engines like vLLM~\cite{vllm2023} and TensorRT-LLM achieve 10-20$\times$ throughput improvements over naive implementations through these optimizations.

\textbf{Training workload}: Requires gradient accumulation across batches, optimizer state storage (Adam momentum terms), activation checkpointing for memory efficiency, and distributed communication for multi-GPU training.
These requirements often conflict with inference optimizations, particularly around memory layout and kernel fusion strategies.

The naive approach of alternating between inference and training on the same GPUs suffers from severe inefficiencies.
Context switching between inference and training configurations can take seconds, while optimal GPU utilization requires different batch sizes for each workload.
Furthermore, gradient computation requires storing activations that inference can discard, leading to memory conflicts.

\subsection{Asynchronous Training-Inference Architecture}
\label{subsec:async-architecture}

Our solution, and that adopted by concurrent work~\cite{deepSWE2025}, separates inference and training onto different GPU pools connected via high-speed interconnects.
This architecture enables several critical optimizations:

\textbf{Specialized configurations}: Inference servers run with maximum batch sizes and aggressive memory optimizations, while training nodes maintain gradient buffers and optimizer states.
This specialization can improve overall throughput by 3-5$\times$ compared to unified approaches.

\textbf{Continuous generation}: Inference servers generate trajectories continuously without blocking on gradient updates.
This is particularly important for coding tasks where episode lengths vary dramatically (10-500 actions) and blocking would cause severe underutilization.

\textbf{Weight synchronization}: The key technical challenge becomes synchronizing model weights from training to inference nodes.
We employ NCCL collective operations for this synchronization, achieving sub-second updates for models up to 32B parameters on modern interconnects (InfiniBand, NVLink).

\subsection{Distributed Training Considerations}
\label{subsec:distributed-training}

Scaling RL training across multiple GPUs introduces additional complexity beyond standard supervised learning:

\textbf{Experience correlation}: Episodes generated from the same policy version exhibit high correlation, potentially destabilizing training.
Distributed generation across multiple inference nodes naturally decorrelates experiences, improving training stability.

\textbf{Gradient synchronization}: Policy gradient methods require aggregating gradients across all workers processing experiences from the same policy version.
With heterogeneous episode lengths, naive synchronization barriers cause significant idle time.
We adopt asynchronous gradient accumulation with periodic synchronization, balancing convergence speed with hardware utilization.

\textbf{Memory constraints}: Even with LoRA reducing trainable parameters by 99\%, modern LLMs require substantial GPU memory.
For a 7B parameter model with LoRA rank 64, each GPU requires approximately 20GB for model weights, optimizer states, gradients, and activation checkpoints.
This necessitates careful batch size tuning and potentially gradient accumulation across multiple forward passes.

With the infrastructure challenges addressed, we now examine an equally critical design choice: how to provide meaningful learning signals through reward design.

\section{Reward Design for Code Repair}
\label{sec:reward-design}

The choice of reward function fundamentally shapes what behaviors the RL agent learns.
For code repair, this choice involves balancing computational tractability, signal quality, and alignment with the ultimate goal of producing correct patches.

\subsection{Test-Based vs. Patch-Based Rewards}
\label{subsec:test-vs-patch}

Two primary reward paradigms have emerged in the literature:

\textbf{Test-based rewards} execute the generated patch against a test suite, providing binary (pass/fail) or graduated (percentage passing) signals.
This approach offers strong correctness guarantees and aligns directly with how human developers validate fixes.
However, it requires maintaining executable test environments for each repository, handling diverse testing frameworks (pytest, unittest, nose), and managing computational overhead (some test suites take minutes to run).
DeepSWE~\cite{deepSWE2025} adopts this approach, investing significant engineering effort in containerized test execution.

\textbf{Patch-based rewards} compare the generated patch to a reference solution using textual similarity metrics.
This approach, pioneered by SWE-RL~\cite{wei2025swerladvancingllmreasoning} and adopted in our work, computes similarity using sequence matching algorithms.
While this may miss semantically equivalent but syntactically different solutions, it offers several advantages: language-agnostic operation, minimal computational overhead, no need for repository-specific test harnesses, and consistent, deterministic signals.

\subsection{Similarity Metrics and Normalization}
\label{subsec:similarity-metrics}

Given patch-based rewards, the specific similarity metric significantly impacts learning dynamics.
Common approaches include:

\textbf{Token-level edit distance}: Counts minimum insertions, deletions, and substitutions needed to transform generated patch to reference.
This provides fine-grained credit assignment but can be overly sensitive to formatting differences.

\textbf{Line-level similarity}: Operates on complete lines rather than tokens, providing robustness to minor variations while maintaining semantic alignment.
Python's \texttt{difflib.SequenceMatcher} implements an efficient algorithm for this computation.

\textbf{Abstract syntax tree (AST) comparison}: Parses both patches and compares structural similarity, ignoring formatting and variable naming.
While theoretically appealing, this requires language-specific parsers and fails on syntactically invalid intermediate patches.

Critical to any similarity metric is proper normalization.
Raw similarity scores must be calibrated to provide meaningful learning signals:

\begin{equation}
r_{\text{norm}} = \frac{s - s_{\text{baseline}}}{s_{\text{max}} - s_{\text{baseline}}}
\end{equation}

where $s_{\text{baseline}}$ represents similarity without any edits and $s_{\text{max}}$ represents perfect match.
This normalization ensures that agents receive positive rewards only for improvements over the initial state.

\subsection{Sparse vs. Dense Rewards}
\label{subsec:sparse-dense}

A fundamental challenge in code repair RL is reward sparsity.
Most trajectories end in failure, providing zero reward and thus no gradient signal for improvement.
Several strategies address this:

\textbf{Intermediate rewards}: Providing signals for partial progress, such as successfully locating the buggy file or generating syntactically valid patches.
However, these shaped rewards risk introducing bias if not carefully designed.

\textbf{Curriculum learning}: Starting with simpler bugs where success is more likely, gradually increasing difficulty as the model improves.
This requires careful task sequencing and progress monitoring.

\textbf{Hindsight experience replay}: Reinterpreting failed trajectories as successful for different (retrospectively defined) goals.
While successful in robotics, adapting this to code repair remains an open challenge.

Our approach embraces sparse rewards, relying on the group-relative advantage computation in GRPO to extract learning signals even from predominantly failing trajectories.

Despite these design choices, applying RL to code generation and repair presents unique challenges that distinguish it from both traditional RL domains and standard language modeling tasks.

\section{Challenges in RL for Code}
\label{sec:rl-challenges}

Applying RL to code generation and repair presents unique challenges that distinguish it from both traditional RL domains and standard language modeling tasks.

\subsection{Compositional Action Spaces}
\label{subsec:compositional-actions}

Unlike game-playing agents with discrete action sets, code repair agents must compose actions from an essentially infinite space of possible code modifications.
A single edit might involve changing a function name (billions of possibilities), modifying control flow (exponential combinations), or adjusting data structures (unbounded complexity).
This compositional explosion makes exploration particularly challenging: random actions almost never produce valid code, let alone correct fixes.

The scaffolding approach partially addresses this by constraining actions to well-formed tool calls.
However, even within this constrained space, the agent must learn to coordinate multiple tools effectively, determining when to explore versus exploit current knowledge and how to recover from invalid action sequences.

\subsection{Partial Observability and Context Management}
\label{subsec:partial-observability}

Software repositories represent partially observable environments where the complete state (all code, dependencies, configurations) far exceeds what the agent can process.
A typical repository contains thousands of files, but LLM context windows accommodate only dozens.
This creates a critical exploration-exploitation dilemma: the agent must decide whether to examine new files (exploration) or focus on likely candidates (exploitation).

Furthermore, context management becomes a learned skill.
Agents must develop strategies for information gathering, hypothesis formation based on partial information, and recognizing when additional context is needed versus when to attempt a fix.
These meta-cognitive skills emerge only through extensive trial-and-error learning, highlighting the importance of online training.

\subsection{Catastrophic Forgetting in Multi-Task Settings}
\label{subsec:catastrophic-forgetting}

Training on diverse repositories and bug types risks catastrophic forgetting, where learning to fix one category of bugs degrades performance on previously mastered categories.
This is particularly acute in code repair due to the syntactic and semantic diversity across different codebases, programming paradigms, and bug types.

Standard continual learning techniques (elastic weight consolidation, rehearsal buffers) have seen limited application in LLM-scale RL.
Our approach of using LoRA adapters provides implicit regularization by constraining the update space, but optimal strategies for multi-task code repair RL remain an open research question.

\subsection{Evaluation and Overfitting}
\label{subsec:evaluation-overfitting}

A subtle but critical challenge involves evaluation validity.
Training on bug-fix pairs from open-source repositories risks data contamination if those fixes appear in the LLM's pretraining data.
Even with held-out test sets, models might memorize dataset-specific patterns rather than learning generalizable debugging strategies.

This motivates our emphasis on cross-benchmark evaluation (SWE-Gym for training, SWE-Bench for testing) and analysis of learned behaviors rather than just success rates.
Evidence of systematic debugging strategies (hypothesis testing, incremental modification) provides stronger validation than raw performance metrics.

Having examined the theoretical foundations, practical infrastructure, and fundamental challenges, we now position our work within the broader landscape of concurrent research.

\section{Summary and Research Positioning}
\label{sec:summary-positioning}

\textbf{DeepSWE}~\cite{deepswe2025} is highly similar to our work and validates our approach.
That is, they apply \ac{GRPO} on multi-turn, terminal-based coding agents with an asynchronous inference engine setup.
They used 64 H100s for training, 8 H100s for inference, and trained for 6 days on R2EGym, totaling ~$10368$ H100-hours.
In contrast, we train for ~2 days on 6 A100s (~$288$ A100-hours).
Normalizing by throughput, we conservatively estimate that $1\text{ H100-hour} \approx 2.2\text{ A100-hours}$; thus, we use ~$80\times$ less compute.
They used test-based rewards.
In contrast, we adopt an execution-free, patch-similarity reward, building on the patch-based signal introduced by SWE-RL~\cite{wei2025swerladvancingllmreasoning}.
This choice reduces engineering friction—avoiding per-repository test harnesses and heavyweight containerization—and enables concurrent training across heterogeneous environments and programming languages.
Our approach is also significantly more compute-efficient, as noted above; detailed design considerations are deferred to the methods chapter.

This chapter has established the theoretical and empirical foundations for online \ac{RL} applied to \ac{APR}.
By examining the evolution from rule-based repair systems to modern \ac{LLM}-based approaches, we identified fundamental limitations in current paradigms: the mismatch between static training and the dynamic nature of debugging.

Our review of reinforcement learning techniques, particularly the elegant simplification offered by \ac{GRPO}, demonstrates how computational efficiency can be achieved without sacrificing theoretical guarantees.
The examination of existing coding agents reveals a critical gap—all current systems operate at inference time only, missing the opportunity for models to learn optimal tool usage through experience.

The Nano agent architecture presents an alternative approach to agent design.
By providing only essential tools and relying on learning, we demonstrate that effective debugging capabilities can emerge from simple interfaces.

This thesis makes three fundamental contributions to the field:

\textbf{One of the first open-source online \ac{RL} implementations for \acp{LLM}}: We provide the research community with complete infrastructure for training language models through interactive environmental experience.
This democratizes access to techniques we believe are already employed by leading industry labs but have remained proprietary.
Our implementation enables researchers to explore online learning across diverse domains beyond code repair.

\textbf{Minimalist agent approach}: We demonstrate that a simple agent design with basic tools can achieve strong performance when combined with reinforcement learning, showing that sophisticated debugging behaviors can emerge from minimal interfaces.

\textbf{Bridge between research and practice}: By demonstrating that models can learn effective debugging strategies through \ac{RL} in realistic environments, we bridge the gap between academic research on program repair and practical tools used by developers.
Our approach shows that the future of coding assistance lies not in ever-more-sophisticated prompt engineering, but in models that learn from experience.

\subsection{Limitations of existing work}
\label{sec:limitations-existing}
\textbf{Focus on generation rather than repair}: Existing approaches predominantly emphasize code generation, yet creating new code differs fundamentally from the process of debugging and repairing existing systems.
This distinction is crucial, as effective automated program repair requires models to reason about and modify complex, pre-existing codebases rather than synthesizing code from scratch.

\textbf{Isolated task formulation}: Many prior works formulate tasks at the level of single functions, thereby neglecting the intricate interdependencies and architectural complexity inherent in real-world software systems.
This simplification limits the ability of models to generalize to practical debugging scenarios, where understanding interactions across multiple files and modules is essential.

\textbf{Lack of environmental interaction}: Previous methods typically do not incorporate exploration, tool usage, or iterative refinement into their training regimes.
As a result, models are deprived of opportunities to learn through active engagement with their environment, which is a key aspect of effective debugging and repair in realistic settings.

\textbf{Dependence on test-based rewards}: While relying on test-based rewards can, in principle, provide robust supervision, this strategy becomes increasingly impractical as the number of supported programming languages expands.
Constructing and maintaining comprehensive test suites for each language introduces significant operational challenges, thereby limiting the scalability and applicability of these approaches.

As we proceed to describe our method in detail, keep in mind that our goal is not merely to improve performance metrics on benchmarks, but to demonstrate a fundamentally different, nascent training approach that enables \acp{LLM} to learn not from labeled data points, but from experience.
