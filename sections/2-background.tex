\chapter{Theoretical Background and Related Work}
\label{ch:background}

This chapter establishes the theoretical and empirical background for online \ac{RL} applied to \ac{APR} with tool-using \acp{LLM}.
We first situate \ac{APR} historically, from search- and learning-based methods before \acp{LLM} to contemporary, repository-grounded systems, and introduce a simple taxonomy spanning manual, agentless, and agentic workflows.
We then review optimization methods for training such policies, including \ac{PPO}, \ac{GRPO}, and \acs{GSPO}, as well as practical issues such as variance collapse.
Complementary techniques for scalable training (parameter-efficient adaptation via \ac{LoRA} and sampling choices that balance exploration and exploitation) are summarized alongside evaluation resources such as \ac{SWE-Bench} and \ac{TauBench}.
We close with related work and limitations that motivate the minimalist agent and online training recipe developed in the following chapters.

\section{Automated Program Repair before \acsp{LLM}}\label{sec:apr-pre-llm}

Prior to \acp{LLM}, empirical \ac{APR} often evaluated on test-suite-based Java benchmarks and small algorithmic suites.
\textit{Defects4J} and the recent \textit{GitBug\textendash Java} provide full projects with reproducible builds and JUnit tests, enabling repository\textendash level reasoning; in practice, many repair systems nevertheless restricted themselves to local search spaces and single\textendash file patches for tractability~\cite{defects4J2014,gitbugJava2024}.
In contrast, \textit{QuixBugs} is explicitly snippet\textendash level: a collection of small standalone Java/Python programs with single\textendash line defects and accompanying tests~\cite{quixBugs2017}.

Within this setting, three families of approaches dominated.
\emph{Generate\textendash and\textendash validate} systems search a mutation space and accept candidates that satisfy the tests, with GenProg as a canonical example and PAR illustrating pattern\textendash guided edits~\cite{genProg2012,par2013}.
\emph{Semantics\textendash based} repair relies on program analysis—symbolic execution and constraint solving—to synthesize expressions at instrumented locations, as in SemFix~\cite{semFix2013}.
\emph{Learning\textendash guided} methods add statistical priors to prioritize candidates; Prophet learns a patch ranking model from past fixes~\cite{prophet2016}.
These lines collectively established feasibility on test\textendash suite benchmarks while also exposing the limits of test adequacy as a correctness proxy (overfitting, brittle generalization).
Those constraints, together with the need to reason beyond local contexts, set the stage for the repository\textendash grounded, model\textendash based approaches reviewed next.

\section{Automated Program Repair with LLMs}\label{sec:apr-with-llms}

Codex marked an inflection point where large pretrained models on code enabled practical repair and synthesis, shifting \ac{APR} from rules or search to learned edit priors~\cite{codex2021}.
Since then, frontier labs have converged on repository-grounded evaluation (e.g., \ac{SWE-Bench} Verified) and increasingly ship first-party \emph{agent harnesses} such as OpenAI's Codex \ac{CLI}, Anthropic's Claude Code, and Google's Gemini \ac{CLI} that orchestrate tool calls over live repositories~\cite{sweBench2024,sweBenchVerified2024,openaiCodexCLIRepo,claudeCode,geminiCLIRepo}.

We therefore organize \ac{LLM}-assisted repair by the degree of autonomy at inference: (i) \emph{scaffold-free} models operating without an execution harness, (ii) \emph{agentless} scaffolds that curate repository context without centralized autonomy, and (iii) \emph{agentic} systems that plan and execute tools end-to-end.
% This taxonomy clarifies which capabilities are evaluated and how results transfer across settings.

\subsection{Scaffold-free}\label{subsec:scaffold-free}
Scaffold-free approaches frame repair as translation from buggy code to fixed code.
The model receives a localized and self-contained context, typically a snippet or a single file with a narrow edit window, and proposes a patch in one interaction turn.
There is no repository traversal, no tool use during generation, and no execution signal in the loop.
Any tests are run outside the model after the fact.

Within this regime we focus on post-training methods that adapt an existing \ac{LLM} for repair.
A representative example is \emph{RepairLLaMA}, which fine-tunes a general code model on curated bug-fix pairs so that it proposes minimal and targeted edits without relying on an execution harness~\cite{repairllama2023}.
A parallel thread brings \ac{RL} into the same scaffold-free setting at the function or snippet level.
\emph{CodeRL} optimizes code generation with unit test execution as feedback while operating on localized contexts \cite{codeRL2022}.

These setups are attractive because data and pipelines are simple, adaptation is straightforward, and inference is fast.
It provides an effective baseline for measuring translation quality in localized contexts.
Its limitations follow from the design: without an execution loop the model cannot test hypotheses, observe runtime behavior, or coordinate edits across files.
Repository state and build systems remain external to generation.

\subsection{Agentless}\label{subsec:agentless}
Moving beyond scaffold-free translation, \emph{agentless} systems typically operate over a live repository while keeping execution outside the model.
A thin scaffold retrieves relevant files, builds diffs, and structures the prompt; the model returns patch blocks or edit intents; a human or scripted harness runs tests and shell commands and applies the edits.
This preserves repository grounding and repeatability without autonomous tool use.

We treat agentless systems as interfaces rather than agents: they make the contributions of context curation and patch formatting explicit and easy to ablate.
In practice, components inside the scaffold (retrievers, rankers, patch validators) can be trained, and logs from these pipelines support offline learning or imitation without exposing the model to a full tool loop.
Representative analyses show that a simple three-stage pipeline (localize, repair, validate) can match or exceed more complex setups when the scaffold is well-engineered~\cite{xia2024agentlessdemystifyingllmbasedsoftware}.
Related work begins to introduce reinforcement signals in repository settings while still short of a fully autonomous terminal loop, e.g., SWE-RL for repository-level reasoning~\cite{wei2025swerladvancingllmreasoning}.

The limitation is structural.
Because execution remains outside the model, the scaffold decides what to surface, when to run tests, and how to apply or revert edits.
The model does not learn the decision-making that drives repair.
What to try next, when to backtrack, when to submit or abandon a patch; because those choices live in the interface rather than in the policy.
The reward signal therefore entangles scaffold choices with model output, and the resulting histories are not suitable for end-to-end training.
Agentless setups are excellent for reproducible evaluation and for collecting logs, but they are a poor fit when the goal is to train a model to use tools by itself.

\todoinline{State that these methods often use LLMs as "functions" in these systems instead of end-to-end processes, meaning conversation histories are modified / restarted frequently which makes end-to-end learning hard / infeasible}

\subsection{Agentic}\label{subsec:agentic}

Agentic systems operate directly over a live repository and let the model initiate actions.
The model can open and edit files, run tests and shell commands, read raw outputs, and iterate until a patch is ready.
Public research harnesses make this concrete: \emph{SWE-Agent} formalizes an agent-computer loop for software tasks, \emph{OpenHands} provides a general terminal-based platform for coding agents, and \emph{mini-swe-agent} shows that a small, disciplined tool set in a tight loop is effective~\cite{sweAgent2024,openHands2024, miniSWEAgentRepo}.
Our Nano agent (\cref{sec:nano-agent}) adopts a similar minimalist philosophy, providing only essential terminal commands and file patching capabilities, allowing effective debugging behaviors to emerge through \ac{RL}.

Operationally, these systems rely on structured tool calls, typically via \acs{JSON}.
The model emits a well-formed function call with arguments, an external executor runs it, and the resulting output is returned to the model.
\begin{verbatim}
{
  "function_name": "apply_patch",
  "arguments": {
    "file_path": "src/utils.py",
    "old_content": "def buggy_function():",
    "new_content": "def fixed_function():"
  }
}
\end{verbatim}

This approach transforms \acp{LLM} from passive text generators into systems capable of interacting with their environment by invoking tools and observing their results.

This shift has also shaped how models are packaged and evaluated: recent open-weight releases pair model checkpoints with first-party harnesses and report agent-style evaluations, while disclosing only partial training details.
We summarize representative examples.

\paragraph{Qwen3\textendash Coder (Alibaba/Qwen team).}
Qwen3\textendash Coder is positioned as an \emph{agentic} coding model and ships with a first-party command-line harness (\emph{Qwen Code}) that drives repository-grounded autonomous workflows~\cite{qwen3CoderBlog2025,qwenCodeCLI}.
The release materials emphasize agent behavior in realistic development loops and demonstrate how the paired harness surfaces those capabilities.
At the same time, the post-training recipe, data generation, reward design, and optimization details is described only at a high level.
Weights and the harness are available, but a reproducible end-to-end training pipeline is not.

\paragraph{Kimi\textendash K2 (Moonshot AI).}
Kimi-K2 details an \emph{agentic} data synthesis pipeline paired with a joint reinforcement learning stage that mixes real and synthetic environment interactions.
\cite{kimiK2_2025}
The stated goal is to shape multi-step decision making—tool selection, iterative trial-and-error, and revision—rather than single-pass text generation.
The report frames this as necessary for repository-grounded coding and other multi-step tasks.
As with Qwen3, model artifacts and high-level methodology are public, but \emph{no} training recipe.

\paragraph{GLM\textendash 4.5 (Zhipu AI).}
GLM\textendash 4.5 positions itself as an \emph{agentic, reasoning, and coding} (ARC) family and explicitly reports a comprehensive post-training phase that combines expert iteration with \ac{SFT} and \ac{RL}, aimed at strengthening multi-step agentic tool use.
~\cite{glm45_2025}.
The public release provides open-weight checkpoints but does \emph{not} include a reproducible end-to-end agentic RL training pipeline or environment traces.

Taken together, these open-weight efforts converge on the same direction: agent-facing post-training that couples tool use, long-context coding, and multi-turn control.
They document \ac{SFT} and \ac{RL} training stages for agentic post-training and evaluations, but stop short of releasing reproducible training stacks.
In contrast, recent work such as \emph{DeepSWE} publicly reports an RL-trained coding agent within a terminal environment and releases open-weight models alongside practical training details, offering a rare end-to-end reference point for open agentic RL on software tasks.
\cite{deepSWE2025}

In practical terms, agentic systems close the loop between hypothesis and verification: the model requests an action, observes the real outcome, and can adjust.
This makes online \ac{RL} feasible and aligns the learning signal with the deployed behavior.
By contrast with agentless setups, the model is exposed to the full decision process of repair rather than a curated slice, so the resulting histories are suitable for end-to-end training.

Having established how agentic systems enable end-to-end learning, we now turn to the theoretical foundations of training such systems through reinforcement learning.

\section{\acl{RL} for Language Models}
\label{sec:rl-language-models}
\todoinline{the goal with this section is to incrementally showcase unqiue changes to \ac{PPO} which culminates in the final version we use (in methods we will say \ac{GRPO} with \ac{GSPO} importance, with \ac{DAPO} clipping, with Dr.
GRPO bias aversions)} This section fixes notation and states the \acl{PPO} objective that will serve as our baseline.
Subsequent sections build on this with \acl{GRPO} and \acl{GSPO}.
\subsection{Preliminaries}
\label{subsec:preliminaries}

An autoregressive language model with parameters $\theta$ is treated as a policy $\pi_\theta$ over sequences.
Let $\mathcal{D}$ denote a set of queries, with $x \in \mathcal{D}$, and let $y = (y_1,\ldots,y_{|y|})$ be a tokenized response.
The sequence likelihood factorizes as \begin{equation}
\pi_\theta(y \mid x) \;=\; \prod_{t=1}^{|y|}\, \pi_\theta\!\big(y_t \mid x,\, y_{<t}\big).
\end{equation} A reward function $r$ assigns a scalar outcome to a completed pair, with $r(x,y) \in [0,1]$.
Expectations are taken over $x \sim \mathcal{D}$ and over responses sampled from a frozen reference policy $\pi_{\theta_{\text{old}}}$ that generated the data for the current update.
The operator $\operatorname{clip}(\cdot, 1-\epsilon, 1+\epsilon)$ denotes elementwise clipping of importance ratios into a range controlled by $\epsilon \!
>\! 0$.
We omit the customary $\mathrm{KL}$ regularizer to a reference model both in practice and for clarity.

\subsection{\acl{PPO}}
\label{subsec:ppo}

\acl{PPO} performs multiple gradient steps on samples drawn from the old policy while constraining update size through clipping \cite{ppo2017}.
Using per token importance ratios and averaging across the sequence length, the objective is \begin{equation}
J_{\mathrm{PPO}}(\theta)
\;=\;
\mathbb{E}_{\,x \sim \mathcal{D},\; y \sim \pi_{\theta_{\text{old}}}(\cdot \mid x)}
\left[
\frac{1}{|y|}
\sum_{t=1}^{|y|}
\min\!\Big(
w_t(\theta)\,\hat{A}_t,\;
\operatorname{clip}\!\big(w_t(\theta),\, 1-\epsilon,\, 1+\epsilon\big)\,\hat{A}_t
\Big)
\right],
\end{equation} where the per token importance ratio compares next token probabilities under the current and old policies, \begin{equation}
w_t(\theta)
\;=\;
\frac{\pi_\theta\!\big(y_t \mid x,\, y_{<t}\big)}
{\pi_{\theta_{\text{old}}}\!\big(y_t \mid x,\, y_{<t}\big)}.
\end{equation} The averaging by $|y|$ normalizes for length so that long responses do not dominate the minibatch objective.
The term $\hat{A}_t$ is an advantage supplied by a learned value model.
This thesis uses terminal outcome supervision with no discounting, so the return at every prefix equals the final outcome $R := r(x,y) \in [0,1]$: \begin{equation}
G_t \;=\; R \quad \text{for all } t \in \{1,\ldots,|y|\}.
\end{equation} Accordingly, the per token advantage takes the exact form \begin{equation}
\hat{A}_t \;=\; G_t \;-\; V_\phi\!\big(x, y_{<t}\big)
\;=\; R \;-\; V_\phi\!\big(x, y_{<t}\big).
\end{equation} Intuitively, the advantage contrasts the realized outcome with what the model expected at each prefix: positive values increase the probability of $y_t$, negative values decrease it, and values near zero leave it largely unchanged.
In \ac{LLM} settings, maintaining a separate value model that is comparable in size to the policy substantially increases the overall memory footprint during training.
This elevated memory demand becomes especially pronounced for long responses, as both the policy and value models must process extended sequences in parallel, leading to significant resource consumption.

\subsection{\acl{GRPO}}
\label{subsec:grpo}

\acl{GRPO} replaces the learned value baseline with a group relative baseline computed from multiple responses to the same query \cite{grpo2024}.
For each $x \in \mathcal{D}$, the old policy $\pi_{\theta_{\text{old}}}$ samples a group $\{y_i\}_{i=1}^G$ and each response receives an outcome $r(x,y_i) \in [0,1]$.
Define the group mean and standard deviation \begin{equation}
\mu \;=\; \frac{1}{G}\sum_{i=1}^{G} r(x,y_i),
\qquad
\sigma \;=\; \sqrt{\frac{1}{G}\sum_{i=1}^{G}\big(r(x,y_i)-\mu\big)^{2}},
\end{equation} and the standardized group relative advantage for response $y_i$ \begin{equation}
A_i \;=\; \frac{r(x,y_i)-\mu}{\sigma + \varepsilon}.
\end{equation} All tokens in $y_i$ share the same scalar $A_i$.
With per token importance ratios \begin{equation}
w_{i,t}(\theta) \;=\; \frac{\pi_\theta\!\big(y_{i,t}\mid x,\,y_{i,<t}\big)}{\pi_{\theta_{\text{old}}}\!\big(y_{i,t}\mid x,\,y_{i,<t}\big)},
\end{equation} the surrogate mirrors PPO’s clipped form while averaging first within a sequence and then across the group: \begin{equation}
J_{\mathrm{GRPO}}(\theta)
\;=\;
\mathbb{E}_{\,x \sim \mathcal{D},\; \{y_i\}_{i=1}^{G} \sim \pi_{\theta_{\text{old}}}(\cdot \mid x)}
\left[
\frac{1}{G}\sum_{i=1}^{G}
\frac{1}{|y_i|}\sum_{t=1}^{|y_i|}
\min\!\Big(
w_{i,t}(\theta)\,A_i,\;
\operatorname{clip}\!\big(w_{i,t}(\theta),\, 1-\epsilon,\, 1+\epsilon\big)\,A_i
\Big)
\right].
\end{equation} The appeal is direct.
GRPO preserves the PPO update logic but substitutes a per query group baseline for the value model, which removes the critic, lowers memory, and simplifies training while keeping the clipped ratio safeguard and on policy sampling \cite{grpo2024}.

\subsubsection{Dr.\,GRPO}
\label{subsubsec:drgrpo}

Dr.
\,GRPO identifies two biases in the GRPO objective and removes the corresponding normalization terms \cite{drGRPO2025}.
First, dividing each sample's loss by $|y_i|$ introduces a response level length bias.
Second, dividing by the group standard deviation $\sigma$ couples update magnitude to the within group difficulty spread.
Dr.
\,GRPO therefore uses an unnormalized, unbiased group advantage and removes response level length averaging:
\begin{equation}
A_i^{\text{Dr}} \;=\; r(x,y_i) \;-\; \frac{1}{G}\sum_{j=1}^{G} r(x,y_j),
\end{equation}
\begin{equation}
J_{\mathrm{DrGRPO}}(\theta)
\;=\;
\mathbb{E}\!
\left[
\frac{1}{G}\sum_{i=1}^{G}
\sum_{t=1}^{|y_i|}
\min\!\Big(
w_{i,t}(\theta)\,A_i^{\text{Dr}},\;
\operatorname{clip}\!\big(w_{i,t}(\theta),\, 1-\epsilon,\, 1+\epsilon\big)\,A_i^{\text{Dr}}
\Big)
\right].
\end{equation}
In practice the inner sum is implemented as a masked mean over all tokens in the batch with a constant denominator rather than $|y_i|$ per sample.
Removing the two normalizations improves token efficiency and reduces the tendency to inflate the length of incorrect responses \cite{drGRPO2025}.

\subsubsection{\acl{DAPO}}
\label{subsubsec:dapo}

\ac{DAPO} is a large scale recipe that builds on \ac{GRPO} with four concrete changes that target entropy collapse, reward noise, and instability \cite{dapo2025}.
The objective decouples clipping and aggregates at token level across the whole group: \begin{equation}
J_{\mathrm{DAPO}}(\theta)
\;=\;
\mathbb{E}
\left[
\frac{1}{\sum_{i=1}^{G} |y_i|}
\sum_{i=1}^{G}
\sum_{t=1}^{|y_i|}
\min\!\Big(
w_{i,t}(\theta)\,A_i,\;
\operatorname{clip}\!\big(w_{i,t}(\theta),\, 1-\epsilon_{\text{low}},\, 1+\epsilon_{\text{high}}\big)\,A_i
\Big)
\right],
\end{equation} with $A_i$ the group standardized advantage as in GRPO and asymmetric clipping $(\epsilon_{\text{low}},\epsilon_{\text{high}})$.
The algorithm removes the \ac{KL} penalty, uses token level loss, and adds two stabilizers: \emph{dynamic sampling} with a buffer that enforces a balanced mix of correct and incorrect samples for each query group, and \emph{overlong filtering and shaping} that masks loss on truncated or timed out generations and downweights overlong failures.
The higher upper clip encourages exploration and prevents early entropy collapse; the dynamic buffer reduces variance and improves data efficiency; token level aggregation and overlong filtering make training robust for very long reasoning traces.

\subsubsection{\acl{GSPO}}
\label{subsubsec:gspo}

Token-level importance weighting in GRPO is fragile at scale.
Small logit discrepancies between training and inference engines accumulate across long sequences, and in sparse Mixture-of-Experts models the set of activated experts can change between $\pi_{\theta_{\text{old}}}$ and $\pi_\theta$, making token-level ratios volatile and invalid as off-policy corrections.
These effects amplify under clipping and have been observed to trigger irreversible collapse; prior \ac{MoE} work required routing-replay to stabilize GRPO.
\cite{gspo2025}

\emph{\ac{GSPO}} restores a coherent importance sampler by matching the optimization unit to the reward unit: entire responses.
With the same group-relative advantage as GRPO, \begin{equation}
A_i \;=\; \frac{r(x,y_i)\;-\;\frac{1}{G}\sum_{j=1}^{G} r(x,y_j)}{\sqrt{\frac{1}{G}\sum_{j=1}^{G}\big(r(x,y_j) - \frac{1}{G}\sum_{k=1}^{G} r(x,y_k)\big)^2} \;+\; \varepsilon},
\end{equation} GSPO defines a \emph{length-normalized sequence-level} importance ratio \begin{equation}
s_i(\theta)
\;=\;
\left(\frac{\pi_\theta(y_i \mid x)}{\pi_{\theta_{\text{old}}}(y_i \mid x)}\right)^{\!1/|y_i|}
\;=\;
\exp\!\left(
\frac{1}{|y_i|}\sum_{t=1}^{|y_i|}
\log\frac{\pi_\theta(y_{i,t}\mid x, y_{i,<t})}{\pi_{\theta_{\text{old}}}(y_{i,t}\mid x, y_{i,<t})}
\right),
\end{equation} and optimizes a clipped sequence-level surrogate \begin{equation}
J_{\mathrm{GSPO}}(\theta)
\;=\;
\mathbb{E}_{\,x \sim \mathcal{D},\;\{y_i\}_{i=1}^{G}\sim \pi_{\theta_{\text{old}}}(\cdot\mid x)}
\left[
\frac{1}{G}\sum_{i=1}^{G}
\min\!\Big(
s_i(\theta)\,A_i,\;
\operatorname{clip}\!\big(s_i(\theta),\,1-\epsilon_1,\,1+\epsilon_2\big)\,A_i
\Big)
\right].
\end{equation} Length normalization in $s_i(\theta)$ keeps ratios numerically comparable across response lengths and prevents a few token-level logit differences from exploding the sequence weight; the resulting clipping ranges are therefore orders of magnitude smaller than in token-level schemes.
\cite{gspo2025}

The gradient makes the stability contrast explicit (omitting clipping for brevity): \begin{equation}
\nabla_\theta J_{\mathrm{GSPO}}(\theta)
\;=\;
\mathbb{E}\!\left[
\frac{1}{G}\sum_{i=1}^{G}
s_i(\theta)\,A_i\;\cdot\;
\frac{1}{|y_i|}\sum_{t=1}^{|y_i|}
\nabla_\theta \log \pi_\theta(y_{i,t}\mid x, y_{i,<t})
\right],
\end{equation} so all tokens in a response are weighted equally by the same sequence factor $s_i(\theta)$.
In GRPO, by contrast, each token carries its own noisy ratio $w_{i,t}$, and those unequal weights accumulate unpredictably along the sequence.
GSPO's sequence weighting eliminates that instability, removes the need for routing-replay in \acs{MoE} \ac{RL}, and is tolerant to training-vs-inference log-probprecision differences—often allowing direct use of inference-engine likelihoods without recomputation.
\cite{gspo2025}

\section{\acl{LoRA} (\acs{LoRA})} \label{sec:lora}

Parameter-efficient adaptation is essential for training large models without the overhead of full fine-tuning.
\acl{LoRA} learns a low-rank update to frozen weights \cite{lora2021}:
\begin{equation}
\Delta W = BA,\qquad W = W_0 + \frac{\alpha}{r}\,BA,
\end{equation}
with $B \in \mathbb{R}^{d\times r}$, $A \in \mathbb{R}^{r\times k}$, and rank $r \ll \min(d,k)$.
Initializing $B{=}0$ preserves the base model at the start ($W{=}W_0$), which stabilizes optimization.
For online \acl{RL} with coding agents, \ac{LoRA} reduces trainable state, supports clean policy-reference separation (adapters differ, base is shared), and accelerates iteration.
Adapter placement is flexible—practitioners select which modules to adapt based on capacity/efficiency goals; rank $r$ and scale $\alpha$ set the capacity-efficiency trade-off.

With the theoretical and practical foundations established, we now examine the benchmarks used to evaluate code repair systems in realistic settings.

\section{Training and Evaluation Datasets}
\label{sec:training-evaluation-datasets}

Effective training and evaluation of code repair agents requires diverse, realistic datasets that capture the complexity of real-world debugging scenarios across multiple programming languages.

\subsection{Training Datasets}
\label{subsec:training-datasets}

Our training approach utilizes two complementary datasets to enable multilingual debugging capability.
SWE-Gym~\cite{sweGym2025} provides approximately 2,400 Python bug-fixing tasks derived from real GitHub repositories, offering a substantial foundation for learning core debugging strategies.
SWE-Bench-Multilingual~\cite{sweBenchMultilingual2025} contributes approximately 300 debugging tasks across nine programming languages (Rust, Java, PHP, Ruby, JavaScript, TypeScript, Go, C, and C++), enabling cross-language generalization.
This combination allows for multilingual training while maintaining sufficient data volume for stable convergence.

\subsection{Evaluation Benchmarks}
\label{subsec:evaluation-benchmarks}

The SWE-Bench benchmark series~\cite{sweBench2024} serves as the primary evaluation framework for coding agents on realistic software engineering tasks.
SWE-Bench-Verified~\cite{sweBenchVerified2024} contains approximately 500 carefully validated Python debugging tasks that eliminate ambiguous descriptions and ensure deterministic outcomes, providing reliable performance measurement.
For cross-language evaluation, we utilize external benchmarks such as GitBug-Java~\cite{gitbugJava2024} to assess transfer capabilities to languages not seen during training.
\ac{TauBench}~\cite{taubench2024} enables broader assessment of how tool-usage skills transfer to adjacent tasks beyond code repair.

% \subsubsection{Evolution of SWE-Bench}

% \textbf{Original \ac{SWE-Bench} (2023)}~\cite{sweBench2024}: The foundational dataset introduced 2,294 task instances drawn from 12 popular Python repositories. Each instance consists of:
% \begin{itemize}
% 	\item A GitHub issue describing a bug or feature request
% 	\item The repository state at issue creation time
% 	\item The developer-written patch that resolved the issue
% 	\item Test cases that fail before and pass after the patch
% \end{itemize}

% This design captures the full complexity of real software development: understanding natural language descriptions, navigating large codebases, and implementing solutions that satisfy existing tests.

% \textbf{\ac{SWE-Bench-Verified} (2024)}~\cite{sweBenchVerified2024}: Addressing quality concerns in the original dataset, this refined version includes 500 carefully validated instances that:
% \begin{itemize}
% 	\item Eliminate ambiguous issue descriptions
% 	\item Ensure deterministic test outcomes
% 	\item Remove trivial string replacements
% 	\item Verify patch minimality and correctness
% 	\item Balance difficulty across different problem types
% \end{itemize}

% Human annotators achieve 97\% success on \ac{SWE-Bench-Verified} compared to 73\% on the original, confirming the removal of problematic instances while maintaining challenging, realistic tasks.

% \textbf{\ac{SWE-Bench}
% 	Extensions}: While extensions to other programming languages have been proposed, our work focuses exclusively on the original Python-based benchmark suite, which provides the most mature and extensively validated evaluation framework.

% This diversity demonstrates the complexity and realism of the evaluation framework.

% \subsubsection{Task Format and Complexity}

% Each \ac{SWE-Bench} instance presents a naturalistic debugging scenario:

% \begin{verbatim}
% ISSUE DESCRIPTION:
% Title: DataFrame.apply() fails with axis=1 when columns have mixed types

% When calling df.apply(func, axis=1) on a DataFrame with both numeric 
% and string columns, the function receives Series with incorrect dtypes.

% Example to reproduce:
% ```python
% df = pd.DataFrame({'A': [1, 2], 'B': ['x', 'y']})
% df.apply(lambda row: row['A'] + len(row['B']), axis=1)
% # Raises TypeError
% ```

% REPOSITORY STATE:
% - 847 Python files totaling 284,000 lines
% - Complex module dependencies
% - 15,000+ test cases
% \end{verbatim}

% The evaluation system places the model in this repository state and measures whether it can produce a patch functionally equivalent to the developer's solution.
% This requires:
% \begin{itemize}
% 	\item Understanding the issue from natural language description
% 	\item Reproducing the bug through code execution
% 	\item Navigating the codebase to locate relevant modules
% 	\item Understanding existing implementation patterns
% 	\item Implementing a fix that maintains backward compatibility
% 	\item Ensuring the fix passes all existing tests
% \end{itemize}

% \subsubsection{Evaluation Methodology}

% \ac{SWE-Bench} employs rigorous evaluation protocols:

% \textbf{Functional verification}: Patches are evaluated by running the full test suite, not through textual comparison.
% This allows semantically equivalent but syntactically different solutions.

% \textbf{Isolated execution}: Each evaluation runs in a fresh Docker container to prevent cross-contamination and ensure reproducibility.

% \textbf{Time limits}: Solutions must complete within 5 minutes, reflecting real-world constraints on automated tools.

% \textbf{Minimal patches}: Credit is given only for patches that don't introduce unnecessary changes, encouraging precise solutions.

% \subsubsection{Why Success Rates Remain Low}

% Despite rapid progress in \ac{LLM} capabilities, even state-of-the-art systems achieve only ~20-25\% success rates on \ac{SWE-Bench}.
% This persistent challenge stems from several factors:

% \textbf{Repository complexity}: The average task requires understanding code distributed across 6.3 files, with deep call chains and complex dependencies.
% Current models struggle to maintain coherent understanding across such scales.

% \textbf{Ambiguity in natural language}: Issue descriptions often assume domain knowledge, use project-specific terminology, or describe symptoms rather than root causes.
% Models must infer substantial context.

% \textbf{Execution feedback requirement}: Unlike code generation tasks solvable through pattern matching, debugging requires iterative hypothesis testing through code execution—a capability most models lack.

% \textbf{Test suite complexity}: Solutions must satisfy not just the reported issue but maintain compatibility with thousands of existing tests, requiring deep understanding of system invariants.

% \textbf{Long-tail distribution}: Many bugs involve rare edge cases or unique project-specific patterns absent from training data, testing true generalization rather than memorization.

% These challenges make \ac{SWE-Bench} an ideal testbed for online training, where models can learn through environmental interaction rather than attempting single-shot solutions to complex, multi-faceted problems.

Having examined the theoretical foundations, we now position our work within the broader landscape of concurrent research.

\section{Summary and Research Positioning}
\label{sec:summary-positioning}

\textbf{DeepSWE}~\cite{deepSWE2025} is highly similar to our work and validates our approach.
That is, they apply \ac{GRPO} on multi-turn, terminal-based coding agents with an asynchronous inference engine setup.
They used 64 H100s for training, 8 H100s for inference, and trained for 6 days on R2EGym, totaling ~$10368$ H100-hours.
In contrast, we train for ~2 days on 6 A100s (~$288$ A100-hours).
Normalizing by throughput, we conservatively estimate that $1\text{ H100-hour} \approx 2.2\text{ A100-hours}$; thus, we use ~$80\times$ less compute.
They used test-based rewards.
In contrast, we adopt an execution-free, patch-similarity reward, building on the patch-based signal introduced by SWE-RL~\cite{wei2025swerladvancingllmreasoning}.
This choice reduces engineering friction—avoiding per-repository test harnesses and heavyweight containerization—and enables concurrent training across heterogeneous environments and programming languages.
Our approach is also significantly more compute-efficient, as noted above; detailed design considerations are deferred to the methods chapter.

This chapter has established the theoretical and empirical foundations for online \ac{RL} applied to \ac{APR}.
By examining the evolution from rule-based repair systems to modern \ac{LLM}-based approaches, we identified fundamental limitations in current paradigms: the mismatch between static training and the dynamic nature of debugging.

Our review of reinforcement learning techniques, particularly the elegant simplification offered by \ac{GRPO}, demonstrates how computational efficiency can be achieved without sacrificing theoretical guarantees.
The examination of existing coding agents reveals a critical gap—all current systems operate at inference time only, missing the opportunity for models to learn optimal tool usage through experience.

The Nano agent architecture presents an alternative approach to agent design.
By providing only essential tools and relying on learning, we demonstrate that effective debugging capabilities can emerge from simple interfaces.

This thesis makes three fundamental contributions to the field:

\textbf{One of the first open-source online \ac{RL} implementations for \acp{LLM}}: We provide the research community with complete infrastructure for training language models through interactive environmental experience.
This democratizes access to techniques we believe are already employed by leading industry labs but have remained proprietary.
Our implementation enables researchers to explore online learning across diverse domains beyond code repair.

\textbf{Minimalist agent approach}: We demonstrate that a simple agent design with basic tools can achieve strong performance when combined with reinforcement learning, showing that sophisticated debugging behaviors can emerge from minimal interfaces.

\textbf{Bridge between research and practice}: By demonstrating that models can learn effective debugging strategies through \ac{RL} in realistic environments, we bridge the gap between academic research on program repair and practical tools used by developers.
Our approach shows that the future of coding assistance lies not in ever-more-sophisticated prompt engineering, but in models that learn from experience.

\subsection{Limitations of existing work}
\label{sec:limitations-existing}
\textbf{Focus on generation rather than repair}: Existing approaches predominantly emphasize code generation, yet creating new code differs fundamentally from the process of debugging and repairing existing systems.
This distinction is crucial, as effective automated program repair requires models to reason about and modify complex, pre-existing codebases rather than synthesizing code from scratch.

\textbf{Isolated task formulation}: Many prior works formulate tasks at the level of single functions, thereby neglecting the intricate interdependencies and architectural complexity inherent in real-world software systems.
This simplification limits the ability of models to generalize to practical debugging scenarios, where understanding interactions across multiple files and modules is essential.

\textbf{Lack of environmental interaction}: Previous methods typically do not incorporate exploration, tool usage, or iterative refinement into their training regimes.
As a result, models are deprived of opportunities to learn through active engagement with their environment, which is a key aspect of effective debugging and repair in realistic settings.

\textbf{Dependence on test-based rewards}: While relying on test-based rewards can, in principle, provide robust supervision, this strategy becomes increasingly impractical as the number of supported programming languages expands.
Constructing and maintaining comprehensive test suites for each language introduces significant operational challenges, thereby limiting the scalability and applicability of these approaches.

As we proceed to describe our method in detail, keep in mind that our goal is not merely to improve performance metrics on benchmarks, but to demonstrate a fundamentally different, nascent training approach that enables \acp{LLM} to learn not from labeled data points, but from experience.
