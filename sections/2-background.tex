\chapter{Theoretical Background and Related Work}
\label{ch:background}

This chapter establishes the theoretical and empirical foundations for applying online \ac{RL} to \ac{APR} with tool-augmented \acp{LLM}.
We situate \ac{APR} historically, progressing from pre-LLM search- and learning-based methods to contemporary repository-grounded systems, introducing a taxonomy spanning scaffold-free, agentless, and agentic paradigms.
Policy optimization algorithms (\ac{PPO}, \ac{GRPO}, DAPO, \ac{GSPO}) are reviewed with emphasis on group-relative methods and stability challenges in variable-length trajectories.
Complementary techniques—parameter-efficient adaptation via \ac{LoRA}, exploration-exploitation balance through sampling—are summarized alongside evaluation benchmarks (SWE-Bench, TauBench).
We conclude with related work and design constraints that motivate the minimalist Nano agent and execution-free training methodology presented in subsequent chapters.

\section{Automated Program Repair before \acsp{LLM}}\label{sec:apr-pre-llm}

Empirical \ac{APR} research prior to \acp{LLM} evaluated primarily on test-suite-based Java benchmarks and function-level algorithmic benchmarks.
\textit{Defects4J} established reproducible evaluation with full-project test suites, while \textit{GitBug-Java} expanded repository-level benchmarking with additional real-world projects and build configurations~\cite{defects4J2014,gitbugJava2024}.
Despite these repository-level resources, repair methods of that era commonly focused on local search spaces and single-file patches, reflecting the computational constraints and prevailing algorithmic approaches of the time.
\textit{QuixBugs} operates at function granularity: standalone Java/Python programs with a single seeded defect per program and accompanying unit tests~\cite{quixBugs2017}.

Three families of approaches dominated this era.
\emph{Generate-and-validate} systems search mutation spaces, accepting candidates that satisfy test suites; GenProg pioneered this approach, while PAR demonstrated pattern-guided edit selection~\cite{genProg2012,par2013}.
\emph{Semantics-based} methods employ program analysis (symbolic execution, constraint solving) to synthesize expressions at instrumented program locations, exemplified by SemFix~\cite{semFix2013}.
\emph{Learning-guided} approaches incorporate statistical priors to prioritize search; Prophet learns patch ranking models from historical fixes~\cite{prophet2016}.
These methods established feasibility on test-suite benchmarks while revealing limitations of test adequacy as a correctness proxy: overfitting to specific test cases and brittle generalization to untested scenarios.
These constraints, combined with difficulty reasoning beyond localized contexts, motivated the repository-grounded, model-based approaches that emerged with \acp{LLM}.

\section{Automated Program Repair with \acsp{LLM}}\label{sec:apr-with-llms}

Codex marked an inflection point: large-scale pretraining on code enabled practical repair and synthesis, shifting \ac{APR} from symbolic search to learned edit distributions~\cite{codex2021}.
Frontier laboratories subsequently converged on repository-grounded evaluation (SWE-Bench, SWE-Bench-Verified) and deployed first-party agent harnesses (OpenAI Codex CLI, Anthropic Claude Code, Google Gemini CLI) that orchestrate tool invocations over live repositories~\cite{sweBench2024,sweBenchVerified2024,openaiCodexCLIRepo,claudeCode,geminiCLIRepo}.

We organize \ac{LLM}-based repair by inference-time autonomy: (i) \emph{scaffold-free} models that generate patches without tool execution, (ii) \emph{agentless} systems where external scripts curate context and apply edits, and (iii) \emph{agentic} systems where models autonomously plan, invoke tools, and iterate based on observations.

\subsection{Scaffold-Free}\label{subsec:scaffold-free}
Scaffold-free approaches frame repair as translation from buggy code to fixed code.
The model receives a localized and self-contained context, typically a snippet or a single file with a narrow edit window, and proposes a patch in one interaction turn.
There is no repository traversal, no tool use during generation, and no execution signal in the loop.
Any tests are run outside the model after the fact.

Within this regime we focus on post-training methods that adapt an existing \ac{LLM} for repair.
A representative example is \emph{RepairLLaMA}, which fine-tunes a general code model on curated bug-fix pairs so that it proposes minimal and targeted edits without relying on an execution harness~\cite{repairllama2023}.
A parallel thread brings \ac{RL} into the same scaffold-free setting at the function or snippet level.
\emph{CodeRL} optimizes code generation with unit test execution as feedback while operating on localized contexts \cite{codeRL2022}.

These setups are attractive because data and pipelines are simple, adaptation is straightforward, and inference is fast.
It provides an effective baseline for measuring translation quality in localized contexts.
Its limitations follow from the design: without an execution loop the model cannot test hypotheses, observe runtime behavior, or coordinate edits across files.
Repository state and build systems remain external to generation.

\subsection{Agentless}\label{subsec:agentless}
Moving beyond scaffold-free translation, \emph{agentless} systems typically operate over a live repository while keeping execution outside the model.
A thin scaffold retrieves relevant files, builds diffs, and structures the prompt; the model returns patch blocks or edit intents; a human or scripted harness runs tests and shell commands and applies the edits.
This preserves repository grounding and repeatability without autonomous tool use.

We treat agentless systems as interfaces rather than agents: they make the contributions of context curation and patch formatting explicit and easy to ablate.
In practice, components inside the scaffold (retrievers, rankers, patch validators) can be trained, and logs from these pipelines support offline learning or imitation without exposing the model to a full tool loop.
Representative analyses show that a simple three-stage pipeline (localize, repair, validate) can match or exceed more complex setups when the scaffold is well-engineered~\cite{xia2024agentlessdemystifyingllmbasedsoftware}.
Related work begins to introduce reinforcement signals in repository settings while still short of a fully autonomous terminal loop, e.g., SWE-RL for repository-level reasoning~\cite{wei2025swerladvancingllmreasoning}.

The limitation is structural.
Because execution remains outside the model, the scaffold decides what to surface, when to run tests, and how to apply or revert edits.
The model does not learn the decision-making that drives repair.
What to try next, when to backtrack, when to submit or abandon a patch; because those choices live in the interface rather than in the policy.
The reward signal therefore entangles scaffold choices with model output, and the resulting histories are not suitable for end-to-end training.
Agentless setups are excellent for reproducible evaluation and for collecting logs, but they are a poor fit when the goal is to train a model to use tools by itself.

Moreover, many agentless systems invoke \acp{LLM} as stateless functions rather than maintaining continuous conversation histories.
Between invocations, the system modifies or restarts context windows to inject new information, apply filters, or enforce structural constraints—interventions that break the causal chain necessary for gradient-based learning.
When conversation histories are frequently pruned, reordered, or regenerated by the outer scaffold, the model cannot learn from the natural consequences of its prior outputs.
This architectural choice optimizes for immediate task performance through careful prompt engineering but renders end-to-end policy learning infeasible, as the training signal would conflate the model's decisions with the scaffold's context manipulation.

\subsection{Agentic}\label{subsec:agentic}

Agentic systems operate directly over a live repository and let the model initiate actions.
The model can open and edit files, run tests and shell commands, read raw outputs, and iterate until a patch is ready.
Public research harnesses make this concrete: \emph{SWE-Agent} formalizes an agent-computer loop for software tasks, \emph{OpenHands} provides a general terminal-based platform for coding agents, and \emph{mini-swe-agent} shows that a small, disciplined tool set in a tight loop is effective~\cite{sweAgent2024,openHands2024, miniSWEAgentRepo}.
Our Nano agent (\cref{sec:nano-agent}) adopts a similar minimalist philosophy, providing only essential terminal commands and file patching capabilities, allowing effective debugging behaviors to emerge through \ac{RL}.

Operationally, these systems rely on structured tool calls, typically via \acs{JSON}.
The model emits a well-formed function call with arguments, an external executor runs it, and the resulting output is returned to the model.
\begin{verbatim}
{
  "function_name": "apply_patch",
  "arguments": {
    "file_path": "src/utils.py",
    "old_content": "def buggy_function():",
    "new_content": "def fixed_function():"
  }
}
\end{verbatim}

This approach transforms \acp{LLM} from passive text generators into systems capable of interacting with their environment by invoking tools and observing their results.

This shift has also shaped how models are packaged and evaluated: recent open-weight releases pair model checkpoints with first-party harnesses and report agent-style evaluations, while disclosing only partial training details.
We summarize representative examples.

\paragraph{Qwen3\textendash Coder (Alibaba/Qwen team).}
Qwen3\textendash Coder is positioned as an \emph{agentic} coding model and ships with a first-party command-line harness (\emph{Qwen Code}) that drives repository-grounded autonomous workflows~\cite{qwen3CoderBlog2025,qwenCodeCLI}.
The release materials emphasize agent behavior in realistic development loops and demonstrate how the paired harness surfaces those capabilities.
At the same time, the post-training recipe, data generation, reward design, and optimization details is described only at a high level.
Weights and the harness are available, but a reproducible end-to-end training pipeline is not.

\paragraph{Kimi\textendash K2 (Moonshot AI).}
Kimi-K2 details an \emph{agentic} data synthesis pipeline paired with a joint reinforcement learning stage that mixes real and synthetic environment interactions.
\cite{kimiK2_2025}
The stated goal is to shape multi-step decision making—tool selection, iterative trial-and-error, and revision—rather than single-pass text generation.
The report frames this as necessary for repository-grounded coding and other multi-step tasks.
As with Qwen3, model artifacts and high-level methodology are public, but \emph{no} training recipe.

\paragraph{GLM\textendash 4.5 (Zhipu AI).}
GLM\textendash 4.5 positions itself as an \emph{agentic, reasoning, and coding} (ARC) family and explicitly reports a comprehensive post-training phase that combines expert iteration with \ac{SFT} and \ac{RL}, aimed at strengthening multi-step agentic tool use.
~\cite{glm45_2025}.
The public release provides open-weight checkpoints but does \emph{not} include a reproducible end-to-end agentic \ac{RL} training pipeline or environment traces.

Taken together, these open-weight efforts converge on the same direction: agent-facing post-training that couples tool use, long-context coding, and multi-turn control.
They document \ac{SFT} and \ac{RL} training stages for agentic post-training and evaluations, but stop short of releasing reproducible training stacks.
In contrast, recent work such as \emph{DeepSWE} publicly reports an \ac{RL}-trained coding agent within a terminal environment and releases open-weight models alongside practical training details, offering a rare end-to-end reference point for open agentic \ac{RL} on software tasks.
\cite{deepSWE2025}

In practical terms, agentic systems close the loop between hypothesis and verification: the model requests an action, observes the real outcome, and can adjust.
This makes online \ac{RL} feasible and aligns the learning signal with the deployed behavior.
By contrast with agentless setups, the model is exposed to the full decision process of repair rather than a curated slice, so the resulting histories are suitable for end-to-end training.

Having established how agentic systems enable end-to-end learning, we now turn to the theoretical foundations of training such systems through reinforcement learning.

\section{\acl{RL} for Language Models}
\label{sec:rl-language-models}
This section reviews policy optimization algorithms for \acp{LLM}, progressing from \ac{PPO} to group-relative policy optimization (\ac{GRPO}), and concluding with \ac{GSPO}, which we ultimately adopt for its superior robustness in multi-turn agent training.
We first establish \ac{PPO} notation, then present the group-relative baseline principle introduced by \ac{GRPO} along with subsequent refinements (Dr.
\,\ac{GRPO}, DAPO), and conclude with \ac{GSPO}'s sequence-level importance weighting that addresses stability challenges in variable-length trajectories.

\subsection{Preliminaries}
\label{subsec:preliminaries}
An autoregressive language model with parameters $\theta$ is treated as a policy $\pi_\theta$ over sequences.
Let $\mathcal{D}$ denote a set of queries, with $x \in \mathcal{D}$, and let $y=(y_1,\ldots,y_{|y|})$ be a tokenized response.
The sequence likelihood factorizes as \begin{equation}
\pi_\theta(y \mid x) \;=\; \prod_{t=1}^{|y|} \pi_\theta\!\big(y_t \mid x,\, y_{<t}\big).
\end{equation} A reward function $r$ assigns a scalar outcome to a completed pair, with $r(x,y)\in[0,1]$.
Expectations are taken over $x \sim \mathcal{D}$ and over responses sampled from a frozen behavior policy $\pi_{\theta_{\text{old}}}$ that generated the data for the current update.
The operator $\operatorname{clip}(w)$ denotes clipping importance ratio $w$ into the symmetric range $(1-\epsilon,\, 1+\epsilon)$ controlled by $\epsilon>0$; when asymmetric clipping is used, we write the bounds explicitly.
A frozen reference model is denoted $\pi_{\mathrm{ref}}$ and a \ac{KL} penalty to this reference is written $\beta\,D_{\mathrm{KL}}(\pi_\theta\| \pi_{\mathrm{ref}})$ where $\beta\geq 0$ controls the penalty strength.

This section surveys policy optimization methods for \acp{LLM}, presenting their original formulations to establish context for our adoption of \ac{GSPO}.
Each method's original design with respect to \ac{KL} regularization is preserved in the presentation: \ac{PPO}, \ac{GRPO}, and Dr.
\,\ac{GRPO} include an optional \ac{KL} penalty to $\pi_{\mathrm{ref}}$, whereas DAPO and \ac{GSPO} do not.
Our actual training methodology—using \ac{GSPO} with a light \ac{KL} stabilizer for small effective batch sizes—is detailed in \cref{ch:method}.

\subsection{\acl{PPO}}
\label{subsec:ppo}
\acs{PPO} performs multiple gradient steps on samples drawn from the old policy while constraining update size through clipping \cite{ppo2017}.
With per-token importance ratio \begin{equation}
w_t(\theta) \;=\; \frac{\pi_\theta(y_t \mid x, y_{<t})}{\pi_{\theta_{\text{old}}}(y_t \mid x, y_{<t})},
\end{equation} and averaging across the sequence length, the objective is \begin{equation}
J_{\mathrm{PPO}}(\theta)
=\;
\mathbb{E}
\left[
\frac{1}{|y|}
\sum_{t=1}^{|y|}
\min\!\big(
w_t\,\hat{A}_t,\;
\operatorname{clip}(w_t)\,\hat{A}_t
\big)
\right]
\;-\;\beta\,D_{\mathrm{KL}}(\pi_\theta\| \pi_{\mathrm{ref}}).
\end{equation} The averaging by $|y|$ normalizes for length so long responses do not dominate the minibatch objective.
We use terminal outcome supervision with no discounting, so the return at every prefix equals the final outcome $R := r(x,y)\in[0,1]$: \begin{equation}
G_t \;=\; R \quad \text{for all } t \in \{1,\ldots,|y|\},
\qquad
\hat{A}_t \;=\; G_t \;-\; V_\phi(x, y_{<t}) \;=\; R \;-\; V_\phi(x, y_{<t}).
\end{equation} Intuitively, the advantage contrasts the realized outcome with what the value model expected at each prefix: positive values increase the probability of $y_t$, negative values decrease it, and values near zero leave it largely unchanged.

Maintaining a separate value model comparable in size to the policy substantially increases the memory footprint during training; this overhead is exacerbated for long responses, as both models must process extended sequences in parallel.

\subsection{\acl{GRPO}}
\label{subsec:grpo}
\acs{GRPO} replaces the learned value baseline with a group-relative baseline computed from multiple responses to the same query \cite{grpo2024}.
For each $x \in \mathcal{D}$, the behavior policy $\pi_{\theta_{\text{old}}}$ samples a group $\{y_i\}_{i=1}^G$ and each response receives an outcome $r(x,y_i)\in[0,1]$.
Define the group mean, standard deviation, and standardized advantage: \begin{equation}
\mu \;=\; \frac{1}{G}\sum_{i=1}^{G} r(x,y_i),
\quad
\sigma \;=\; \sqrt{\frac{1}{G}\sum_{i=1}^{G}\big(r(x,y_i)-\mu\big)^{2}},
\quad
A_i \;=\; \frac{r(x,y_i)-\mu}{\sigma + \varepsilon}.
\end{equation} With per-token ratios $w_{i,t}=\frac{\pi_\theta(y_{i,t}\mid x,y_{i,<t})}{\pi_{\theta_{\text{old}}}(y_{i,t}\mid x,y_{i,<t})}$, the surrogate mirrors \ac{PPO}'s clipped form while averaging first within a sequence and then across the group: \begin{equation}
J_{\mathrm{GRPO}}(\theta)
=\;
\mathbb{E}\left[
\frac{1}{G}\sum_{i=1}^{G}
\frac{1}{|y_i|}\sum_{t=1}^{|y_i|}
\min\big(
w_{i,t}\,A_i,\;
\operatorname{clip}(w_{i,t})\,A_i
\big)
\right]
\;-\;\beta\,D_{\mathrm{KL}}(\pi_\theta\| \pi_{\mathrm{ref}}).
\end{equation} \ac{GRPO} preserves the \ac{PPO} update logic but substitutes a per-query group baseline for the critic, removing the value model, lowering memory, and simplifying training while keeping the clipped-ratio safeguard and on-policy sampling \cite{grpo2024}.

\subsubsection{Dr.\,GRPO}
\label{subsubsec:drgrpo}
Dr.
\,GRPO identifies two biases in the \ac{GRPO} objective and removes the corresponding normalization terms \cite{drGRPO2025}.
First, dividing each sample's loss by $|y_i|$ introduces a response-level length bias.
Second, dividing by the group standard deviation $\sigma$ couples update magnitude to the within-group difficulty spread.
Dr.
\,GRPO therefore uses an unnormalized, unbiased group advantage
\begin{equation}
A_i^{\text{Dr}} \;=\; r(x,y_i) \;-\; \frac{1}{G}\sum_{j=1}^{G} r(x,y_j)
\end{equation}
and removes response-level length averaging:
\begin{equation}
J_{\mathrm{DrGRPO}}(\theta)
=\mathbb{E}\left[
\frac{1}{G}\sum_{i=1}^{G}
\sum_{t=1}^{|y_i|}
\min\big(
w_{i,t}\,A_i^{\text{Dr}},\;
\operatorname{clip}(w_{i,t})\,A_i^{\text{Dr}}
\big)
\right]
\;-\;\beta\,D_{\mathrm{KL}}(\pi_\theta\| \pi_{\mathrm{ref}}).
\end{equation}
In practice the inner sum is implemented as a masked mean over all tokens in the batch with a constant denominator rather than $|y_i|$ per sample.
Removing the two normalizations improves token efficiency and reduces the tendency to inflate the length of incorrect responses \cite{drGRPO2025}.

\subsubsection{\acf{DAPO}}
\label{subsubsec:dapo}
\acs{DAPO} is a large-scale recipe that builds on \ac{GRPO} with four concrete changes that target entropy collapse, reward noise, and instability \cite{dapo2025}.
The objective uses asymmetric clipping and aggregates at token level across the whole group: \begin{equation}
J_{\mathrm{DAPO}}(\theta)
=\;
\mathbb{E}\left[
\frac{1}{\sum_{i=1}^{G} |y_i|}
\sum_{i=1}^{G}
\sum_{t=1}^{|y_i|}
\min\big(
w_{i,t}\,A_i,\;
\operatorname{clip}(w_{i,t},\, 1-\epsilon_{\text{low}},\, 1+\epsilon_{\text{high}})\,A_i
\big)
\right].
\end{equation} Here $A_i$ is the group-standardized advantage as in \ac{GRPO}.
Unlike the symmetric clipping used in \ac{PPO}, \ac{GRPO}, and Dr.
\,\ac{GRPO}, \ac{DAPO} employs asymmetric bounds with $\epsilon_{\text{high}} > \epsilon_{\text{low}}$, allowing stronger positive reinforcement while constraining negative updates more tightly.
This asymmetry mitigates training collapse, since excessive negative reinforcement can destabilize the policy.
The algorithm removes the \ac{KL} penalty, uses token-level loss, and adds two stabilizers: \emph{dynamic sampling} with a buffer that enforces a balanced mix of correct and incorrect samples for each query group, and \emph{overlong filtering and shaping} that masks loss on truncated or timed-out generations and downweights overlong failures.
Together, the asymmetric clipping, dynamic buffer, and overlong filtering make training robust for very long reasoning traces while preventing early entropy collapse.

\subsubsection*{\acf{GSPO}}
\label{subsubsec:gspo}
\acs{GSPO}~\cite{gspo2025} addresses critical stability challenges in group-relative policy optimization, particularly for variable-length sequences and \ac{MoE} architectures.
The algorithm tackles two fundamental problems that limit the robustness of token-level importance weighting.
First is aligning the importance weighting to sequence-level advantages.
Second is handling small logit discrepancies between training and inference engines that accumulate across long sequences.
Token-level importance weighting in \ac{GRPO} is fragile at scale: the same set of weights operating on the same input sequences may not produce identical logits between training and inference engines due to accumulated floating-point error and differing kernel implementations.
In sparse \ac{MoE}, the logit differences are even more apparent, since the set of activated experts can change between $\pi_{\theta_{\text{old}}}$ and $\pi_\theta$, making token-level ratios volatile and invalid as off-policy corrections.
These effects amplify under clipping and have been observed to trigger irreversible collapse; prior \ac{MoE} work required routing replay to stabilize \ac{GRPO} \cite{gspo2025}.
\emph{\ac{GSPO}} restores a coherent importance sampler by matching the optimization unit to the reward unit: entire responses.
With the same group-relative advantage $A_i$ as in \ac{GRPO}, define the \emph{length-normalized sequence-level} importance ratio \begin{equation}
s_i(\theta)
=\;
\left(\frac{\pi_\theta(y_i \mid x)}{\pi_{\theta_{\text{old}}}(y_i \mid x)}\right)^{\!1/|y_i|}
\;=\;
\exp\left(
\frac{1}{|y_i|}\sum_{t=1}^{|y_i|}
\log\frac{\pi_\theta(y_{i,t}\mid x, y_{i,<t})}{\pi_{\theta_{\text{old}}}(y_{i,t}\mid x, y_{i,<t})}
\right),
\end{equation} and optimize the clipped sequence-level surrogate \begin{equation}
J_{\mathrm{GSPO}}(\theta)
=\;
\mathbb{E}\left[
\frac{1}{G}\sum_{i=1}^{G}
\min\big(
s_i\,A_i,\;
\operatorname{clip}(s_i, 1{-}\epsilon_{\text{low}}, 1{+}\epsilon_{\text{high}})\,A_i
\big)
\right].
\end{equation} Length normalization in $s_i$ keeps ratios numerically comparable across response lengths and prevents a few token-level logit differences from exploding the sequence weight; the resulting clipping ranges are therefore orders of magnitude smaller than in token-level schemes \cite{gspo2025}.
Like \ac{DAPO}, \ac{GSPO} employs asymmetric clipping bounds to allow stronger positive reinforcement while constraining negative updates.
The gradient makes the stability contrast explicit (omitting clipping for brevity): \begin{align}
\nabla_\theta J_{\mathrm{GSPO}}(\theta)
\;=\;
\mathbb{E}\!\left[
\frac{1}{G}\sum_{i=1}^{G}
s_i(\theta)\,A_i\;\cdot\;
\frac{1}{|y_i|}\sum_{t=1}^{|y_i|}
\nabla_\theta \log \pi_\theta(y_{i,t}\mid x, y_{i,<t})
\right],
\end{align} so all tokens in a response are weighted equally by the same sequence factor $s_i(\theta)$.
In \ac{GRPO}, by contrast, each token carries its own noisy ratio $w_{i,t}$, and those unequal weights accumulate unpredictably along the sequence.
\ac{GSPO}'s sequence weighting eliminates that instability, removes the need for routing replay in \acs{MoE} \ac{RL}, and tolerates training–inference log-probability precision differences—often allowing direct use of inference-engine likelihoods without recomputation.
By default \ac{GSPO} does not use an explicit \ac{KL} penalty; when we work with very small effective batch sizes, we sometimes introduce a light \ac{KL} term purely as a stabilizer and mark this deviation explicitly.

The superior robustness of \ac{GSPO} makes it particularly attractive for multi-turn agent training where episodes vary substantially in length and structure.
By matching the importance weighting granularity to the reward granularity—both at the sequence level—\ac{GSPO} provides more stable gradients than token-level approaches when training on variable-length trajectories with sparse terminal rewards.
These properties led to our adoption of \ac{GSPO} as the policy optimization algorithm for the Nano agent training described in Chapter~\ref{ch:method}.

\section{\acl{LoRA} (\acs{LoRA})} \label{sec:lora}

Parameter-efficient adaptation is essential for training large models without the overhead of full fine-tuning.
\acl{LoRA} learns a low-rank update to frozen weights \cite{lora2021}:
\begin{equation}
\Delta W = BA,\qquad W = W_0 + \frac{\alpha}{r}\,BA,
\end{equation}
with $B \in \mathbb{R}^{d\times r}$, $A \in \mathbb{R}^{r\times k}$, and rank $r \ll \min(d,k)$.
Initializing $B{=}0$ preserves the base model at the start ($W{=}W_0$), which stabilizes optimization.
For online \acl{RL} with coding agents, \ac{LoRA} reduces trainable state, supports clean policy-reference separation (adapters differ, base is shared), and accelerates iteration.
Adapter placement is flexible—practitioners select which modules to adapt based on capacity/efficiency goals; rank $r$ and scale $\alpha$ set the capacity-efficiency trade-off.

With the theoretical and practical foundations established, we now examine the benchmarks used to evaluate code repair systems in realistic settings.

\section{Training and Evaluation Datasets}
\label{sec:training-evaluation-datasets}

Effective training and evaluation of code repair agents requires diverse, realistic datasets that capture the complexity of real-world debugging scenarios across multiple programming languages.

\subsection{Training Datasets}
\label{subsec:training-datasets}

Training employs a 1,000-task curriculum combining Python and multilingual data.
SWE-Gym~\cite{sweGym2025} provides approximately 2,400 Python bug-fixing tasks extracted from real GitHub repositories.
We select 750 of these tasks for the training curriculum.
SWE-Bench-Multilingual~\cite{sweBenchMultilingual2025} offers debugging tasks spanning nine programming languages: Rust, Java, PHP, Ruby, JavaScript, TypeScript, Go, C, and C++.
From this collection, we incorporate 250 tasks into training and reserve 50 as a held-out evaluation set.
Instruction-driven, repository-level multilingual debugging datasets remain substantially scarcer than Python-only resources, constraining the scale of cross-language training currently feasible in the field.

\subsection{Evaluation Benchmarks}
\label{subsec:evaluation-benchmarks}

SWE-Bench-Verified~\cite{sweBenchVerified2024} serves as the primary evaluation benchmark, containing approximately 500 carefully validated Python debugging tasks with deterministic test outcomes.
This benchmark provides reliable performance measurement for repository-level repair capabilities.
Multilingual generalization is assessed on the 50-task SWE-Bench-Multilingual holdout (disjoint from training), reporting patch-similarity metrics per language.
Scaffold transfer experiments evaluate zero-shot and few-shot performance on Mini-SWE-Agent, Aider, and OpenHands harness interfaces, measuring whether behaviors learned in Nano generalize across different tool-use protocols.

Having examined the theoretical foundations, we now position our work within the broader landscape of concurrent research.

\section{Summary and Research Positioning}
\label{sec:summary-positioning}

\textbf{DeepSWE}~\cite{deepSWE2025} is highly similar to our work and validates our approach.
That is, they apply \ac{GRPO} on multi-turn, terminal-based coding agents with an asynchronous inference engine setup.
They used 64 H100s for training, 8 H100s for inference, and trained for 6 days on R2EGym, totaling ~$10368$ H100-hours.
In contrast, we train for ~2 days on 6 A100s (~$288$ A100-hours).
Normalizing by throughput, we conservatively estimate that $1\text{ H100-hour} \approx 2.2\text{ A100-hours}$; thus, we use ~$80\times$ less compute.
They used test-based rewards.
In contrast, we adopt an execution-free, patch-similarity reward, building on the patch-based signal introduced by SWE-RL~\cite{wei2025swerladvancingllmreasoning}.
This choice reduces engineering friction—avoiding per-repository test harnesses and heavyweight containerization—and enables concurrent training across heterogeneous environments and programming languages.
Our approach is also significantly more compute-efficient, as noted above; detailed design considerations are deferred to the methods chapter.

This chapter has established the theoretical and empirical foundations for online \ac{RL} applied to \ac{APR}.
By examining the evolution from rule-based repair systems to modern \ac{LLM}-based approaches, we identified fundamental limitations in current paradigms: the mismatch between static training and the dynamic nature of debugging.

Our review \ac{RL} techniques, particularly the progression from \ac{PPO} to group-relative methods culminating in \ac{GSPO}, demonstrates how computational efficiency and training stability can be achieved through careful algorithmic design without sacrificing theoretical guarantees.
The examination of existing coding agents reveals a critical gap—all current systems operate at inference time only, missing the opportunity for models to learn optimal tool usage through experience.

The Nano agent architecture presents an alternative approach to agent design.
By providing only essential tools and relying on learning, we demonstrate that effective debugging capabilities can emerge from simple interfaces.

This thesis makes three fundamental contributions to the field:

\textbf{One of the first open-source online \ac{RL} implementations for \acp{LLM}}: We provide the research community with complete infrastructure for training language models through interactive environmental experience.
This democratizes access to techniques we believe are already employed by leading industry labs but have remained proprietary.
Our implementation enables researchers to explore online learning across diverse domains beyond code repair.

\textbf{Minimalist agent approach}: We demonstrate that a simple agent design with basic tools can achieve strong performance when combined with reinforcement learning, showing that sophisticated debugging behaviors can emerge from minimal interfaces.

\textbf{Bridge between research and practice}: By demonstrating that models can learn effective debugging strategies through \ac{RL} in realistic environments, we bridge the gap between academic research on program repair and practical tools used by developers.
Our approach shows that the future of coding assistance lies not in ever-more-sophisticated prompt engineering, but in models that learn from experience.

\subsection{Limitations of existing work}
\label{sec:limitations-existing}
\textbf{Focus on generation rather than repair}: Existing approaches predominantly emphasize code generation, yet creating new code differs fundamentally from the process of debugging and repairing existing systems.
This distinction is crucial, as effective automated program repair requires models to reason about and modify complex, pre-existing codebases rather than synthesizing code from scratch.

\textbf{Isolated task formulation}: Many prior works formulate tasks at the level of single functions, thereby neglecting the intricate interdependencies and architectural complexity inherent in real-world software systems.
This simplification limits the ability of models to generalize to practical debugging scenarios, where understanding interactions across multiple files and modules is essential.

\textbf{Lack of environmental interaction}: Previous methods typically do not incorporate exploration, tool usage, or iterative refinement into their training regimes.
As a result, models are deprived of opportunities to learn through active engagement with their environment, which is a key aspect of effective debugging and repair in realistic settings.

\textbf{Dependence on test-based rewards}: While relying on test-based rewards can, in principle, provide robust supervision, this strategy becomes increasingly impractical as the number of supported programming languages expands.
Constructing and maintaining comprehensive test suites for each language introduces significant operational challenges, thereby limiting the scalability and applicability of these approaches.

As we proceed to describe our method in detail, keep in mind that our goal is not merely to improve performance metrics on benchmarks, but to demonstrate a fundamentally different, nascent training approach that enables \acp{LLM} to learn not from labeled data points, but from experience.
