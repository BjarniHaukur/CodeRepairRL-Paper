\chapter{Theoretical Background and Related Work}
\label{ch:background}

This chapter establishes the theoretical and empirical background for online \ac{RL} applied to \ac{APR} with tool-using \acp{LLM}.
We first situate \ac{APR} historically, from search- and learning-based methods before \acp{LLM} to contemporary, repository-grounded systems, and introduce a simple taxonomy spanning manual, agentless, and agentic workflows.
We then formalize agentic repair as a history-based \ac{MDP} driven by structured tool calls, and review optimization methods for training such policies, including \ac{PPO}, \ac{GRPO}, and \ac{GSPO}, as well as practical issues such as variance collapse.
Complementary techniques for scalable training (parameter-efficient adaptation via \ac{LoRA} and sampling choices that balance exploration and exploitation) are summarized alongside evaluation resources such as \ac{SWE-Bench} and \ac{TauBench}.
We close with related work and limitations that motivate the minimalist agent and online training recipe developed in the following chapters.

\section{Automated Program Repair before \acsp{LLM}}\label{sec:apr-pre-llm}

Prior to \acp{LLM}, empirical \ac{APR} often evaluated on test-suite-based Java benchmarks and small algorithmic suites.
\textit{Defects4J} and the recent \textit{GitBug\textendash Java} provide full projects with reproducible builds and JUnit tests, enabling repository\textendash level reasoning; in practice, many repair systems nevertheless restricted themselves to local search spaces and single\textendash file patches for tractability~\cite{defects4J2014,gitbugJava2024}.
In contrast, \textit{QuixBugs} is explicitly snippet\textendash level: a collection of small standalone Java/Python programs with single\textendash line defects and accompanying tests~\cite{quixBugs2017}.

Within this setting, three families of approaches dominated.
\emph{Generate\textendash and\textendash validate} systems search a mutation space and accept candidates that satisfy the tests, with GenProg as a canonical example and PAR illustrating pattern\textendash guided edits~\cite{genProg2012,par2013}.
\emph{Semantics\textendash based} repair relies on program analysis—symbolic execution and constraint solving—to synthesize expressions at instrumented locations, as in SemFix~\cite{semFix2013}.
\emph{Learning\textendash guided} methods add statistical priors to prioritize candidates; Prophet learns a patch ranking model from past fixes~\cite{prophet2016}.
These lines collectively established feasibility on test\textendash suite benchmarks while also exposing the limits of test adequacy as a correctness proxy (overfitting, brittle generalization).
Those constraints, together with the need to reason beyond local contexts, set the stage for the repository\textendash grounded, model\textendash based approaches reviewed next.

\section{Automated Program Repair with LLMs}\label{sec:apr-with-llms}

Codex marked an inflection point where large pretrained models on code enabled practical repair and synthesis, shifting \ac{APR} from rules or search to learned edit priors~\cite{codex2021}.
Since then, frontier labs have converged on repository-grounded evaluation (e.g., \ac{SWE-Bench} Verified) and increasingly ship first-party \emph{agent harnesses} such as OpenAI's Codex \ac{CLI}, Anthropic's Claude Code, and Google's Gemini \ac{CLI} that orchestrate tool calls over live repositories~\cite{sweBench2024,sweBenchVerified2024,openaiCodexCLIRepo,claudeCode,geminiCLIRepo}.

We therefore organize \ac{LLM}-assisted repair by the degree of autonomy at inference: (i) \emph{scaffold-free} models operating without an execution harness, (ii) \emph{agentless} scaffolds that curate repository context without centralized autonomy, and (iii) \emph{agentic} systems that plan and execute tools end-to-end.
This taxonomy clarifies which capabilities are evaluated and how results transfer across settings.

\subsection{Scaffold-free}\label{subsec:scaffold-free}
Scaffold-free approaches frame repair as translation from buggy code to fixed code.
The model receives a localized and self-contained context, typically a snippet or a single file with a narrow edit window, and proposes a patch in one interaction turn.
There is no repository traversal, no tool use during generation, and no execution signal in the loop.
Any tests are run outside the model after the fact.

Within this regime we focus on post-training methods that adapt an existing \ac{LLM} for repair.
A representative example is \emph{RepairLLaMA}, which fine-tunes a general code model on curated bug-fix pairs so that it proposes minimal and targeted edits without relying on an execution harness~\cite{repairllama2023}.
A parallel thread brings \ac{RL} into the same scaffold-free setting at the function or snippet level.
\emph{CodeRL} optimizes code generation with unit test execution as feedback while operating on localized contexts \cite{codeRL2022}.

These setups are attractive because data and pipelines are simple, adaptation is straightforward, and inference is fast.
It provides an effective baseline for measuring translation quality in localized contexts.
Its limitations follow from the design: without an execution loop the model cannot test hypotheses, observe runtime behavior, or coordinate edits across files.
Repository state and build systems remain external to generation.

\subsection{Agentless}\label{subsec:agentless}
Moving beyond scaffold-free translation, \emph{agentless} systems typically operate over a live repository while keeping execution outside the model.
A thin scaffold retrieves relevant files, builds diffs, and structures the prompt; the model returns patch blocks or edit intents; a human or scripted harness runs tests and shell commands and applies the edits.
This preserves repository grounding and repeatability without autonomous tool use.

We treat agentless systems as interfaces rather than agents: they make the contributions of context curation and patch formatting explicit and easy to ablate.
In practice, components inside the scaffold (retrievers, rankers, patch validators) can be trained, and logs from these pipelines support offline learning or imitation without exposing the model to a full tool loop.
Representative analyses show that a simple three-stage pipeline (localize, repair, validate) can match or exceed more complex setups when the scaffold is well-engineered~\cite{xia2024agentlessdemystifyingllmbasedsoftware}.
Related work begins to introduce reinforcement signals in repository settings while still short of a fully autonomous terminal loop, e.g., SWE-RL for repository-level reasoning~\cite{wei2025swerladvancingllmreasoning}.

The limitation is structural.
Because execution remains outside the model, the scaffold decides what to surface, when to run tests, and how to apply or revert edits.
The model does not learn the decision-making that drives repair.
What to try next, when to backtrack, when to submit or abandon a patch; because those choices live in the interface rather than in the policy.
The reward signal therefore entangles scaffold choices with model output, and the resulting histories are not suitable for end-to-end training.
Agentless setups are excellent for reproducible evaluation and for collecting logs, but they are a poor fit when the goal is to train a model to use tools by itself.

\todoinline{State that these methods often use LLMs as "functions" in these systems instead of end-to-end processes, meaning conversation histories are modified / restarted frequently which makes end-to-end learning hard / infeasible}

\subsection{Agentic}\label{subsec:agentic}

Agentic systems operate directly over a live repository and let the model initiate actions.
The model can open and edit files, run tests and shell commands, read raw outputs, and iterate until a patch is ready.
Public research harnesses make this concrete: \emph{SWE-Agent} formalizes an agent-computer loop for software tasks, \emph{OpenHands} provides a general terminal-based platform for coding agents, and \emph{mini-swe-agent} shows that a small, disciplined tool set in a tight loop is effective~\cite{sweAgent2024,openHands2024, miniSWEAgentRepo}.
Our Nano agent (\cref{sec:nano-agent}) adopts a similar minimalist philosophy, providing only essential terminal commands and file patching capabilities, allowing effective debugging behaviors to emerge through \ac{RL}.

Operationally, these systems rely on structured tool calls, typically via \acs{JSON}.
The model emits a well-formed function call with arguments, an external executor runs it, and the resulting output is returned to the model.
\begin{verbatim}
{
  "function_name": "apply_patch",
  "arguments": {
    "file_path": "src/utils.py",
    "old_content": "def buggy_function():",
    "new_content": "def fixed_function():"
  }
}
\end{verbatim}

This approach transforms \acp{LLM} from passive text generators into systems capable of interacting with their environment by invoking tools and observing their results.

This shift has also shaped how models are packaged and evaluated: recent open-weight releases pair model checkpoints with first-party harnesses and report agent-style evaluations, while disclosing only partial training details.
We summarize representative examples.

\paragraph{Qwen3\textendash Coder (Alibaba/Qwen team).}
Qwen3\textendash Coder is positioned as an \emph{agentic} coding model and ships with a first-party command-line harness (\emph{Qwen Code}) that drives repository-grounded autonomous workflows~\cite{qwen3CoderBlog2025,qwenCodeCLI}.
The release materials emphasize agent behavior in realistic development loops and demonstrate how the paired harness surfaces those capabilities.
At the same time, the post-training recipe, data generation, reward design, and optimization details is described only at a high level.
Weights and the harness are available, but a reproducible end-to-end training pipeline is not.

\paragraph{Kimi\textendash K2 (Moonshot AI).}
Kimi-K2 details an \emph{agentic} data synthesis pipeline paired with a joint reinforcement learning stage that mixes real and synthetic environment interactions.
\cite{kimiK2_2025}
The stated goal is to shape multi-step decision making—tool selection, iterative trial-and-error, and revision—rather than single-pass text generation.
The report frames this as necessary for repository-grounded coding and other multi-step tasks.
As with Qwen3, model artifacts and high-level methodology are public, but \emph{no} training recipe.

\paragraph{GLM\textendash 4.5 (Zhipu AI).}
GLM\textendash 4.5 positions itself as an \emph{agentic, reasoning, and coding} (ARC) family and explicitly reports a comprehensive post-training phase that combines expert iteration with \ac{SFT} and \ac{RL}, aimed at strengthening multi-step agentic tool use.
~\cite{glm45_2025}.
The public release provides open-weight checkpoints but does \emph{not} include a reproducible end-to-end agentic RL training pipeline or environment traces.

Taken together, these open-weight efforts converge on the same direction: agent-facing post-training that couples tool use, long-context coding, and multi-turn control.
They document \ac{SFT} and \ac{RL} training stages for agentic post-training and evaluations, but stop short of releasing reproducible training stacks.
In contrast, recent work such as \emph{DeepSWE} publicly reports an RL-trained coding agent within a terminal environment and releases open-weight models alongside practical training details, offering a rare end-to-end reference point for open agentic RL on software tasks.
\cite{deepSWE2025}

In practical terms, agentic systems close the loop between hypothesis and verification: the model requests an action, observes the real outcome, and can adjust.
This makes online \ac{RL} feasible and aligns the learning signal with the deployed behavior.
By contrast with agentless setups, the model is exposed to the full decision process of repair rather than a curated slice, so the resulting histories are suitable for end-to-end training.

Having established how agentic systems enable end-to-end learning, we now turn to the theoretical foundations of training such systems through reinforcement learning.

\section{\acl{RL} for Language Models}
\label{sec:rl-language-models}

This section fixes notation and states the \acl{PPO} objective that will serve as our baseline.
Subsequent sections build on this with \acl{GRPO} and \acl{GSPO}.
\subsection{Preliminaries}
\label{subsec:preliminaries}

An autoregressive language model with parameters $\theta$ is treated as a policy $\pi_\theta$ over sequences.
Let $\mathcal{D}$ denote a set of queries, with $x \in \mathcal{D}$, and let $y = (y_1,\ldots,y_{|y|})$ be a tokenized response.
The sequence likelihood factorizes as \begin{equation}
\pi_\theta(y \mid x) \;=\; \prod_{t=1}^{|y|}\, \pi_\theta\!\big(y_t \mid x,\, y_{<t}\big).
\end{equation} A reward function $r$ assigns a scalar outcome to a completed pair, with $r(x,y) \in [0,1]$.
Expectations are taken over $x \sim \mathcal{D}$ and over responses sampled from a frozen reference policy $\pi_{\theta_{\text{old}}}$ that generated the data for the current update.
The operator $\operatorname{clip}(\cdot, 1-\epsilon_1, 1+\epsilon_2)$ denotes elementwise clipping of importance ratios into a range controlled by $\epsilon_1, \epsilon_2 \!
>\! 0$.
We omit the customary $\mathrm{KL}$ regularizer to a reference model both in practice and for clarity.

\subsection{\acl{PPO}}
\label{subsec:ppo}

\acl{PPO} performs multiple gradient steps on samples drawn from the old policy while constraining update size through clipping \cite{ppo2017}.
Using per token importance ratios and averaging across the sequence length, the objective is \begin{equation}
J_{\mathrm{PPO}}(\theta)
\;=\;
\mathbb{E}_{\,x \sim \mathcal{D},\; y \sim \pi_{\theta_{\text{old}}}(\cdot \mid x)}
\left[
\frac{1}{|y|}
\sum_{t=1}^{|y|}
\min\!\Big(
w_t(\theta)\,\hat{A}_t,\;
\operatorname{clip}\!\big(w_t(\theta),\, 1-\epsilon_1,\, 1+\epsilon_2\big)\,\hat{A}_t
\Big)
\right],
\end{equation} where the per token importance ratio compares next token probabilities under the current and old policies, \begin{equation}
w_t(\theta)
\;=\;
\frac{\pi_\theta\!\big(y_t \mid x,\, y_{<t}\big)}
{\pi_{\theta_{\text{old}}}\!\big(y_t \mid x,\, y_{<t}\big)}.
\end{equation} The averaging by $|y|$ normalizes for length so that long responses do not dominate the minibatch objective.
The term $\hat{A}_t$ is an advantage supplied by a learned value model.
This thesis uses terminal outcome supervision with no discounting, so the return at every prefix equals the final outcome $R := r(x,y) \in [0,1]$: \begin{equation}
G_t \;=\; R \quad \text{for all } t \in \{1,\ldots,|y|\}.
\end{equation} Accordingly, the per token advantage takes the exact form \begin{equation}
\hat{A}_t \;=\; G_t \;-\; V_\phi\!\big(x, y_{<t}\big)
\;=\; R \;-\; V_\phi\!\big(x, y_{<t}\big).
\end{equation} Intuitively, the advantage contrasts the realized outcome with what the model expected at each prefix: positive values increase the probability of $y_t$, negative values decrease it, and values near zero leave it largely unchanged.
In \ac{LLM} settings, maintaining a separate value model that is comparable in size to the policy substantially increases the overall memory footprint during training.
This elevated memory demand becomes especially pronounced for long responses, as both the policy and value models must process extended sequences in parallel, leading to significant resource consumption.

\subsection{\acl{GRPO}}
\label{subsec:grpo}

Good algorithm,

but the following changes have been proposed to mitigate some downfalls.
% The following is from DeepSWE and I want the discussion on these papers to be condition on this% Clip High (DAPO): Increasing the upper bound of GRPO/PPO’s surrogate loss encourages exploration and stabilizes entropy.% No KL Loss (DAPO): Eliminating KL loss prevents the LLM from being constrained to the trust region of the original SFT model.% No Reward Standard Deviation (Dr.GRPO): Removing reward standard deviation removes difficulty bias in GRPO’s loss, ensuring hard and easy problems are better differentiated.% Length Normalization (Dr.GRPO): Dividing surrogate loss by max context length removes length bias present in GRPO, which increases the length of incorrect responses.% Compact Filtering (Us): Inspired by DAPO, we mask the loss for trajectories that reach max context length, timeout during generation (20 minutes), or reach maximum steps. Described further below.

\subsubsection{Dr.
GRPO} \label{subsubsec:drgrpo}

length norm, no reward scaling

\subsubsection{DAPO} \label{subsubsec:dapo} clip high, no KL, etc.

length normalization?

\subsubsection{GSPO}

% \subsection{Advantages for Multi-Turn Tool Use}
% \label{subsec:gspo-advantages}

% For our coding agent training scenario, \ac{GSPO} provides several crucial benefits:

% \textbf{Numerical Stability}: Sequence-level operations are more robust to the logit discrepancies between vLLM inference and training computation, preventing the accumulation of numerical errors that plague token-level methods.

% \textbf{MoE Compatibility}: The Qwen team demonstrated that \ac{GSPO} inherently stabilizes Mixture-of-Experts training without additional interventions, opening possibilities for scaling to larger, more efficient architectures.

% Empirical results from the Qwen3 models show that \ac{GSPO} not only stabilizes training but also achieves superior final performance compared to \ac{GRPO}, validating its theoretical advantages in practice.
% For our work, adopting \ac{GSPO} represents a critical engineering decision that enables stable, long-duration training of coding agents through thousands of multi-turn interactions.

% While the choice of optimization algorithm addresses training stability, the computational demands of fine-tuning billion-parameter models necessitate parameter-efficient approaches.

\section{Low-Rank Adaptation (LoRA)}
\label{sec:lora}

Parameter-efficient fine-tuning has become essential for adapting large language models to specific tasks without the computational overhead of full fine-tuning.
Low-Rank Adaptation (\ac{LoRA}) represents one of the most successful approaches in this domain.

\subsection{Mathematical Foundation}
\label{subsec:lora-math}

\ac{LoRA} is based on the hypothesis that weight updates during fine-tuning have low intrinsic rank.
For a pre-trained weight matrix $W_0 \in \mathbb{R}^{d \times k}$, \ac{LoRA} represents the update $\Delta W$ as:

\begin{equation}
\Delta W = BA
\end{equation}

where $B \in \mathbb{R}^{d \times r}$ and $A \in \mathbb{R}^{r \times k}$ with rank $r \ll \min(d,k)$.
The adapted weight becomes:

\begin{equation}
W = W_0 + \Delta W = W_0 + BA
\end{equation}

During training, $W_0$ remains frozen while only $A$ and $B$ are updated.
This dramatically reduces the number of trainable parameters from $dk$ to $r(d+k)$.

\subsection{Implementation Details}
\label{subsec:lora-implementation}

\subsubsection{Initialization Strategy}

\ac{LoRA} uses a specific initialization scheme to ensure training stability:
\begin{itemize}
	\item Matrix $A$ is initialized using random Gaussian values
	\item Matrix $B$ is initialized to zero, ensuring $\Delta W = BA = 0$ at the start
	\item This guarantees that the adapted model initially behaves identically to the pre-trained model
\end{itemize}

\subsubsection{Scaling Factor}

The \ac{LoRA} update is typically scaled by a factor $\alpha/r$ where $\alpha$ is a hyperparameter:

\begin{equation}
W = W_0 + \frac{\alpha}{r}BA
\end{equation}

This scaling allows for consistent learning rates across different rank values and provides a simple way to control adaptation strength.

\subsection{Computational Advantages}
\label{subsec:lora-advantages}

\ac{LoRA} offers substantial practical benefits for \ac{RL} training:

\textbf{Memory Efficiency}: With typical rank values $r = 8$ to $64$, \ac{LoRA} reduces trainable parameters by 99\%+ for large models, dramatically lowering \ac{GPU} memory requirements.

\textbf{Training Speed}: Fewer parameters mean faster gradient computation and reduced communication overhead in distributed training setups.

\textbf{Storage Efficiency}: \ac{LoRA} adapters are small (typically <100MB vs multi-GB full models), enabling efficient storage and distribution of task-specific adaptations.

\textbf{Modularity}: Multiple \ac{LoRA} adapters can be trained for different tasks and dynamically loaded, enabling flexible model deployment.

\subsection{Integration with Reinforcement Learning}
\label{subsec:lora-rl}

\ac{LoRA}'s benefits become particularly pronounced in \ac{RL} settings where multiple model instances must be maintained:

\textbf{Policy-Reference Separation}: \ac{RL} algorithms like \ac{PPO} require keeping both current and reference policies.
With \ac{LoRA}, the reference policy can share the frozen base weights while only the adapter differs.

\subsection{Theoretical Considerations}
\label{subsec:lora-theory}

Recent research has examined \ac{LoRA}'s representational capacity and limitations:

\textbf{Expressiveness}: While \ac{LoRA} cannot represent arbitrary weight updates, empirical evidence suggests that many fine-tuning scenarios do indeed have low-rank structure, making \ac{LoRA}'s constraints reasonable.

\textbf{Task Transfer}: \ac{LoRA} adapters learned for related tasks can serve as initialization for new tasks, potentially accelerating learning through transfer.

\textbf{Rank Selection}: Choosing appropriate rank values requires balancing expressiveness against efficiency.
Higher ranks provide more flexibility but reduce computational savings.

The combination of \ac{LoRA}'s efficiency with online \ac{RL} creates opportunities for more extensive experimentation and deployment of coding agents across diverse software engineering contexts.

Beyond parameter-efficient training, the choice of sampling strategy during generation plays a straightforward, yet crucial role in the exploration-exploitation trade-off fundamental to RL.

\section{Sampling Strategies}
\label{sec:sampling-strategies}

The temperature parameter in language model sampling directly controls the exploration-exploitation trade-off during inference.
Higher temperatures increase randomness and exploration of diverse solutions, while lower temperatures favor exploitation of high-probability actions.
Critically, deterministic sampling (temperature=0) prevents any exploration, making it unsuitable for reinforcement learning where the agent must discover new strategies through trial and error.
During \ac{RL} training, we typically use temperature values between 0.7 and 1.0 to balance exploration of novel debugging approaches with exploitation of learned patterns (see \cref{app:sampling-params} for detailed sampling parameters).

With the theoretical and practical foundations established, we now examine the benchmarks used to evaluate code repair systems in realistic settings.

\section{Training and Evaluation Datasets}
\label{sec:training-evaluation-datasets}

Effective training and evaluation of code repair agents requires diverse, realistic datasets that capture the complexity of real-world debugging scenarios across multiple programming languages.

\subsection{Training Datasets}
\label{subsec:training-datasets}

Our training approach utilizes two complementary datasets to enable multilingual debugging capability.
SWE-Gym~\cite{sweGym2025} provides approximately 2,400 Python bug-fixing tasks derived from real GitHub repositories, offering a substantial foundation for learning core debugging strategies.
SWE-Bench-Multilingual~\cite{sweBenchMultilingual2025} contributes approximately 300 debugging tasks across nine programming languages (Rust, Java, PHP, Ruby, JavaScript, TypeScript, Go, C, and C++), enabling cross-language generalization.
This combination allows for multilingual training while maintaining sufficient data volume for stable convergence.

\subsection{Evaluation Benchmarks}
\label{subsec:evaluation-benchmarks}

The SWE-Bench benchmark series~\cite{sweBench2024} serves as the primary evaluation framework for coding agents on realistic software engineering tasks.
SWE-Bench-Verified~\cite{sweBenchVerified2024} contains approximately 500 carefully validated Python debugging tasks that eliminate ambiguous descriptions and ensure deterministic outcomes, providing reliable performance measurement.
For cross-language evaluation, we utilize external benchmarks such as GitBug-Java~\cite{gitbugJava2024} to assess transfer capabilities to languages not seen during training.
TauBench~\cite{taubench2024} and TerminalBench~\cite{tbench_2025} enable broader assessment of whether learned debugging skills transfer to adjacent tasks beyond code repair.

% \subsubsection{Evolution of SWE-Bench}

% \textbf{Original \ac{SWE-Bench} (2023)}~\cite{sweBench2024}: The foundational dataset introduced 2,294 task instances drawn from 12 popular Python repositories. Each instance consists of:
% \begin{itemize}
% 	\item A GitHub issue describing a bug or feature request
% 	\item The repository state at issue creation time
% 	\item The developer-written patch that resolved the issue
% 	\item Test cases that fail before and pass after the patch
% \end{itemize}

% This design captures the full complexity of real software development: understanding natural language descriptions, navigating large codebases, and implementing solutions that satisfy existing tests.

% \textbf{\ac{SWE-Bench-Verified} (2024)}~\cite{sweBenchVerified2024}: Addressing quality concerns in the original dataset, this refined version includes 500 carefully validated instances that:
% \begin{itemize}
% 	\item Eliminate ambiguous issue descriptions
% 	\item Ensure deterministic test outcomes
% 	\item Remove trivial string replacements
% 	\item Verify patch minimality and correctness
% 	\item Balance difficulty across different problem types
% \end{itemize}

% Human annotators achieve 97\% success on \ac{SWE-Bench-Verified} compared to 73\% on the original, confirming the removal of problematic instances while maintaining challenging, realistic tasks.

% \textbf{\ac{SWE-Bench}
% 	Extensions}: While extensions to other programming languages have been proposed, our work focuses exclusively on the original Python-based benchmark suite, which provides the most mature and extensively validated evaluation framework.

% This diversity demonstrates the complexity and realism of the evaluation framework.

% \subsubsection{Task Format and Complexity}

% Each \ac{SWE-Bench} instance presents a naturalistic debugging scenario:

% \begin{verbatim}
% ISSUE DESCRIPTION:
% Title: DataFrame.apply() fails with axis=1 when columns have mixed types

% When calling df.apply(func, axis=1) on a DataFrame with both numeric 
% and string columns, the function receives Series with incorrect dtypes.

% Example to reproduce:
% ```python
% df = pd.DataFrame({'A': [1, 2], 'B': ['x', 'y']})
% df.apply(lambda row: row['A'] + len(row['B']), axis=1)
% # Raises TypeError
% ```

% REPOSITORY STATE:
% - 847 Python files totaling 284,000 lines
% - Complex module dependencies
% - 15,000+ test cases
% \end{verbatim}

% The evaluation system places the model in this repository state and measures whether it can produce a patch functionally equivalent to the developer's solution.
% This requires:
% \begin{itemize}
% 	\item Understanding the issue from natural language description
% 	\item Reproducing the bug through code execution
% 	\item Navigating the codebase to locate relevant modules
% 	\item Understanding existing implementation patterns
% 	\item Implementing a fix that maintains backward compatibility
% 	\item Ensuring the fix passes all existing tests
% \end{itemize}

% \subsubsection{Evaluation Methodology}

% \ac{SWE-Bench} employs rigorous evaluation protocols:

% \textbf{Functional verification}: Patches are evaluated by running the full test suite, not through textual comparison.
% This allows semantically equivalent but syntactically different solutions.

% \textbf{Isolated execution}: Each evaluation runs in a fresh Docker container to prevent cross-contamination and ensure reproducibility.

% \textbf{Time limits}: Solutions must complete within 5 minutes, reflecting real-world constraints on automated tools.

% \textbf{Minimal patches}: Credit is given only for patches that don't introduce unnecessary changes, encouraging precise solutions.

% \subsubsection{Why Success Rates Remain Low}

% Despite rapid progress in \ac{LLM} capabilities, even state-of-the-art systems achieve only ~20-25\% success rates on \ac{SWE-Bench}.
% This persistent challenge stems from several factors:

% \textbf{Repository complexity}: The average task requires understanding code distributed across 6.3 files, with deep call chains and complex dependencies.
% Current models struggle to maintain coherent understanding across such scales.

% \textbf{Ambiguity in natural language}: Issue descriptions often assume domain knowledge, use project-specific terminology, or describe symptoms rather than root causes.
% Models must infer substantial context.

% \textbf{Execution feedback requirement}: Unlike code generation tasks solvable through pattern matching, debugging requires iterative hypothesis testing through code execution—a capability most models lack.

% \textbf{Test suite complexity}: Solutions must satisfy not just the reported issue but maintain compatibility with thousands of existing tests, requiring deep understanding of system invariants.

% \textbf{Long-tail distribution}: Many bugs involve rare edge cases or unique project-specific patterns absent from training data, testing true generalization rather than memorization.

% These challenges make \ac{SWE-Bench} an ideal testbed for online training, where models can learn through environmental interaction rather than attempting single-shot solutions to complex, multi-faceted problems.

Having examined the theoretical foundations, we now position our work within the broader landscape of concurrent research.

\section{Summary and Research Positioning}
\label{sec:summary-positioning}

\textbf{DeepSWE}~\cite{deepSWE2025} is highly similar to our work and validates our approach.
That is, they apply \ac{GRPO} on multi-turn, terminal-based coding agents with an asynchronous inference engine setup.
They used 64 H100s for training, 8 H100s for inference, and trained for 6 days on R2EGym, totaling ~$10368$ H100-hours.
In contrast, we train for ~2 days on 6 A100s (~$288$ A100-hours).
Normalizing by throughput, we conservatively estimate that $1\text{ H100-hour} \approx 2.2\text{ A100-hours}$; thus, we use ~$80\times$ less compute.
They used test-based rewards.
In contrast, we adopt an execution-free, patch-similarity reward, building on the patch-based signal introduced by SWE-RL~\cite{wei2025swerladvancingllmreasoning}.
This choice reduces engineering friction—avoiding per-repository test harnesses and heavyweight containerization—and enables concurrent training across heterogeneous environments and programming languages.
Our approach is also significantly more compute-efficient, as noted above; detailed design considerations are deferred to the methods chapter.

This chapter has established the theoretical and empirical foundations for online \ac{RL} applied to \ac{APR}.
By examining the evolution from rule-based repair systems to modern \ac{LLM}-based approaches, we identified fundamental limitations in current paradigms: the mismatch between static training and the dynamic nature of debugging.

Our review of reinforcement learning techniques, particularly the elegant simplification offered by \ac{GRPO}, demonstrates how computational efficiency can be achieved without sacrificing theoretical guarantees.
The examination of existing coding agents reveals a critical gap—all current systems operate at inference time only, missing the opportunity for models to learn optimal tool usage through experience.

The Nano agent architecture presents an alternative approach to agent design.
By providing only essential tools and relying on learning, we demonstrate that effective debugging capabilities can emerge from simple interfaces.

This thesis makes three fundamental contributions to the field:

\textbf{One of the first open-source online \ac{RL} implementations for \acp{LLM}}: We provide the research community with complete infrastructure for training language models through interactive environmental experience.
This democratizes access to techniques we believe are already employed by leading industry labs but have remained proprietary.
Our implementation enables researchers to explore online learning across diverse domains beyond code repair.

\textbf{Minimalist agent approach}: We demonstrate that a simple agent design with basic tools can achieve strong performance when combined with reinforcement learning, showing that sophisticated debugging behaviors can emerge from minimal interfaces.

\textbf{Bridge between research and practice}: By demonstrating that models can learn effective debugging strategies through \ac{RL} in realistic environments, we bridge the gap between academic research on program repair and practical tools used by developers.
Our approach shows that the future of coding assistance lies not in ever-more-sophisticated prompt engineering, but in models that learn from experience.

\subsection{Limitations of existing work}
\label{sec:limitations-existing}
\textbf{Focus on generation rather than repair}: Existing approaches predominantly emphasize code generation, yet creating new code differs fundamentally from the process of debugging and repairing existing systems.
This distinction is crucial, as effective automated program repair requires models to reason about and modify complex, pre-existing codebases rather than synthesizing code from scratch.

\textbf{Isolated task formulation}: Many prior works formulate tasks at the level of single functions, thereby neglecting the intricate interdependencies and architectural complexity inherent in real-world software systems.
This simplification limits the ability of models to generalize to practical debugging scenarios, where understanding interactions across multiple files and modules is essential.

\textbf{Lack of environmental interaction}: Previous methods typically do not incorporate exploration, tool usage, or iterative refinement into their training regimes.
As a result, models are deprived of opportunities to learn through active engagement with their environment, which is a key aspect of effective debugging and repair in realistic settings.

\textbf{Dependence on test-based rewards}: While relying on test-based rewards can, in principle, provide robust supervision, this strategy becomes increasingly impractical as the number of supported programming languages expands.
Constructing and maintaining comprehensive test suites for each language introduces significant operational challenges, thereby limiting the scalability and applicability of these approaches.

As we proceed to describe our method in detail, keep in mind that our goal is not merely to improve performance metrics on benchmarks, but to demonstrate a fundamentally different, nascent training approach that enables \acp{LLM} to learn not from labeled data points, but from experience.
