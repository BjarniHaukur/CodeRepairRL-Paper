\newpage
\thispagestyle{plain}
~\\
\vfill
{ \setstretch{1.1}
\subsection*{Authors}
Bjarni Haukur Bjarnason <bhbj@kth.se>\\ Electrical Engineering and Computer Science\\ KTH Royal Institute of Technology

\subsection*{Place for Project} Stockholm, Sweden\\ KTH

\subsection*{Examiner} Martin Monperrus <monperrus@kth.se>\\ Division of Theoretical Computer Science\\ KTH Royal Institute of Technology

\subsection*{Supervisor} André Afonso Nunes Silva <andreans@kth.se>\\ Division of Theoretical Computer Science\\ KTH Royal Institute of Technology ~ }

\newpage \thispagestyle{plain} \input{sections/0.1-acronyms}

\newpage \thispagestyle{plain} \chapter*{Abstract} Software engineering agents have shown strong real-world debugging capabilities, yet a core mismatch persists between multi-step, interactive deployment and training that uses static datasets of isolated code changes.
This thesis presents a fully open, end-to-end system for online, execution-free \ac{RL} that trains \acp{LLM} inside a coding agent scaffold (Nano).
The system uses live weight synchronization via \acs{NCCL} to push policy updates to running inference servers during training.
While industry systems exhibit agentic coding capabilities consistent with agent-based training, their methods remain undisclosed.
This work provides an open, reproducible recipe.

We train using \ac{GSPO} with a light \ac{KL} regularizer on a 1{,}000-task curriculum spanning ten programming languages, completing in 144 \ac{GPU}-hours on 3 A100s.
On SWE-Bench-Verified, patch submission rates (non-empty patches) rise from 37\% to 78\% and mean patch-similarity rewards increase by 54\%, while test-verified success remains approximately flat at 6--7\%.
These results establish that online, execution-free \ac{RL} reliably improves agent operational competence within academic compute budgets.
Translating these gains to functional correctness likely requires longer training or alternative reward design.
We release all infrastructure, methodology, and evaluation protocols to enable reproducible study of online \ac{RL} for interactive coding agents.

\subsection*{Keywords}
automated program repair, coding agents, reinforcement learning, group relative policy optimization, group sequence policy optimization, online learning, tool-augmented language models, execution-free training, multilingual generalization

\newpage
\thispagestyle{plain}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%	 The Swedish abstract         %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter*{Sammanfattning}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Kodagenter har visat starka verkliga felsökningsförmågor, men en grundläggande diskrepans kvarstår mellan flerstegs, interaktiv implementation och träning som använder statiska dataset av isolerade kodändringar.
Denna avhandling presenterar ett helt öppet, end-to-end-system för online, exekveringsfri \ac{RL} som tränar \acp{LLM} inuti en kodagentskall (Nano).
Systemet använder live viktsynkronisering via \acs{NCCL} för att skicka policyuppdateringar till körande inferensservrar under träning.
Medan industrisystem uppvisar agentiska kodningsförmågor som överensstämmer med agentbaserad träning, förblir deras metoder hemliga.
Detta arbete tillhandahåller ett öppet, reproducerbart recept.

Vi tränar med \ac{GSPO} med en lätt \ac{KL}-regularisator på ett 1{,}000-uppgifters läroplan som spänner över tio programmeringsspråk, och slutför på 144 \ac{GPU}-timmar på 3 A100s.
På SWE-Bench-Verified ökar patch-inlämningsfrekvenser (icke-tomma patchar) från 37\% till 78\% och genomsnittliga patch-likhetsbelöningar ökar med 54\%, medan testverifierad framgång förblir ungefär platt vid 6--7\%.
Dessa resultat fastställer att online, exekveringsfri \ac{RL} pålitligt förbättrar agentoperativ kompetens inom akademiska beräkningsbudgetar.
Att översätta dessa vinster till funktionell korrekthet kräver sannolikt längre träning eller alternativ belöningsdesign.
Vi släpper all infrastruktur, metodik och utvärderingsprotokoll för att möjliggöra reproducerbar studie av online \ac{RL} för interaktiva kodagenter.

\subsection*{Nyckelord}
automatisk programreparation, kodagenter, förstärkningsinlärning, grupprelativ policyoptimering, gruppsekvens-policyoptimering, online-inlärning, verktygsförstärkta språkmodeller, exekveringsfri träning, flerspråkig generalisering

\newpage
\thispagestyle{plain}
\chapter*{Acknowledgements}
I would like to express my sincere gratitude to my supervisor, André Afonso Nunes Silva, and my examiner, Martin Monperrus, for their exceptional guidance throughout this thesis.
Their supervision extended beyond technical feedback: they trusted me to pursue an ambitious research direction and supported a complete methodological pivot midway through the project when evidence suggested a more promising path.

This research would not have been possible without access to the computational infrastructure provided by the National Academic Infrastructure for Supercomputing in Sweden (\acs{NAISS}).
Training on the Alvis and Berzelius \ac{GPU} clusters enabled experimentation at a scale far beyond typical academic resources, making the investigation of online \ac{RL} for coding agents feasible.
I am grateful to the infrastructure providers and the Swedish research community for maintaining these shared resources.

I thank the members of ASSERT-KTH who regularly attended the Monday meetings, which operated as an informal book club for those interested in \ac{AI} for software engineering.
These sessions provided a welcoming forum to discuss recent papers, share emerging ideas, and engage with a group of like-minded researchers.

I acknowledge the developers and maintainers of the open-source projects used in this research: TRL and the broader HuggingFace ecosystem, vLLM, DeepSpeed, PyTorch and more.

Lastly, I thank my friends at KTH, Jonas and Navin, who have been with me from the start of my master's studies.
Their steady friendship and camaraderie are the foundation on which these studies stand.

\newpage

\etocdepthtag.toc{mtchapter}
\etocsettagdepth{mtchapter}{subsection}
\etocsettagdepth{mtappendix}{none}
\thispagestyle{plain}
\tableofcontents

\newpage
