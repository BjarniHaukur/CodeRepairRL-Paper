\chapter{Conclusions and Future Work}
\label{ch:conclusions}

This thesis investigates online \ac{RL} training for repository-level code repair through a single minimalist agent, demonstrating that execution-free patch-similarity rewards enable meaningful performance improvements within academic compute constraints.
We developed infrastructure for live weight synchronization between distributed trainers and deployed inference servers, enabling efficient multi-turn agent training on modest \ac{GPU} clusters.
\todoinline{Update}
Training Qwen3-14B with \ac{GSPO} on execution-free rewards doubled baseline performance on SWE-Bench-Verified from 6\% to ~10\%, validating that interactive experiential learning improves coding agent capabilities without language-specific test infrastructure.

\section{Summary of Contributions}
\label{sec:summary-contributions}

Our work makes several methodological and infrastructure contributions to online \ac{RL} training for coding agents:

\subsection{Execution-Free Training Methodology}

\textbf{Language-Agnostic Rewards}: We demonstrate that execution-free patch-similarity rewards enable effective \ac{RL} training across programming languages without language-specific execution infrastructure.
Training on a 1{,}000-task curriculum mixing Python and nine additional languages validates that static diff comparison provides sufficient learning signal for repository-level code repair.

\textbf{Nano Agent Design}: The minimalist Nano agent operates with only two tools—restricted bash and search-and-replace patching—demonstrating that effective debugging behaviors can develop through \ac{RL} training without engineered complexity.
By sidestepping diff generation complexity through deterministic git-based canonical diff computation, the design eliminates an entire class of formatting errors orthogonal to the semantic debugging task.

\subsection{Technical Infrastructure}

\textbf{Live Weight Synchronization via \ac{NCCL}}: We extend vLLM's async engine to accept live parameter updates from distributed trainers through persistent \ac{NCCL} channels, enabling continuous policy improvement without server restarts.
The implementation handles layer-wise parameter gathering, \ac{LoRA} adapter merging before transmission, and complete KV-cache invalidation after updates, maintaining consistency between weights and cached activations.
This eliminates the conventional training-then-deploy cycle, enabling true online learning where policy updates immediately propagate to deployed inference infrastructure.

\textbf{Asynchronous Multi-Turn Architecture}: By leveraging vLLM's OpenAI-compatible \ac{API}, we enable individual agents to progress through multi-turn interactions asynchronously while maintaining synchronous batches at the episode level for group-relative optimization.
This decouples \ac{AI} application logic from trainer abstractions—the agent runs identically during training and deployment, requiring no agent-specific code in the training loop.
The architecture substantially improves throughput over synchronous per-turn execution while satisfying \ac{GSPO} batch requirements.

\textbf{Academic Feasibility Optimizations}: Through composed optimizations—\ac{LoRA} parameter-efficient adaptation, DeepSpeed ZeRO-2 for optimizer state sharding, gradient checkpointing for activation memory, and custom Triton kernels for \ac{GSPO} loss computation—we demonstrate 14B parameter training on three A100 \acp{GPU} (two training, one inference).
These techniques compose synergistically to address distinct memory and computation bottlenecks, making online \ac{RL} for multi-turn agents tractable within typical academic allocations.

\textbf{Open-Source Release}: Complete training infrastructure, agent implementations, SLURM orchestration scripts, and evaluation protocols are released open-source, enabling reproducible investigation of online \ac{RL} techniques and reducing barriers to academic research in experiential learning for coding agents.

\section{Research Question Answers}
\label{sec:research-answers}

Our experiments provide evidence addressing the three research questions established in Chapter~\ref{ch:introduction}:

\subsection{RQ1: Nano Harness Adaptation} \label{subsec:rq1-answer}

\textbf{Research Question}: How does \ac{GSPO} training improve Nano harness adaptation?

\textbf{Findings}: Training improves tool-call success rates, reduces invalid actions, and shifts command usage patterns toward more effective debugging strategies.
The model learns to adhere to the Nano interface constraints while developing systematic exploration behaviors—examining project structure, searching strategically, and gathering context before attempting fixes.
\todoinline{Insert specific quantitative improvements from Chapter 5: tool success rate deltas, invalid call reduction percentages, actions-per-episode evolution.}

\textbf{Implications}: Policy gradient optimization successfully reinforces effective tool-usage patterns within the single-agent paradigm, demonstrating co-adaptation between model and harness through environmental interaction.

\subsection{RQ2: Execution-Free Patch-Similarity \ac{RL}
Performance} \label{subsec:rq2-answer}

\textbf{Research Question}: Does execution-free patch-similarity \ac{RL} training improve SWE-Bench-Verified performance?

\todoinline{Update}
\textbf{Findings}: \ac{GSPO} training on execution-free patch-similarity rewards doubled test-verified success rates on SWE-Bench-Verified, improving from 6\% baseline to ~10\% after training.
This demonstrates that static diff-based rewards provide sufficient learning signal for functional bug-fixing improvements despite not directly optimizing for test passage.
\todoinline{Insert bootstrap confidence intervals and statistical significance tests from Chapter 5.}

\textbf{Implications}: Execution-free rewards serve as effective proxies for functional correctness, validating the core methodological approach while substantially reducing infrastructure complexity compared to execution-based training.

\subsection{RQ3: Language-Agnostic Training Effectiveness}
\label{subsec:rq3-answer}

\textbf{Research Question}: Does execution-free \ac{RL} enable effective multilingual training without language-specific engineering?

\todoinline{fill in numbers}
\textbf{Findings}: Training on the 1{,}000-task mixed curriculum (750 Python + 250 multilingual) improved performance on the 50-task SWE-Bench-Multilingual holdout spanning nine programming languages.
\todoinline{Insert per-language reward improvements with bootstrap confidence intervals from Chapter 5, highlighting languages with strongest/weakest gains and discussing language family effects.}

\textbf{Implications}: Execution-free patch-similarity rewards enable unified training across diverse programming languages without language-specific adaptations.
This validates the language-agnostic design principle: by decoupling learning signals from execution infrastructure, a single methodology trains effectively across Python, Rust, Java, PHP, Ruby, JavaScript, TypeScript, Go, C, and C++ without specialized engineering for each ecosystem.

\section{Broader Implications}
\label{sec:broader-implications}

\subsection{Open Science and Democratization}
\label{subsec:open-science}

By providing complete open-source implementations, we enable broader academic investigation of online \ac{RL} techniques for coding agents.
This open approach facilitates reproducible research and community-driven development of advanced agent training methods, reducing barriers to entry in this emerging field.

\subsection{From Games to Real Tasks: \acp{LLM} as the Missing Foundation}
\label{subsec:design-philosophy}

The application of \ac{RL} to real-world tasks like coding represents a fundamental shift in what reinforcement learning can accomplish.
AlphaZero's superhuman game performance demonstrated the power of learning through interaction, yet simultaneously exposed a critical limitation: games provide reward signals for every legal move, while real-world tasks require deep conceptual understanding just to participate meaningfully.
Before \acp{LLM}, applying \ac{RL} to tasks like code repair was futile not because the algorithms were inadequate, but because models lacked the prerequisite knowledge to attempt such problems coherently.
An \ac{RL} agent cannot learn from experience if it cannot generate experiences worth learning from—there is no gradient to climb when every action produces meaningless output.

\acp{LLM} solved this foundational problem by providing the conceptual substrate necessary for coherent interaction with complex environments.
When a modern \ac{RL} agent attempts to fix a bug, it can execute valid commands, parse error messages, and propose syntactically reasonable patches.
These capabilities—acquired through massive-scale pretraining—give the agent that crucial first rung on the ladder, enabling the generation of rich learning signals that \ac{RL} algorithms can exploit.
For the first time, we can apply decades of reinforcement learning research to problems that matter, because \acp{LLM} have bridged the gap between random exploration and meaningful interaction.

\section{Limitations and Constraints} \label{sec:limitations}

\subsection{Performance Levels and Task Scope}
\todoinline{fill in}
While \ac{GSPO} training doubled baseline performance from 6\% to ~10\%, the absolute success rate on SWE-Bench-Verified remains modest compared to frontier systems exceeding 70\%.
This gap reflects multiple factors: the minimalist Nano toolset provides less sophisticated capabilities than multi-scaffold systems, the 14B parameter model size is smaller than frontier models, and training compute budgets (three A100 \acp{GPU}) are substantially constrained relative to industrial deployments.

The evaluation focuses on single-commit bug fixes rather than complex architectural changes or multi-file refactoring tasks.
Repository-level changes spanning dozens of files with intricate dependency relationships present substantially harder challenges that may require different agent designs or training methodologies.

\subsection{Task Distribution and Language Coverage}

Training data combines 750 Python tasks with 250 multilingual tasks spanning nine additional languages.
This distribution reflects dataset availability: instruction-driven, repository-level multilingual debugging datasets remain substantially scarcer than Python-only resources.
The 50-task multilingual holdout provides preliminary evidence for cross-language generalization but represents limited coverage of each language's unique characteristics and common bug patterns.

\subsection{Computational Resource Requirements}

Online \ac{RL} training requires substantially more computation than supervised fine-tuning: each training example involves multi-turn agent interaction rather than single forward-backward passes.
While our optimizations make training tractable on three A100 \acp{GPU}, the complete system—distributed training, live inference serving, weight synchronization infrastructure, and episode orchestration—presents implementation complexity that may challenge researchers without prior distributed training experience.
The open-source release includes comprehensive documentation and SLURM orchestration scripts to mitigate these barriers.

\subsection{Execution Environment Isolation}

Episodes execute in ephemeral repository checkouts with restricted bash (rbash) rather than full containerization.
This approach provides sufficient isolation for safe training and reproducible execution while remaining simpler to deploy on HPC clusters where container runtimes may require special permissions.
However, container-based isolation would enable higher parallelism by safely running more concurrent episodes per node, substantially increasing throughput at the cost of additional infrastructure complexity.

\subsection{Reward Function Trade-offs}

Execution-free patch-similarity rewards trade functional verification for infrastructure simplicity and language-agnostic applicability.
Static diff comparison may occasionally reward syntactically plausible but functionally incorrect patches, or penalize alternative valid solutions that achieve the same functional outcome through different code changes.
Conversely, test-based rewards would require maintaining language-specific execution environments, test runners, build systems, and dependency management for each programming language, substantially increasing engineering complexity.
The trade-off favors execution-free rewards for academic research contexts prioritizing broad language coverage and rapid experimental iteration.

\subsection{Model Architecture and Scale}

This work focuses on the Qwen3 model family (primarily 14B parameters) due to strong tool-calling capabilities and computational accessibility within academic budgets.
The methodology's effectiveness across models with different architectural characteristics—varying attention mechanisms, tokenization schemes, or pre-training objectives—requires validation beyond the limited multi-size experiments documented in Appendix~\ref{app:model-comparison}.
Whether the observed improvements generalize to other model families or depend on Qwen3-specific capabilities remains an open empirical question.

\section{Future Research Directions}
\label{sec:future-work}

\subsection{Cross-Group Walltime Optimization}
\label{subsec:cross-group-optimization}

Our current implementation optimizes intra-group asynchronous behavior, allowing individual episodes within a training group to progress independently.
However, the system treats episode groups synchronously: inference pauses once the configured group size is reached, waiting for training to complete and weights to synchronize before new episodes begin.
Given the substantial variation in episode completion times—some problems resolve in five turns while others require twenty or more—this synchronous grouping creates walltime dependencies where inference servers sit idle during training updates.

Future implementations could eliminate this bottleneck through slightly off-policy episodes.
Instead of pausing inference during training, new episodes would begin immediately using the current policy while training proceeds on completed episodes in parallel.
When weight updates broadcast to inference servers, any in-flight episodes would continue with the updated policy, making them slightly off-policy: they began with policy version $\pi_n$ but will complete under $\pi_{n+1}$ or later.
Given the small, gradual nature of \ac{GSPO} updates and continuous weight synchronization, this off-policy gap remains bounded and acceptable.

This approach would maintain continuous inference server utilization, eliminate idle time during training phases, and enable training to begin as soon as any subset of episodes completes rather than waiting for full groups.
The optimization would particularly benefit throughput when episode length distributions exhibit high variance, as faster episodes immediately feed into training while longer episodes continue without blocking progress.

\subsection{Unified Training-Inference Architecture}

The current system separates training and inference across distinct \ac{GPU} pools with explicit weight synchronization.
While this design maximizes throughput by overlapping computation, it increases infrastructure requirements: even modest experiments demand three \acp{GPU} (two for training, one for inference).
Future implementations could co-locate training and inference on a single set of weights, eliminating synchronization overhead entirely and reducing the entry barrier for \ac{RL} experimentation.

In this unified architecture, the model would alternate between inference and training phases rather than running them concurrently.
Episodes would accumulate using the current policy weights, and once sufficient experience is collected, the system would pause episode generation, perform training updates in-place, then resume inference with the updated policy.
This eliminates the need for separate inference servers, weight broadcasting infrastructure, and \ac{NCCL} synchronization—reducing the minimum viable system to a single training node.

The trade-off is straightforward: training updates pause inference, reducing overall throughput compared to the parallel design.
However, for researchers with limited compute budgets or those conducting initial feasibility studies, this reduced throughput may be acceptable in exchange for dramatically lower infrastructure complexity and resource requirements.
The design would particularly benefit early-stage experimentation where validating core concepts matters more than maximizing sample efficiency.

\subsection{Multi-Task Curriculum Training}

The OpenAI-compatible \ac{API} abstraction and plug-and-play harness design naturally extend to multi-task training scenarios.
Rather than optimizing a single objective (code repair), the system could train concurrently on diverse tasks—code repair, email classification, document summarization, or any other problem admitting a structured reward signal.
Episodes from different task distributions would interleave within training groups, and the shared policy would learn to handle heterogeneous environments through the same tool-calling interface.

This multi-task formulation raises fundamental questions about capability emergence and transfer in \ac{RL} post-training.
Does sharing model capacity across diverse objectives improve generalization, allowing debugging skills to transfer to related reasoning tasks?
Or does multi-task training introduce interference, degrading performance on individual objectives compared to specialized single-task policies?
The answers would illuminate whether \ac{RL}-trained \acp{LLM} benefit from curriculum diversity analogous to pre-training's broad data mixtures, or whether task-specific specialization remains necessary for complex reasoning domains.

Investigating these questions could reveal fundamental insights about the future of \ac{RL} post-training: whether general-purpose reasoning emerges from diverse experiential curricula, or whether different cognitive skills require isolated optimization.
The infrastructure documented in this thesis provides the foundation for such investigations, requiring only task-specific harnesses and reward functions to explore multi-objective training systematically.

\section{Lessons Learned from Implementation}
\label{sec:lessons-learned}

The process of implementing and deploying online \ac{RL} for coding agents yielded insights that extend beyond the specific technical contributions documented in this thesis.
These reflections illuminate both the opportunities and challenges inherent in experiential learning approaches for complex reasoning tasks.

\subsection{System Complexity and Engineering Effort}

Integrating large language models with online \ac{RL} training revealed engineering challenges that significantly exceeded initial estimates.

Many critical failure modes only emerged during extended training runs spanning multiple days, revealing the difficulty of predicting distributed system behavior through local testing.
Robust monitoring infrastructure and iterative refinement proved essential—ambitious initial designs consistently required substantial simplification before achieving stability.

Engineering effort for production-ready implementation substantially outweighed theoretical development.
Implementing weight-synchronized OpenAI-compatible \ac{API} servers required many weeks of trial and error to achieve stable operation, revealing that infrastructure reliability demands sustained investment beyond algorithmic innovation.

Integrating bleeding-edge libraries amplified this challenge.
The system relied on rapidly evolving dependencies—TRL for \ac{RL} training, vLLM for inference serving, Liger-Kernel for memory-efficient kernels, and Flash Attention for optimized attention—each offering significant performance benefits but introducing breaking changes with every update.
We forked and extensively modified these projects to resolve conflicts, implement missing features, and fix blocking bugs.
Many modifications were contributed back upstream, but each library update frequently broke the entire system, requiring substantial effort to restore compatibility.
This experience highlights an inherent tension in cutting-edge research: leveraging the latest capabilities requires accepting integration burden that can exceed core research implementation effort.

\subsection{Reward Misspecification}

Initial experiments with auxiliary rewards revealed subtle failure modes in multi-objective training.
We introduced a file-matching reward to provide intermediate learning signal, hypothesizing it would help the agent learn to identify relevant files before optimizing patch quality.
However, because the primary diff similarity reward remained sparse while the file-matching reward provided frequent feedback, the model optimized aggressively for the auxiliary objective to the detriment of actual patch similarity.

Evaluating successive training checkpoints on SWE-Bench-Verified exposed this misspecification clearly: downstream performance improved initially but noticeably worsened after a certain point in training, even as file-matching accuracy continued increasing.
The agent learned to modify the correct files consistently while producing increasingly poor patches—a textbook case of reward hacking where optimizing a proxy metric degraded the true objective.
This experience reinforced the value of sparse, well-aligned rewards over dense but misspecified shaping signals.

\subsection{Broader Insights for Experiential Learning}

Several insights transcend the specific domain of code repair and inform experiential learning research more broadly.
The tension between exploration and exploitation manifests differently in multi-turn structured environments than in traditional \ac{RL} domains.
Agents must balance exploring alternative debugging strategies against exploiting known successful patterns while simultaneously learning which environmental observations warrant deeper investigation versus cursory examination.
This hierarchical exploration challenge suggests that flat action spaces may inadequately represent the decision structure of complex reasoning tasks.

The importance of execution-free rewards extends beyond computational convenience to methodological flexibility.
By decoupling learning signals from language-specific test infrastructure, we enabled rapid experimentation across programming languages and failure modes without maintaining complex execution environments.
This design choice traded some precision in correctness evaluation for substantial gains in experimental velocity and scope.
Similar trade-offs may benefit other domains where approximate but tractable reward signals enable broader exploration than expensive ground-truth evaluation.

The minimalist agent design validates that effective capabilities can arise from basic tool primitives when combined with adequate training.
Rather than engineering complex tool suites that embed human debugging knowledge through careful interface design, we provided basic primitives and relied on learning to discover effective strategies.
This philosophy aligns with core \ac{RL} principles—capabilities develop through environmental interaction rather than pre-programmed behaviors—though it requires sufficient training time and sample efficiency to realize in practice.

\section{Long-Term Vision}
\label{sec:long-term-vision}

The long-term vision of this research direction involves developing increasingly capable software engineering assistance through experiential learning approaches.
Our methodological framework provides a foundation for investigating how agents can acquire debugging and programming skills through environmental interaction.

Future developments in this direction could enhance software development productivity by providing intelligent assistance for debugging, code review, and implementation tasks.
Such systems could particularly benefit educational contexts by providing interactive learning environments for programming skill development.

Enhanced automated debugging capabilities could accelerate research workflows by reducing time spent on implementation details, allowing researchers to focus more extensively on conceptual innovation and experimental design.

\section{Final Reflections}
\label{sec:final-reflections}

This work demonstrates that online \ac{RL} training for coding agents is practically feasible within academic compute constraints and yields measurable performance improvements.
The convergence of \acp{LLM} and \ac{RL} enables training approaches previously confined to simple domains with well-defined action spaces to extend to complex reasoning tasks like repository-level debugging.

\acp{LLM} fundamentally transformed what reinforcement learning can accomplish by providing the conceptual foundation necessary for coherent interaction with complex environments.
Where previous generations of models could not generate meaningful actions in domains like code repair—leaving \ac{RL} algorithms with no gradient to climb—modern \acp{LLM} enable agents to participate meaningfully from the start, creating the rich learning signals that policy gradient algorithms require.
Decades of reinforcement learning research can now apply to problems requiring multi-step reasoning and strategic planning, because \acp{LLM} provide that crucial first rung on the ladder of competence.

By demonstrating that debugging behaviors emerge from simple tools combined with \ac{RL} training, this thesis provides evidence that experiential learning complements supervised approaches for interactive agent development.
The infrastructure and methodologies developed here offer a foundation for future investigations into how agents acquire complex skills through environmental interaction, extending beyond coding to domains where \acp{LLM} provide sufficient baseline capabilities for meaningful participation.

Perhaps most importantly, this work demonstrates that sophisticated \ac{AI} research infrastructure can be developed within academic settings through careful engineering and open-source collaboration.
By providing accessible implementations of online \ac{RL} training systems, we enable broader investigation of experiential learning approaches that might otherwise remain confined to well-resourced industry laboratories.

The core principles validated here—execution-free rewards enabling language-agnostic training, live weight synchronization enabling efficient multi-turn agent training, and systematic optimization making online \ac{RL} tractable within academic budgets—suggest productive directions for future research.
Whether these approaches scale to match frontier capabilities or require fundamental architectural advances remains an open question, but the foundation established here demonstrates that open, reproducible research in experiential learning for coding agents is both viable and valuable.
The questions worth asking are now more clearly defined, and the tools to investigate them are available to the broader research community.
