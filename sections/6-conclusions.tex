\chapter{Conclusions and Future Work}
\label{ch:conclusions}

This thesis introduces a novel methodological approach to training coding agents through experiential learning, developing the "hill-climbing the coding agent gradient" paradigm.
We present comprehensive infrastructure for online \ac{RL} training of code repair agents, establishing theoretical foundations and practical implementation strategies for this emerging research direction.

\section{Summary of Contributions}
\label{sec:summary-contributions}

Our work makes several methodological and infrastructure contributions to the emerging field of experiential learning for coding agents:

\subsection{Methodological Innovation} \label{subsec:methodological-innovation}

\textbf{Training-Inference Duality}: Our implementation collapses the conventional boundary between training and inference phases through continuous serving and real-time weight updates.
This unified approach makes large-scale agent training practically feasible within academic research budgets by eliminating the need for separate training and deployment phases.

\textbf{\ac{GSPO}
Optimization for Agents}: We adapt \ac{GSPO}~\cite{gspo2025} with KL regularization for coding agents, demonstrating its suitability as an alternative to traditional actor-critic methods while reducing computational overhead through elimination of value network training and stabilizing small-batch updates.
\todoinline{Ensure GSPO reference is included in the bibliography.}

\subsection{Methodological Validation}
\label{subsec:methodological-validation}

\textbf{Nano Agent Architecture}: Our minimalist Nano agent implementation demonstrates that sophisticated debugging behaviors can emerge from simple interfaces when combined with \ac{RL}.
The agent operates with only basic terminal commands and file operations, proving the viability of experiential learning approaches.

\textbf{Hill-Climbing Framework}: We establish the conceptual framework of "hill-climbing the coding agent gradient," providing a systematic approach to iterative improvement of coding agents through environmental interaction.

\textbf{Model Size Portability}: \todoinline{Complete experimental validation pending: demonstrate that the GSPO recipe improves SWE-Bench-Verified success across Qwen3-8B/14B/30B while documenting the Llama3.1-8B reference run.}

\subsection{Technical Infrastructure}
\label{subsec:technical-infrastructure}

\textbf{Compute-Efficient Training Architecture}: Our implementation demonstrates that sophisticated online \ac{RL} for coding agents remains tractable within academic compute budgets through systematic optimization.
We implemented real-time weight synchronization to OpenAI-compatible \ac{API} servers using \ac{NCCL}, enabling continuous batching where episodes progress independently within turns through asynchronous request handling.
Through careful memory management and differential weight updates, the system enables practical training of models up to 30B parameters on modest \ac{GPU} clusters.

\textbf{Generalizable Design Choices}: By operating through standard OpenAI-compatible \ac{API} interfaces, the architecture naturally extends beyond coding agents to other interactive applications where outcomes can be rewarded.
The training pipeline remains application-agnostic, suggesting potential applicability to training task-specific assistants, dialogue agents, or document processors where conventional \ac{API} interfaces already exist.

\textbf{Open-Source Implementation}: Complete open-source release of training infrastructure, agent implementations, and evaluation protocols enables reproducible investigation of online \ac{RL} techniques and reduces barriers to academic research in this domain.

\section{Research Question Answers}
\label{sec:research-answers}

Our methodology development and preliminary investigations provide insights into the research questions, with comprehensive experimental validation ongoing:

\subsection{RQ1: Adaptation to the Harness} \label{subsec:rq1-answer}

\textbf{Theoretical Foundation}: Interactive \ac{RL} should improve adaptation to the Nano harness through reinforcement of effective tool-usage patterns and reduction of invalid actions.

\textbf{Evidence}: \todoinline{Experimental validation pending: measure tool success rate improvements, invalid call rate reduction, and actions per solved task across training iterations.}

\textbf{Mechanism}: Policy updates should reinforce effective tool-usage sequences and prompt adherence, yielding co-adaptation between model and harness.

\subsection{RQ2: Multilingual Holdout}
\label{subsec:rq2-answer}

\textbf{Theoretical Foundation}: Exposure to the multilingual curriculum should raise average reward on the 50-sample SWE-Bench-Multilingual holdout without overfitting to the training languages.

\textbf{Evidence}: \todoinline{Experimental validation pending: measure pre/post holdout rewards with bootstrap confidence intervals and highlight language-specific shifts.}

\textbf{Significance}: Positive holdout gains would indicate that execution-free rewards enable multilingual transfer without bespoke per-language infrastructure.

\subsection{RQ3: Scaffold Transfer}
\label{subsec:rq3-answer}

\textbf{Theoretical Foundation}: Behaviours learned in the minimalist Nano scaffold should persist when the policy interacts with richer tool suites.\todoinline{Capture zero-shot vs few-shot expectations for each harness.}

\textbf{Evidence}: \todoinline{Experimental validation pending: report zero-shot and few-shot performance on Mini-SWE-Agent-, Aider-, and OpenHands-style harnesses relative to Nano baselines.}

\textbf{Significance}: Demonstrating transfer would show that Nano-based training yields reusable interaction routines, reducing the need for scaffold-specific retraining.

\section{Broader Implications}
\label{sec:broader-implications}

\subsection{Paradigm Shift in \ac{AI}
Training} \label{subsec:paradigm-shift}

Our methodological framework suggests potential for a fundamental shift from static dataset-based training to experiential environment-based learning for complex reasoning tasks.
The viability of online \ac{RL} infrastructure indicates that many \ac{AI} capabilities may be better acquired through direct interaction rather than passive observation, particularly for domains requiring multi-step reasoning and strategic planning.

\subsection{Software Engineering Automation}
\label{subsec:sw-engineering-auto}

The potential for monotonic improvement through \ac{RL} training indicates substantial room for advancement in automated debugging capabilities.
\todoinline{Pending experimental validation: quantify actual performance improvements and trajectory analysis.}
The methodological foundation supports future development toward practical deployment of autonomous debugging systems.

\subsection{Open Science and Democratization}
\label{subsec:open-science}

By providing complete open-source implementations, we enable broader academic investigation of online \ac{RL} techniques for coding agents.
This open approach facilitates reproducible research and community-driven development of advanced agent training methods, reducing barriers to entry in this emerging field.

\subsection{Design Philosophy and the Bitter Lesson}
\label{subsec:design-philosophy}

\subsubsection{From Games to Real Tasks: \acp{LLM} as the Missing Foundation}

The application of \ac{RL} to real-world tasks like coding represents a fundamental shift in what reinforcement learning can accomplish.
AlphaZero's superhuman game performance demonstrated the power of learning through interaction, yet simultaneously exposed a critical limitation: games provide reward signals for every legal move, while real-world tasks require deep conceptual understanding just to participate meaningfully.
Before \acp{LLM}, applying \ac{RL} to tasks like code repair was futile not because the algorithms were inadequate, but because models lacked the prerequisite knowledge to attempt such problems coherently.
An \ac{RL} agent cannot learn from experience if it cannot generate experiences worth learning from—there is no gradient to climb when every action produces meaningless output.

\acp{LLM} solved this foundational problem by providing the conceptual substrate necessary for coherent interaction with complex environments.
When a modern \ac{RL} agent attempts to fix a bug, it can execute valid commands, parse error messages, and propose syntactically reasonable patches.
These capabilities—acquired through massive-scale pretraining—give the agent that crucial first rung on the ladder, enabling the generation of rich learning signals that \ac{RL} algorithms can exploit.
For the first time, we can apply decades of reinforcement learning research to problems that matter, because \acp{LLM} have bridged the gap between random exploration and meaningful interaction.

\subsubsection{Learning Versus Engineering: The Minimalist Approach}

Our experimental results inform broader design philosophy questions in agent development.
\todoinline{Based on Chapter 5 results: discuss how the bitter lesson applies to training software engineering agents, comparing simple tools with extensive learning versus sophisticated engineering approaches.
Analyze whether learned behaviors from minimal interfaces outperform hand-crafted sophisticated tooling, referencing specific performance comparisons from §5.
}

The minimalist Nano agent design—deliberately simple tools combined with extensive \ac{RL} training—represents a test of whether learning-based approaches can match or exceed carefully engineered systems.
The empirical evidence from our training dynamics and transfer experiments provides insight into the relative merits of learning versus engineering for coding agent capabilities.

\subsection{Continued Improvement Potential}
\label{subsec:improvement-potential}

\todoinline{Based on Chapter 5 training curves: analyze evidence for continued improvement potential through longer training periods and larger models.
Reference specific training dynamics from §5.1 showing whether performance plateaued or continued improving, discuss sample efficiency trends, and project potential gains from extended training budgets.
}

The observed training dynamics throughout our experiments inform expectations about future capability improvements.
Whether performance curves exhibit continued upward trends or approach plateaus carries implications for the scalability of online \ac{RL} approaches to more complex software engineering tasks.

\section{Limitations and Constraints} \label{sec:limitations}

\subsection{Experimental Validation Status}
\label{subsec:experimental-validation}

The primary limitation of this work is that comprehensive experimental validation remains ongoing.
While we have developed robust theoretical foundations and implementation infrastructure, the quantitative claims about performance improvements, statistical significance, and cross-domain generalization require completion of systematic experimental evaluation.

\subsection{Evaluation Scope Constraints}
\label{subsec:eval-scope}

The planned evaluation focuses on Python debugging tasks within containerized environments.
Comprehensive evaluation across diverse programming paradigms (functional, systems programming, web development) and languages remains for future work.
Additionally, the current scope emphasizes single-commit bug fixes rather than complex architectural changes or multi-file refactoring tasks.

\subsection{Computational Resource Requirements}
\label{subsec:compute-requirements}

Despite optimizations, online training remains computationally intensive compared to traditional supervised learning.
The distributed training infrastructure presents implementation complexity that may challenge smaller research groups, though our open-source release aims to mitigate these barriers through comprehensive documentation and modular design.

\subsection{Rollout Isolation}
\label{subsec:rollout-isolation}

Our isolation approach uses ephemeral repository checkouts, restricted bash environments, and cgroups rather than containerization.
Infrastructure constraints—specifically, unavailability of Docker on the training cluster—necessitated this simpler strategy.
While adequate for safe training and reproducible execution, this limitation constrains parallelism: proper containerization would enable safely running more concurrent episodes per node, substantially increasing throughput.

\subsection{Reward Function Simplifications}
\label{subsec:reward-simplifications}

Our reward formulation relies on patch similarity rather than functional correctness verification through test execution.
While computationally tractable, this approach may occasionally reward syntactically correct but functionally incorrect patches, or penalize alternative valid solutions.
The limitation reflects practical constraints rather than fundamental methodological issues.

\subsection{Model Architecture Dependencies}
\label{subsec:model-dependencies}

Initial development focuses on the Qwen model family due to its strong tool-calling capabilities and computational accessibility.
Validation across models with different architectural characteristics, training objectives, or tool-calling paradigms remains an important area for future investigation.
The approach's potential model-agnostic benefits require systematic validation across broader architectural diversity.

\subsection{Environment Complexity Boundaries}
\label{subsec:env-complexity}

Our containerized environments simulate realistic development scenarios but exclude certain complexities of production systems: external dependencies, network interactions, hardware-specific behaviors, and real-time performance constraints.
The gap between training environments and production deployment contexts may limit practical applicability.

\section{Future Research Directions}
\label{sec:future-work}

\subsection{Completing Experimental Validation}
\label{subsec:completing-validation}

The immediate priority involves completing the comprehensive experimental evaluation outlined in our research questions:

\subsubsection{Performance Quantification}

Systematic measurement of performance improvements across the training process, including adaptation metrics for the Nano harness, cross-model portability validation, and cross-dataset generalization assessment.
This evaluation will provide the empirical foundation needed to validate the theoretical contributions developed in this thesis.

\subsubsection{Statistical Validation}

Rigorous statistical analysis of results with appropriate confidence intervals and significance testing.
Comparison against supervised fine-tuning baselines to establish the relative merits of experiential learning approaches.

\subsection{Enhanced Reward Engineering}
\label{subsec:enhanced-reward}

\subsubsection{Test-Based Validation}

The most natural extension involves incorporating actual test execution into reward computation.
Instead of relying on patch similarity metrics, future systems could execute project test suites and reward agents based on functional correctness.
This approach would provide more accurate correctness signals, enable discovery of alternative valid solutions, support evaluation of partial fixes, and better align training objectives with real-world debugging goals.

\subsubsection{Multi-Objective Optimization}

Future reward functions could incorporate multiple objectives beyond correctness: code quality metrics for readability and maintainability, efficiency considerations for patch minimality and performance impact, robustness measures for edge case handling, and documentation quality for explanations and comments.

\subsection{Scaling and Generalization}
\label{subsec:scaling-generalization}

\subsubsection{Broader Domain Expansion}

Systematic evaluation across different programming contexts and application domains would validate the universality of learned debugging skills.
This includes specialized training for security vulnerability fixing and performance optimization; evaluation on large-scale enterprise codebases with complex dependencies; and adaptation to different development workflows and coding conventions.

\subsubsection{Model Architecture Exploration}

Investigation of experiential training across different model architectures could reveal optimal designs for agentic programming tasks, including architecture variants like mixture-of-experts models, systematic study of performance scaling effects, multimodal integration with visual debugging information, and external memory systems for long-term context.

\subsection{Infrastructure Optimization}
\label{subsec:infra-optimization}

\subsubsection{Cross-Group Walltime Optimization}

Our current implementation optimizes intra-group asynchronous behavior, allowing individual episodes within a training group to progress independently.
However, the system treats episode groups synchronously: inference pauses once the configured group size is reached, waiting for training to complete and weights to synchronize before new episodes begin.
Given the substantial variation in episode completion times—some problems resolve in five turns while others require twenty or more—this creates walltime dependencies where the inference servers sit idle during training updates.

Future implementations could eliminate these barriers through slightly off-policy rollouts.
Instead of pausing inference during training, new episodes would begin immediately using the current policy while training proceeds on completed episodes in parallel.
When weight updates broadcast to inference servers, any in-flight episodes would continue with the updated policy, making them slightly off-policy: they began with policy version $\pi_n$ but will complete under $\pi_{n+1}$ or later.
Given the small, gradual nature of \ac{GSPO} updates and continuous weight synchronization, this off-policy gap remains bounded and acceptable.
This approach would maintain continuous inference server utilization, eliminate idle time during training phases, and enable training to begin as soon as any subset of episodes completes rather than waiting for full groups.
The optimization would particularly benefit throughput when episode length distributions exhibit high variance, as faster episodes would immediately feed into training while longer episodes continue exploration without blocking progress.

\subsubsection{Enhanced Containerization}

As discussed in \S\ref{subsec:rollout-isolation}, proper containerization would enable higher parallelism by safely multiplexing more episodes per node.
Combined with off-policy rollouts described above, this would eliminate on of the primary throughput bottlenecks in the current system.

\subsection{Advanced Agent Architectures}
\label{subsec:advanced-agents}

\subsubsection{Hierarchical and Modular Agents}

Future agent designs could incorporate hierarchical planning with high-level strategy formation and detailed execution sub-agents, modular debugging skills that can be combined for complex repairs, adaptive scaffolding with dynamic tool selection, and collaborative multi-agent approaches for complex debugging tasks.

\subsubsection{Meta-Learning and Continual Learning}

Agents that rapidly adapt to new environments represent an important frontier, including few-shot adaptation to new programming environments, continual learning without catastrophic forgetting, systematic transfer learning approaches, and self-improvement through reflection on debugging experiences.

\subsection{Human-\ac{AI}
Collaboration} \label{subsec:human-ai-collab}

Rather than fully autonomous agents, future systems could focus on human-\ac{AI} partnerships through explanatory debugging that shares reasoning, incremental assistance adapting to developer preferences, learning from natural human feedback, and preference learning for individual coding styles.

Educational applications could revolutionize programming instruction through tutoring systems that teach debugging skills, adaptive curricula based on individual weaknesses, automated skill assessment and improvement, and code review training through interactive analysis.

\subsection{Infrastructure and Tooling Advances}
\label{subsec:infra-tooling}

Continued optimization could enable broader access through federated learning across institutions, efficient communication protocols for model updates, robust fault tolerance for distributed systems, and dynamic resource optimization for mixed workloads.

Standardized evaluation frameworks would accelerate progress through comprehensive benchmarks for Python debugging tasks, automated functional correctness assessment, reproducibility tools for research extension, and community platforms for collaborative development.

\section{Lessons Learned from Implementation}
\label{sec:lessons-learned}

The process of implementing and deploying online \ac{RL} for coding agents yielded insights that extend beyond the specific technical contributions documented in this thesis.
These reflections illuminate both the opportunities and challenges inherent in experiential learning approaches for complex reasoning tasks.

\subsection{System Complexity and Engineering Effort}

Integrating large language models with online \ac{RL} training revealed engineering challenges that significantly exceeded initial estimates.

Many critical failure modes only emerged during extended training runs spanning multiple days, revealing the difficulty of predicting distributed system behavior through local testing.
Robust monitoring infrastructure and iterative refinement proved essential—ambitious initial designs consistently required substantial simplification before achieving stability.

Engineering effort for production-ready implementation substantially outweighed theoretical development.
Implementing weight-synchronized OpenAI-compatible \ac{API} servers required many weeks of trial and error to achieve stable operation, revealing that infrastructure reliability demands sustained investment beyond algorithmic innovation.

Integrating bleeding-edge libraries amplified this challenge.
The system relied on rapidly evolving dependencies—TRL for \ac{RL} training, vLLM for inference serving, Liger-Kernel for memory-efficient kernels, and Flash Attention for optimized attention—each offering significant performance benefits but introducing breaking changes with every update.
We forked and extensively modified these projects to resolve conflicts, implement missing features, and fix blocking bugs.
Many modifications were contributed back upstream, but each library update frequently broke the entire system, requiring substantial effort to restore compatibility.
This experience highlights an inherent tension in cutting-edge research: leveraging the latest capabilities requires accepting integration burden that can exceed core research implementation effort.

\subsection{Reward Misspecification}

Initial experiments with auxiliary rewards revealed subtle failure modes in multi-objective training.
We introduced a file-matching reward to provide intermediate learning signal, hypothesizing it would help the agent learn to identify relevant files before optimizing patch quality.
However, because the primary diff similarity reward remained sparse while the file-matching reward provided frequent feedback, the model optimized aggressively for the auxiliary objective to the detriment of actual patch similarity.

Evaluating successive training checkpoints on SWE-Bench-Verified exposed this misspecification clearly: downstream performance improved initially but noticeably worsened after a certain point in training, even as file-matching accuracy continued increasing.
The agent learned to modify the correct files consistently while producing increasingly poor patches—a textbook case of reward hacking where optimizing a proxy metric degraded the true objective.
This experience reinforced the value of sparse, well-aligned rewards over dense but misspecified shaping signals.

\subsection{Broader Insights for Experiential Learning}

Several insights transcend the specific domain of code repair and inform experiential learning research more broadly.
The tension between exploration and exploitation manifests differently in multi-turn structured environments than in traditional \ac{RL} domains.
Agents must balance exploring alternative debugging strategies against exploiting known successful patterns while simultaneously learning which environmental observations warrant deeper investigation versus cursory examination.
This hierarchical exploration challenge suggests that flat action spaces may inadequately represent the decision structure of complex reasoning tasks.

The importance of execution-free rewards extends beyond computational convenience to methodological flexibility.
By decoupling learning signals from language-specific test infrastructure, we enabled rapid experimentation across programming languages and failure modes without maintaining complex execution environments.
This design choice traded some precision in correctness evaluation for substantial gains in experimental velocity and scope.
Similar trade-offs may benefit other domains where approximate but tractable reward signals enable broader exploration than expensive ground-truth evaluation.

The success of minimalist agent design suggests that sophisticated capabilities can emerge from simple interfaces when combined with adequate training.
Rather than engineering complex tool suites that embed human debugging knowledge through careful interface design, we provided basic primitives and relied on learning to discover effective strategies.
This philosophy aligns with core \ac{RL} principles of capability emergence through environmental interaction, though it requires sufficient training time and sample efficiency to realize in practice.

\section{Long-Term Vision}
\label{sec:long-term-vision}

The long-term vision of this research direction involves developing increasingly capable software engineering assistance through experiential learning approaches.
Our methodological framework provides a foundation for investigating how agents can acquire debugging and programming skills through environmental interaction.

Future developments in this direction could enhance software development productivity by providing intelligent assistance for debugging, code review, and implementation tasks.
Such systems could particularly benefit educational contexts by providing interactive learning environments for programming skill development.

Enhanced automated debugging capabilities could accelerate research workflows by reducing time spent on implementation details, allowing researchers to focus more extensively on conceptual innovation and experimental design.

\section{Final Reflections}
\label{sec:final-reflections}

This research demonstrates that the convergence of \acp{LLM} and \ac{RL} opens unprecedented opportunities for developing sophisticated \ac{AI} systems capable of complex reasoning and interaction.
\acp{LLM} have fundamentally transformed what reinforcement learning can accomplish by providing the conceptual foundation necessary for coherent interaction with complex environments.
Where previous generations of models could not generate meaningful actions in domains like code repair—leaving \ac{RL} algorithms with no gradient to climb—modern \acp{LLM} enable agents to participate meaningfully from the start, creating the rich learning signals that \ac{RL} algorithms require.

The success of experiential training in this work validates broader principles about learning through environmental interaction and suggests promising directions for advancing \ac{AI} capabilities across domains requiring multi-step reasoning and strategic planning.
By demonstrating that sophisticated debugging behaviors can emerge from simple tools combined with \ac{RL} training, we provide evidence that decades of reinforcement learning research can now be applied to problems that matter.
The infrastructure and methodologies developed here offer a foundation for future investigations into how agents acquire complex skills through interaction, extending beyond coding to any domain where \acp{LLM} provide that crucial first rung on the ladder of competence.

Perhaps most importantly, our work demonstrates that sophisticated \ac{AI} research infrastructure can be developed within academic settings through careful engineering and open-source collaboration.
By providing accessible implementations of online \ac{RL} training systems, we enable broader investigation of experiential learning approaches that were previously confined to well-resourced industry laboratories.

The journey from passive pattern matching to active environmental interaction represents a potential fundamental evolution in how we train \ac{AI} systems.
While significant challenges remain, the theoretical foundation and infrastructure developed in this work suggest that experiential learning paradigms may play an increasingly important role in developing \ac{AI} systems capable of genuine reasoning and problem-solving.

As we stand at the threshold of increasingly capable \ac{AI} systems, the principles explored in this work—learning through interaction, systematic agent training infrastructure, and open scientific collaboration—may guide the development of even more sophisticated \ac{AI} capabilities.
The future of \ac{AI} development may increasingly incorporate experiential learning approaches that enable agents to learn through environmental interaction, complementing traditional supervised learning paradigms.
