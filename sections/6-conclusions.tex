\chapter{Conclusions and Future Work}
\label{ch:conclusions}

This thesis introduces a novel methodological approach to training coding agents through experiential learning, developing the "hill-climbing the coding agent gradient" paradigm.
We present comprehensive infrastructure for online \ac{RL} training of code repair agents, establishing theoretical foundations and practical implementation strategies for this emerging research direction.

\section{Summary of Contributions}
\label{sec:summary-contributions}

Our work makes several methodological and infrastructure contributions to the emerging field of experiential learning for coding agents:

\subsection{Methodological Innovation} \label{subsec:methodological-innovation}

\textbf{Training-Inference Duality}: Our implementation collapses the conventional boundary between training and inference phases through continuous serving and real-time weight updates.
This unified approach makes large-scale agent training practically feasible within academic research budgets by eliminating the need for separate training and deployment phases.

\textbf{\ac{GSPO}
Optimization for Agents}: We adapt \ac{GSPO}~\cite{gspo2025} with KL regularization for coding agents, demonstrating its suitability as an alternative to traditional actor-critic methods while reducing computational overhead through elimination of value network training and stabilizing small-batch updates.

\subsection{Methodological Validation}
\label{subsec:methodological-validation}

\textbf{Nano Agent Architecture}: Our minimalist Nano agent implementation demonstrates that sophisticated debugging behaviors can emerge from simple interfaces when combined with \ac{RL}.
The agent operates with only basic terminal commands and file operations, proving the viability of experiential learning approaches.

\textbf{Hill-Climbing Framework}: We establish the conceptual framework of "hill-climbing the coding agent gradient," providing a systematic approach to iterative improvement of coding agents through environmental interaction.

\textbf{Model Size Portability}: \todoinline{Complete experimental validation pending: demonstrate that the GSPO recipe improves SWE-Bench-Verified success across Qwen3-8B/14B/30B while documenting the Llama3.1-8B reference run.}

\subsection{Technical Infrastructure}
\label{subsec:technical-infrastructure}

\textbf{Compute-Efficient Training Architecture}: Our implementation demonstrates that sophisticated online \ac{RL} for coding agents remains tractable within academic compute budgets through systematic optimization.
We implemented real-time weight synchronization to OpenAI-compatible \ac{API} servers using \ac{NCCL}, enabling continuous batching where episodes progress independently within turns through asynchronous request handling.
Through careful memory management and differential weight updates, the system enables practical training of models up to 30B parameters on modest \ac{GPU} clusters.

\textbf{Generalizable Design Choices}: By operating through standard OpenAI-compatible \ac{API} interfaces, the architecture naturally extends beyond coding agents to other interactive applications where outcomes can be rewarded.
The training pipeline remains application-agnostic, suggesting potential applicability to training task-specific assistants, dialogue agents, or document processors where conventional \ac{API} interfaces already exist.

\textbf{Open-Source Implementation}: Complete open-source release of training infrastructure, agent implementations, and evaluation protocols enables reproducible investigation of online \ac{RL} techniques and reduces barriers to academic research in this domain.

\section{Research Question Answers}
\label{sec:research-answers}

Our methodology development and preliminary investigations provide insights into the research questions, with comprehensive experimental validation ongoing:

\subsection{RQ1: Nano Harness Adaptation} \label{subsec:rq1-answer}

\textbf{Theoretical Foundation}: Interactive \ac{RL} should improve Nano harness adaptation through reinforcement of effective tool-usage patterns and reduction of invalid actions.

\textbf{Evidence}: \todoinline{Experimental validation pending: measure tool success rate improvements, invalid call rate reduction, and actions per solved task across training iterations.}

\textbf{Mechanism}: Policy updates should reinforce effective tool-usage sequences and prompt adherence, yielding co-adaptation between model and harness.

\subsection{RQ2: Execution-Free Patch-Similarity \ac{RL}
Performance} \label{subsec:rq2-answer}

\textbf{Theoretical Foundation}: Training on static patch-similarity rewards should improve functional bug-fixing success on SWE-Bench-Verified despite not directly optimizing for test passage.

\textbf{Evidence}: \todoinline{Experimental validation pending: measure test-verified success rate improvements comparing pre-training baseline vs. post-GSPO training with bootstrap confidence intervals.}

\textbf{Significance}: Positive test-verified improvements would validate that execution-free patch-similarity rewards serve as effective proxies for functional correctness, enabling tractable multilingual training.

\subsection{RQ3: Language-Agnostic Training Effectiveness}
\label{subsec:rq3-answer}

\textbf{Theoretical Foundation}: Execution-free patch-similarity rewards should enable effective multilingual training without language-specific engineering, yielding improvements across diverse programming languages on the 50-task SWE-Bench-Multilingual holdout.

\textbf{Evidence}: \todoinline{Experimental validation pending: measure per-language reward improvements on the 50-task multilingual holdout with bootstrap confidence intervals, highlighting language-specific patterns.}

\textbf{Significance}: Positive multilingual holdout gains would demonstrate that execution-free rewards enable unified training across programming languages without language-specific test infrastructure, engineering, or adaptations—validating the core design principle of language-agnostic RL training.

\section{Broader Implications}
\label{sec:broader-implications}

\subsection{Paradigm Shift in \ac{AI}
Training} \label{subsec:paradigm-shift}

Our methodological framework suggests potential for a fundamental shift from static dataset-based training to experiential environment-based learning for complex reasoning tasks.
The viability of online \ac{RL} infrastructure indicates that many \ac{AI} capabilities may be better acquired through direct interaction rather than passive observation, particularly for domains requiring multi-step reasoning and strategic planning.

\subsection{Software Engineering Automation}
\label{subsec:sw-engineering-auto}

The potential for monotonic improvement through \ac{RL} training indicates substantial room for advancement in automated debugging capabilities.
\todoinline{Pending experimental validation: quantify actual performance improvements and trajectory analysis.}
The methodological foundation supports future development toward practical deployment of autonomous debugging systems.

\subsection{Open Science and Democratization}
\label{subsec:open-science}

By providing complete open-source implementations, we enable broader academic investigation of online \ac{RL} techniques for coding agents.
This open approach facilitates reproducible research and community-driven development of advanced agent training methods, reducing barriers to entry in this emerging field.

\subsection{From Games to Real Tasks: \acp{LLM} as the Missing Foundation}
\label{subsec:design-philosophy}

The application of \ac{RL} to real-world tasks like coding represents a fundamental shift in what reinforcement learning can accomplish.
AlphaZero's superhuman game performance demonstrated the power of learning through interaction, yet simultaneously exposed a critical limitation: games provide reward signals for every legal move, while real-world tasks require deep conceptual understanding just to participate meaningfully.
Before \acp{LLM}, applying \ac{RL} to tasks like code repair was futile not because the algorithms were inadequate, but because models lacked the prerequisite knowledge to attempt such problems coherently.
An \ac{RL} agent cannot learn from experience if it cannot generate experiences worth learning from—there is no gradient to climb when every action produces meaningless output.

\acp{LLM} solved this foundational problem by providing the conceptual substrate necessary for coherent interaction with complex environments.
When a modern \ac{RL} agent attempts to fix a bug, it can execute valid commands, parse error messages, and propose syntactically reasonable patches.
These capabilities—acquired through massive-scale pretraining—give the agent that crucial first rung on the ladder, enabling the generation of rich learning signals that \ac{RL} algorithms can exploit.
For the first time, we can apply decades of reinforcement learning research to problems that matter, because \acp{LLM} have bridged the gap between random exploration and meaningful interaction.

\subsection{Learning Versus Engineering: The Minimalist Approach}

Our experimental results inform broader design philosophy questions in agent development.
\todoinline{Based on Chapter 5 results: discuss how the bitter lesson applies to training software engineering agents, comparing simple tools with extensive learning versus sophisticated engineering approaches.
Analyze whether learned behaviors from minimal interfaces outperform hand-crafted sophisticated tooling, referencing specific performance comparisons from §5.
}

The minimalist Nano agent design—deliberately simple tools combined with extensive \ac{RL} training—represents a test of whether learning-based approaches can match or exceed carefully engineered systems.
The empirical evidence from our training dynamics provides insight into the relative merits of learning versus engineering for coding agent capabilities.

\subsection{Continued Improvement Potential}
\label{subsec:improvement-potential}

\todoinline{Based on Chapter 5 training curves: analyze evidence for continued improvement potential through longer training periods and larger models.
Reference specific training dynamics from §5.1 showing whether performance plateaued or continued improving, discuss sample efficiency trends, and project potential gains from extended training budgets.
}

The observed training dynamics throughout our experiments inform expectations about future capability improvements.
Whether performance curves exhibit continued upward trends or approach plateaus carries implications for the scalability of online \ac{RL} approaches to more complex software engineering tasks.

\section{Limitations and Constraints} \label{sec:limitations}

\subsection{Experimental Validation Status}
\label{subsec:experimental-validation}

The primary limitation of this work is that comprehensive experimental validation remains ongoing.
While we have developed robust theoretical foundations and implementation infrastructure, the quantitative claims about performance improvements, statistical significance, and cross-domain generalization require completion of systematic experimental evaluation.

\subsection{Evaluation Scope Constraints}
\label{subsec:eval-scope}

The planned evaluation focuses on Python debugging tasks within containerized environments.
Comprehensive evaluation across diverse programming paradigms (functional, systems programming, web development) and languages remains for future work.
Additionally, the current scope emphasizes single-commit bug fixes rather than complex architectural changes or multi-file refactoring tasks.

\subsection{Computational Resource Requirements}
\label{subsec:compute-requirements}

Despite optimizations, online training remains computationally intensive compared to traditional supervised learning.
The distributed training infrastructure presents implementation complexity that may challenge smaller research groups, though our open-source release aims to mitigate these barriers through comprehensive documentation and modular design.

\subsection{Rollout Isolation}
\label{subsec:rollout-isolation}

Our isolation approach uses ephemeral repository checkouts, restricted bash environments, and cgroups rather than containerization.
Infrastructure constraints—specifically, unavailability of Docker on the training cluster—necessitated this simpler strategy.
While adequate for safe training and reproducible execution, this limitation constrains parallelism: proper containerization would enable safely running more concurrent episodes per node, substantially increasing throughput.

\subsection{Reward Function Simplifications}
\label{subsec:reward-simplifications}

Our reward formulation relies on patch similarity rather than functional correctness verification through test execution.
While computationally tractable, this approach may occasionally reward syntactically correct but functionally incorrect patches, or penalize alternative valid solutions.
The limitation reflects practical constraints rather than fundamental methodological issues.

\subsection{Model Architecture Dependencies}
\label{subsec:model-dependencies}

Initial development focuses on the Qwen model family due to its strong tool-calling capabilities and computational accessibility.
Validation across models with different architectural characteristics, training objectives, or tool-calling paradigms remains an important area for future investigation.
The approach's potential model-agnostic benefits require systematic validation across broader architectural diversity.

\subsection{Environment Complexity Boundaries}
\label{subsec:env-complexity}

Our containerized environments simulate realistic development scenarios but exclude certain complexities of production systems: external dependencies, network interactions, hardware-specific behaviors, and real-time performance constraints.
The gap between training environments and production deployment contexts may limit practical applicability.

\section{Future Research Directions}
\label{sec:future-work}

\subsection{Cross-Group Walltime Optimization}
\label{subsec:cross-group-optimization}

Our current implementation optimizes intra-group asynchronous behavior, allowing individual episodes within a training group to progress independently.
However, the system treats episode groups synchronously: inference pauses once the configured group size is reached, waiting for training to complete and weights to synchronize before new episodes begin.
Given the substantial variation in episode completion times—some problems resolve in five turns while others require twenty or more—this synchronous grouping creates walltime dependencies where inference servers sit idle during training updates.

Future implementations could eliminate this bottleneck through slightly off-policy rollouts.
Instead of pausing inference during training, new episodes would begin immediately using the current policy while training proceeds on completed episodes in parallel.
When weight updates broadcast to inference servers, any in-flight episodes would continue with the updated policy, making them slightly off-policy: they began with policy version $\pi_n$ but will complete under $\pi_{n+1}$ or later.
Given the small, gradual nature of \ac{GSPO} updates and continuous weight synchronization, this off-policy gap remains bounded and acceptable.

This approach would maintain continuous inference server utilization, eliminate idle time during training phases, and enable training to begin as soon as any subset of episodes completes rather than waiting for full groups.
The optimization would particularly benefit throughput when episode length distributions exhibit high variance, as faster episodes immediately feed into training while longer episodes continue without blocking progress.

\subsection{Unified Training-Inference Architecture}

The current system separates training and inference across distinct \ac{GPU} pools with explicit weight synchronization.
While this design maximizes throughput by overlapping computation, it increases infrastructure requirements: even modest experiments demand three \acp{GPU} (two for training, one for inference).
Future implementations could co-locate training and inference on a single set of weights, eliminating synchronization overhead entirely and reducing the entry barrier for \ac{RL} experimentation.

In this unified architecture, the model would alternate between inference and training phases rather than running them concurrently.
Episodes would accumulate using the current policy weights, and once sufficient experience is collected, the system would pause rollout generation, perform training updates in-place, then resume inference with the updated policy.
This eliminates the need for separate inference servers, weight broadcasting infrastructure, and \ac{NCCL} synchronization—reducing the minimum viable system to a single training node.

The trade-off is straightforward: training updates pause inference, reducing overall throughput compared to the parallel design.
However, for researchers with limited compute budgets or those conducting initial feasibility studies, this reduced throughput may be acceptable in exchange for dramatically lower infrastructure complexity and resource requirements.
The design would particularly benefit early-stage experimentation where validating core concepts matters more than maximizing sample efficiency.

\subsection{Multi-Task Curriculum Training}

The OpenAI-compatible \ac{API} abstraction and plug-and-play harness design naturally extend to multi-task training scenarios.
Rather than optimizing a single objective (code repair), the system could train concurrently on diverse tasks—code repair, email classification, document summarization, or any other problem admitting a structured reward signal.
Episodes from different task distributions would interleave within training groups, and the shared policy would learn to handle heterogeneous environments through the same tool-calling interface.

This multi-task formulation raises fundamental questions about capability emergence and transfer in \ac{RL} post-training.
Does sharing model capacity across diverse objectives improve generalization, allowing debugging skills to transfer to related reasoning tasks?
Or does multi-task training introduce interference, degrading performance on individual objectives compared to specialized single-task policies?
The answers would illuminate whether \ac{RL}-trained \acp{LLM} benefit from curriculum diversity analogous to pre-training's broad data mixtures, or whether task-specific specialization remains necessary for complex reasoning domains.

Investigating these questions could reveal fundamental insights about the future of \ac{RL} post-training: whether general-purpose reasoning emerges from diverse experiential curricula, or whether different cognitive skills require isolated optimization.
The infrastructure documented in this thesis provides the foundation for such investigations, requiring only task-specific harnesses and reward functions to explore multi-objective training systematically.

\section{Lessons Learned from Implementation}
\label{sec:lessons-learned}

The process of implementing and deploying online \ac{RL} for coding agents yielded insights that extend beyond the specific technical contributions documented in this thesis.
These reflections illuminate both the opportunities and challenges inherent in experiential learning approaches for complex reasoning tasks.

\subsection{System Complexity and Engineering Effort}

Integrating large language models with online \ac{RL} training revealed engineering challenges that significantly exceeded initial estimates.

Many critical failure modes only emerged during extended training runs spanning multiple days, revealing the difficulty of predicting distributed system behavior through local testing.
Robust monitoring infrastructure and iterative refinement proved essential—ambitious initial designs consistently required substantial simplification before achieving stability.

Engineering effort for production-ready implementation substantially outweighed theoretical development.
Implementing weight-synchronized OpenAI-compatible \ac{API} servers required many weeks of trial and error to achieve stable operation, revealing that infrastructure reliability demands sustained investment beyond algorithmic innovation.

Integrating bleeding-edge libraries amplified this challenge.
The system relied on rapidly evolving dependencies—TRL for \ac{RL} training, vLLM for inference serving, Liger-Kernel for memory-efficient kernels, and Flash Attention for optimized attention—each offering significant performance benefits but introducing breaking changes with every update.
We forked and extensively modified these projects to resolve conflicts, implement missing features, and fix blocking bugs.
Many modifications were contributed back upstream, but each library update frequently broke the entire system, requiring substantial effort to restore compatibility.
This experience highlights an inherent tension in cutting-edge research: leveraging the latest capabilities requires accepting integration burden that can exceed core research implementation effort.

\subsection{Reward Misspecification}

Initial experiments with auxiliary rewards revealed subtle failure modes in multi-objective training.
We introduced a file-matching reward to provide intermediate learning signal, hypothesizing it would help the agent learn to identify relevant files before optimizing patch quality.
However, because the primary diff similarity reward remained sparse while the file-matching reward provided frequent feedback, the model optimized aggressively for the auxiliary objective to the detriment of actual patch similarity.

Evaluating successive training checkpoints on SWE-Bench-Verified exposed this misspecification clearly: downstream performance improved initially but noticeably worsened after a certain point in training, even as file-matching accuracy continued increasing.
The agent learned to modify the correct files consistently while producing increasingly poor patches—a textbook case of reward hacking where optimizing a proxy metric degraded the true objective.
This experience reinforced the value of sparse, well-aligned rewards over dense but misspecified shaping signals.

\subsection{Broader Insights for Experiential Learning}

Several insights transcend the specific domain of code repair and inform experiential learning research more broadly.
The tension between exploration and exploitation manifests differently in multi-turn structured environments than in traditional \ac{RL} domains.
Agents must balance exploring alternative debugging strategies against exploiting known successful patterns while simultaneously learning which environmental observations warrant deeper investigation versus cursory examination.
This hierarchical exploration challenge suggests that flat action spaces may inadequately represent the decision structure of complex reasoning tasks.

The importance of execution-free rewards extends beyond computational convenience to methodological flexibility.
By decoupling learning signals from language-specific test infrastructure, we enabled rapid experimentation across programming languages and failure modes without maintaining complex execution environments.
This design choice traded some precision in correctness evaluation for substantial gains in experimental velocity and scope.
Similar trade-offs may benefit other domains where approximate but tractable reward signals enable broader exploration than expensive ground-truth evaluation.

The success of minimalist agent design suggests that sophisticated capabilities can emerge from simple interfaces when combined with adequate training.
Rather than engineering complex tool suites that embed human debugging knowledge through careful interface design, we provided basic primitives and relied on learning to discover effective strategies.
This philosophy aligns with core \ac{RL} principles of capability emergence through environmental interaction, though it requires sufficient training time and sample efficiency to realize in practice.

\section{Long-Term Vision}
\label{sec:long-term-vision}

The long-term vision of this research direction involves developing increasingly capable software engineering assistance through experiential learning approaches.
Our methodological framework provides a foundation for investigating how agents can acquire debugging and programming skills through environmental interaction.

Future developments in this direction could enhance software development productivity by providing intelligent assistance for debugging, code review, and implementation tasks.
Such systems could particularly benefit educational contexts by providing interactive learning environments for programming skill development.

Enhanced automated debugging capabilities could accelerate research workflows by reducing time spent on implementation details, allowing researchers to focus more extensively on conceptual innovation and experimental design.

\section{Final Reflections}
\label{sec:final-reflections}

This research demonstrates that the convergence of \acp{LLM} and \ac{RL} opens unprecedented opportunities for developing sophisticated \ac{AI} systems capable of complex reasoning and interaction.
\acp{LLM} have fundamentally transformed what reinforcement learning can accomplish by providing the conceptual foundation necessary for coherent interaction with complex environments.
Where previous generations of models could not generate meaningful actions in domains like code repair—leaving \ac{RL} algorithms with no gradient to climb—modern \acp{LLM} enable agents to participate meaningfully from the start, creating the rich learning signals that \ac{RL} algorithms require.

The success of experiential training in this work validates broader principles about learning through environmental interaction and suggests promising directions for advancing \ac{AI} capabilities across domains requiring multi-step reasoning and strategic planning.
By demonstrating that sophisticated debugging behaviors can emerge from simple tools combined with \ac{RL} training, we provide evidence that decades of reinforcement learning research can now be applied to problems that matter.
The infrastructure and methodologies developed here offer a foundation for future investigations into how agents acquire complex skills through interaction, extending beyond coding to any domain where \acp{LLM} provide that crucial first rung on the ladder of competence.

Perhaps most importantly, our work demonstrates that sophisticated \ac{AI} research infrastructure can be developed within academic settings through careful engineering and open-source collaboration.
By providing accessible implementations of online \ac{RL} training systems, we enable broader investigation of experiential learning approaches that were previously confined to well-resourced industry laboratories.

The journey from passive pattern matching to active environmental interaction represents a potential fundamental evolution in how we train \ac{AI} systems.
While significant challenges remain, the theoretical foundation and infrastructure developed in this work suggest that experiential learning paradigms may play an increasingly important role in developing \ac{AI} systems capable of genuine reasoning and problem-solving.

As we stand at the threshold of increasingly capable \ac{AI} systems, the principles explored in this work—learning through interaction, systematic agent training infrastructure, and open scientific collaboration—may guide the development of even more sophisticated \ac{AI} capabilities.
The future of \ac{AI} development may increasingly incorporate experiential learning approaches that enable agents to learn through environmental interaction, complementing traditional supervised learning paradigms.
