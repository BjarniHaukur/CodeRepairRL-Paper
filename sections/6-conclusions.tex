\chapter{Conclusions and Future Work}
\label{ch:conclusions}

This thesis investigates online \ac{RL} training for repository-level code repair through a single minimalist agent, demonstrating that execution-free patch-similarity rewards enable meaningful improvements in scaffold adaptation within academic compute constraints.
We developed infrastructure for live weight synchronization between distributed trainers and deployed inference servers, enabling efficient multi-turn agent training on modest \ac{GPU} clusters.
Training Qwen3-14B with \ac{GSPO} on execution-free rewards substantially improves operational reliability, more than doubling patch completion rates from 37\% to 78\%, though test-verified correctness remains flat at approximately 6--7\%.

\section{Summary of Contributions}
\label{sec:summary-contributions}

Our work makes several methodological and infrastructure contributions to online \ac{RL} training for coding agents:

\subsection{Execution-Free Training Methodology}

\textbf{Language-Agnostic Rewards}: We demonstrate that execution-free patch-similarity rewards enable effective \ac{RL} training across programming languages without language-specific execution infrastructure.
Training on a 1{,}000-task curriculum mixing Python and nine additional languages validates that static diff comparison provides sufficient learning signal for repository-level code repair.

\textbf{Nano Agent Design}: The minimalist Nano agent operates with only two tools, restricted bash and search-and-replace patching, demonstrating that operational agent competence can develop through \ac{RL} training without engineered complexity.
By sidestepping diff generation complexity through deterministic git-based canonical diff computation, the design eliminates an entire class of formatting errors orthogonal to the semantic debugging task.

\subsection{Technical Infrastructure}

\textbf{Live Weight Synchronization via \ac{NCCL}}: We extend vLLM's async engine to accept live parameter updates from distributed trainers through persistent \ac{NCCL} channels, enabling continuous policy improvement without server restarts.
The implementation handles layer-wise parameter gathering, \ac{LoRA} adapter merging before transmission, and complete KV-cache invalidation after updates, maintaining consistency between weights and cached activations.
This eliminates the conventional training-then-deploy cycle, enabling true online learning where policy updates immediately propagate to deployed inference infrastructure.

\textbf{Asynchronous Multi-Turn Architecture}: By leveraging vLLM's OpenAI-compatible \ac{API}, we enable individual agents to progress through multi-turn interactions asynchronously while maintaining synchronous batches at the episode level for group-relative optimization.
This decouples \ac{AI} application logic from trainer abstractions, ensuring the agent runs identically during training and deployment without requiring agent-specific code in the training loop.
The architecture substantially improves throughput over synchronous per-turn execution while satisfying \ac{GSPO} batch requirements.

\textbf{Academic Feasibility Optimizations}: Through composed optimizations including \ac{LoRA} parameter-efficient adaptation, DeepSpeed ZeRO-2 for optimizer state sharding, gradient checkpointing for activation memory, and custom Triton kernels for \ac{GSPO} loss computation, we demonstrate 14B parameter training on three A100 \acp{GPU} with two for training and one for inference.
These techniques compose synergistically to address distinct memory and computation bottlenecks, making online \ac{RL} for multi-turn agents tractable within typical academic allocations.

\textbf{Open-Source Release}: Complete training infrastructure, agent implementations, SLURM orchestration scripts, and evaluation protocols are released open-source, enabling reproducible investigation of online \ac{RL} techniques and reducing barriers to academic research in experiential learning for coding agents.
By providing complete open-source implementations, we enable broader academic investigation of online \ac{RL} techniques for coding agents, facilitating reproducible research and community-driven development of advanced agent training methods.

\subsection{Sustainability}
\label{subsec:sustainability}

The compute optimization techniques detailed in \cref{ch:work} directly address sustainability across environmental, economic, and social dimensions.
By enabling 14B parameter online \ac{RL} training on three A100 \acp{GPU} rather than industrial-scale clusters, we reduce both absolute energy consumption and the concentration of research capabilities in well-funded institutions.
Training completed in 144 \ac{GPU}-hours, demonstrating that meaningful \ac{RL} research remains feasible within academic compute budgets and associated energy constraints.
The execution-free reward design eliminates the operational overhead of maintaining language-specific test runners, build systems, and Docker orchestration across nine programming languages, further reducing both engineering complexity and computational requirements.
From an economic perspective, the complete open-source release of training infrastructure, agent implementations, and orchestration scripts (\cref{app:reproducibility}) creates a sustainable research ecosystem where advanced techniques remain accessible beyond proprietary laboratories, enabling sustained academic investigation rather than dependence on selective corporate disclosures.
This low-resource methodology promotes equitable research participation: by targeting modest \ac{GPU} allocations typical of academic clusters and adopting language-agnostic designs that avoid complex per-language execution infrastructure, we reduce barriers that would disproportionately burden under-resourced institutions attempting to replicate or extend this work.

\subsection{Ethics}
\label{subsec:ethics}

Autonomous code repair systems capable of repository modification raise several ethical considerations that warrant careful discussion.
While developed for beneficial bug repair, similar capabilities could theoretically facilitate automated vulnerability exploitation or malicious code injection, presenting dual-use concerns inherent to any autonomous system operating on software artifacts.
We mitigate immediate operational risks through restrictive execution environments during training, including restricted bash, per-episode tool-call limits, and wall-clock timeouts, though we acknowledge these measures target training safety rather than preventing determined misuse of released models.
Training data exclusively uses permissively licensed repositories from SWE-Gym and SWE-Bench-Multilingual, ensuring legal and ethical sourcing while avoiding exposure of proprietary code or credentials, and the execution-free approach prevents accidental leakage of secrets that might occur during test execution.
Production deployments would require stronger isolation than our SLURM-compatible approach provides; we document these limitations transparently to inform responsible deployment decisions.
The complete open-source release embodies a commitment to research transparency: by publishing training recipes, infrastructure code, and evaluation protocols, we enable independent verification and community-driven safety research, contrasting with proprietary development where capabilities and failure modes remain opaque.

\section{Limitations and Constraints} \label{sec:limitations}

The methodology presented here operates within several constraints that bound its immediate applicability and suggest directions for future refinement.
While execution-free training converges reliably and improves operational competence substantially, the approach reflects deliberate trade-offs between performance, generality, and resource requirements.

\subsection{Performance Levels and Task Scope}

While \ac{GSPO} training substantially improved scaffold adaptation, more than doubling patch completion rates from 37\% to 78\%, test-verified success on SWE-Bench-Verified remained flat at approximately 6-7\%.
This gap likely reflects insufficient training duration rather than fundamental methodological limitations.
DeepSWE~\cite{deepSWE2025}, which applies similar online \ac{RL} methods to terminal-based coding agents, demonstrates strong test-verified performance after training with more than 30x computing power for 3x longer.
Given the methodological similarities, replicating such results may primarily require additional training rather than architectural changes.

The evaluation focuses on single-commit bug fixes rather than complex architectural changes or multi-file refactoring tasks.
Repository-level changes spanning dozens of files with intricate dependency relationships present substantially harder challenges that may require different agent designs or training methodologies.

\subsection{Task Distribution and Language Coverage}

Training data combines 750 Python tasks with 250 multilingual tasks spanning nine additional languages.
This distribution reflects dataset availability: instruction-driven, repository-level multilingual debugging datasets remain substantially scarcer than Python-only resources.
Multilingual generalization is assessed through per-language reward progression across training epochs, demonstrating improvements across all languages despite the uneven data distribution.

\subsection{Computational Resource Requirements}

Online \ac{RL} training requires substantially more computation than supervised fine-tuning: each training example involves multi-turn agent interaction rather than single forward-backward passes.
While our optimizations make training tractable on three A100 \acp{GPU}, the complete system integrating distributed training, live inference serving, weight synchronization infrastructure, and episode orchestration presents implementation complexity that may challenge researchers without prior distributed training experience.
The open-source release includes comprehensive documentation and SLURM orchestration scripts to mitigate these barriers.

\subsection{Execution Environment Isolation}

Episodes execute in ephemeral repository checkouts with restricted bash (rbash) rather than full containerization.
This approach provides sufficient isolation for safe training and reproducible execution while remaining simpler to deploy on HPC clusters where container runtimes may require special permissions.
However, container-based isolation would enable higher parallelism by safely running more concurrent episodes per node, substantially increasing throughput at the cost of additional infrastructure complexity.

\subsection{Reward Function Trade-offs}

Execution-free patch-similarity rewards trade functional verification for infrastructure simplicity and language-agnostic applicability.
Static diff comparison may occasionally reward syntactically plausible but functionally incorrect patches, or penalize alternative valid solutions that achieve the same functional outcome through different code changes.
Conversely, test-based rewards would require maintaining language-specific execution environments, test runners, build systems, and dependency management for each programming language, substantially increasing engineering complexity.
The trade-off favors execution-free rewards for academic research contexts prioritizing broad language coverage and rapid experimental iteration.

\subsection{Model Architecture and Scale}

This work focuses on the Qwen3 model family (primarily 14B parameters) due to strong tool-calling capabilities and computational accessibility within academic budgets.
The methodology's effectiveness across models with different architectural characteristics, such as varying attention mechanisms, tokenization schemes, or pre-training objectives, requires validation beyond the limited multi-size experiments documented in Appendix~\ref{app:model-comparison}.
Whether the observed improvements generalize to other model families or depend on Qwen3-specific capabilities remains an open empirical question.

\section{Future Research Directions}
\label{sec:future-work}

\subsection{Cross-Group Walltime Optimization}
\label{subsec:cross-group-optimization}

Our current implementation optimizes intra-group asynchronous behavior, allowing individual episodes within a training group to progress independently.
However, the system treats episode groups synchronously: inference pauses once the configured group size is reached, waiting for training to complete and weights to synchronize before new episodes begin.
Given the substantial variation in episode completion times, where some problems resolve in five turns while others require twenty or more, this synchronous grouping creates walltime dependencies where inference servers sit idle during training updates.

Future implementations could eliminate this bottleneck through slightly off-policy episodes.
Instead of pausing inference during training, new episodes would begin immediately using the current policy while training proceeds on completed episodes in parallel.
When weight updates broadcast to inference servers, any in-flight episodes would continue with the updated policy, making them slightly off-policy since they began with policy version $\pi_n$ but will complete under $\pi_{n+1}$ or later.
Given the small, gradual nature of \ac{GSPO} updates and continuous weight synchronization, this off-policy gap remains bounded and acceptable.

This approach would maintain continuous inference server utilization, eliminate idle time during training phases, and enable training to begin as soon as any subset of episodes completes rather than waiting for full groups.
The optimization would particularly benefit throughput when episode length distributions exhibit high variance, as faster episodes immediately feed into training while longer episodes continue without blocking progress.

\subsection{Unified Training-Inference Architecture}

The current system separates training and inference across distinct \ac{GPU} pools with explicit weight synchronization.
While this design maximizes throughput by overlapping computation, it increases infrastructure requirements: even modest experiments demand three \acp{GPU} (two for training, one for inference).
Future implementations could co-locate training and inference on a single set of weights, eliminating synchronization overhead entirely and reducing the entry barrier for \ac{RL} experimentation.

In this unified architecture, the model would alternate between inference and training phases rather than running them concurrently.
Episodes would accumulate using the current policy weights, and once sufficient experience is collected, the system would pause episode generation, perform training updates in-place, then resume inference with the updated policy.
This eliminates the need for separate inference servers, weight broadcasting infrastructure, and \ac{NCCL} synchronization—reducing the minimum viable system to a single training node.

The trade-off is straightforward: training updates pause inference, reducing overall throughput compared to the parallel design.
However, for researchers with limited compute budgets or those conducting initial feasibility studies, this reduced throughput may be acceptable in exchange for dramatically lower infrastructure complexity and resource requirements.
The design would particularly benefit early-stage experimentation where validating core concepts matters more than maximizing sample efficiency.

\subsection{Multi-Task Curriculum Training}

The OpenAI-compatible \ac{API} abstraction and plug-and-play harness design naturally extend to multi-task training scenarios.
Rather than optimizing a single objective (code repair), the system could train concurrently on diverse tasks—code repair, email classification, document summarization, or any other problem admitting a structured reward signal.
Episodes from different task distributions would interleave within training groups, and the shared policy would learn to handle heterogeneous environments through the same tool-calling interface.

This multi-task formulation raises fundamental questions about capability emergence and transfer in \ac{RL} post-training.
Does sharing model capacity across diverse objectives improve generalization, allowing debugging skills to transfer to related reasoning tasks?
Or does multi-task training introduce interference, degrading performance on individual objectives compared to specialized single-task policies?
The answers would illuminate whether \ac{RL}-trained \acp{LLM} benefit from curriculum diversity analogous to pre-training's broad data mixtures, or whether task-specific specialization remains necessary for complex reasoning domains.

Investigating these questions could reveal fundamental insights about the future of \ac{RL} post-training: whether general-purpose reasoning emerges from diverse experiential curricula, or whether different cognitive skills require isolated optimization.
The infrastructure documented in this thesis provides the foundation for such investigations, requiring only task-specific harnesses and reward functions to explore multi-objective training systematically.

\subsection{Extended Training with Scaled Resources}

Several lines of evidence suggest that extended training with moderately larger models would yield stronger results.
DeepSWE demonstrates that similar online \ac{RL} methodologies achieve strong test-verified performance with longer training duration and more computational resources.
Our reward curves continue trending upward throughout training without plateauing, indicating that the policy has not exhausted learning from the patch-similarity signal.
Additionally, preliminary comparisons in \cref{app:model-comparison} show that Qwen3-32B achieves substantially higher baseline performance than the 14B variant, reflecting that \ac{RL} effectiveness scales with underlying model capability.
Future work should investigate training larger models for extended durations while maintaining the low-resource optimization techniques documented in \cref{ch:work}.
The infrastructure supports 32B parameters on 6 A100s, enabling investigation of whether the flat test-verified success rates reflect model capacity and training duration constraints rather than fundamental methodological limitations.

\section{Lessons Learned from Implementation}
\label{sec:lessons-learned}

The process of implementing and deploying online \ac{RL} for coding agents yielded insights that extend beyond the specific technical contributions documented in this thesis.
These reflections illuminate both the opportunities and challenges inherent in experiential learning approaches for complex reasoning tasks.

\subsection{System Complexity and Engineering Effort}

Integrating large language models with online \ac{RL} training revealed engineering challenges that significantly exceeded initial estimates.

Many critical failure modes only emerged during extended training runs spanning multiple days, revealing the difficulty of predicting distributed system behavior through local testing.
Robust monitoring infrastructure and iterative refinement proved essential—ambitious initial designs consistently required substantial simplification before achieving stability.

Engineering effort for production-ready implementation substantially outweighed theoretical development.
Implementing weight-synchronized OpenAI-compatible \ac{API} servers required many weeks of trial and error to achieve stable operation, revealing that infrastructure reliability demands sustained investment beyond algorithmic innovation.

Integrating bleeding-edge libraries amplified this challenge.
The system relied on rapidly evolving dependencies including TRL for \ac{RL} training, vLLM for inference serving, Liger-Kernel for memory-efficient kernels, and Flash Attention for optimized attention, each offering significant performance benefits but introducing breaking changes with every update.
We forked and extensively modified these projects to resolve conflicts, implement missing features, and fix blocking bugs.
Many modifications were contributed back upstream, but each library update frequently broke the entire system, requiring substantial effort to restore compatibility.
This experience highlights an inherent tension in cutting-edge research where leveraging the latest capabilities requires accepting integration burden that can exceed core research implementation effort.

\subsection{Reward Misspecification}

Initial experiments with auxiliary rewards revealed subtle failure modes in multi-objective training.
We introduced a file-matching reward to provide intermediate learning signal, hypothesizing it would help the agent learn to identify relevant files before optimizing patch quality.
However, because the primary diff similarity reward remained sparse while the file-matching reward provided frequent feedback, the model optimized aggressively for the auxiliary objective to the detriment of actual patch similarity.

Evaluating successive training checkpoints on SWE-Bench-Verified exposed this misspecification clearly.
Downstream performance improved initially but noticeably worsened after a certain point in training, even as file-matching accuracy continued increasing.
The agent learned to modify the correct files consistently while producing increasingly poor patches, a textbook case of reward hacking where optimizing a proxy metric degraded the true objective.
This experience reinforced the value of sparse, well-aligned rewards over dense but misspecified shaping signals.

\section{Broader Reflections on Reinforcement Learning}
\label{sec:final-reflections}

The application of \ac{RL} to real-world tasks like coding represents a fundamental shift in what reinforcement learning can accomplish.
AlphaZero's superhuman game performance demonstrated the power of learning through interaction, yet simultaneously exposed a critical limitation: games provide reward signals for every legal move, while real-world tasks require deep conceptual understanding just to participate meaningfully.
Before \acp{LLM}, applying \ac{RL} to tasks like code repair was futile not because the algorithms were inadequate, but because models lacked the prerequisite knowledge to attempt such problems coherently.
An \ac{RL} agent cannot learn from experience if it cannot generate experiences worth learning from; there is no gradient to climb when every action produces meaningless output.

\acp{LLM} solved this foundational problem by providing the conceptual substrate necessary for coherent interaction with complex environments.
When an \ac{LLM} based \ac{RL} agent attempts to fix a bug, it can execute valid commands, parse error messages, and propose syntactically reasonable patches.
These capabilities, acquired through massive-scale pretraining, give the agent that crucial first rung on the ladder, enabling the generation of rich learning signals that \ac{RL} algorithms can exploit.
For the first time, we can apply decades of reinforcement learning research to problems that matter, because \acp{LLM} have bridged the gap between random exploration and meaningful interaction.
This thesis demonstrates that potential in practice: online \ac{RL} training for coding agents is feasible within academic compute constraints and yields measurable improvements in scaffold adaptation.
The convergence of \acp{LLM} and \ac{RL} enables training approaches previously confined to simple domains with well-defined action spaces to extend to complex reasoning tasks like repository-level debugging.
By demonstrating that debugging behaviors emerge from simple tools combined with \ac{RL} training, this thesis provides evidence that experiential learning complements supervised approaches for interactive agent development.

The infrastructure and methodologies developed here offer a foundation for future investigations into how agents acquire complex skills through environmental interaction, extending beyond coding to domains where \acp{LLM} provide sufficient baseline capabilities for meaningful participation.
Perhaps most importantly, this work demonstrates that sophisticated \ac{AI} research infrastructure can be developed within academic settings through careful engineering and open-source collaboration.
By providing accessible implementations of online \ac{RL} training systems, we enable broader investigation of experiential learning approaches that might otherwise remain confined to well-resourced industry laboratories.

The core principles validated here, including execution-free rewards enabling language-agnostic training, live weight synchronization enabling efficient multi-turn agent training, and systematic optimization making online \ac{RL} tractable within academic budgets, suggest productive directions for future research.
Whether these approaches scale to match frontier capabilities or require fundamental architectural advances remains an open question, but the foundation established here demonstrates that open, reproducible research in experiential learning for coding agents is both viable and valuable.
The questions worth asking are now more clearly defined, and the tools to investigate them are available to the broader research community.
