\newpage
\appendix
\newpage
\etocdepthtag.toc{mtappendix}
\etocsettagdepth{mtchapter}{none}
\etocsettagdepth{mtappendix}{subsection}
\etoctocstyle{1}{Appendix - Contents}
\tableofcontents
\newpage

\chapter{Interactive Sankey Diagrams}
\label{app:sankey}

This appendix contains interactive Sankey diagrams that visualize command flow patterns during training.

\chapter{Model Comparison: Capacity and Architecture}
\label{app:model-comparison}

This appendix presents limited comparisons to illustrate capacity scaling and architectural effects.
We compare Qwen3-8B vs.
Llama3.1-8B (8B architectural comparison) and conduct limited scaling analysis across Qwen3 model sizes (8B, 14B, 30B-A3B, 32B).
These experiments are not central to the thesis investigation but provide insight into model selection and capacity effects.

\section{8B Architectural Comparison: Qwen3-8B vs.
  Llama3.1-8B} \label{app:8b-comparison}

\todoinline{Present head-to-head comparison of Qwen3-8B and Llama3.1-8B training results, showing substantially lower reward progression for Llama (~3x lower than Qwen3-8B).
Illustrate that tool-calling capability is critical for this training regime.
Discuss implications for model selection in agentic RL settings.
}

\section{Limited Scaling Analysis}
\label{app:scaling}

\todoinline{Present limited scaling analysis across Qwen3 model sizes: 8B, 14B (primary), 30B-A3B, and 32B.
Show capacity scaling trends in tool-call success rates, reward progression, and SWE-Bench-Verified performance if available.
Note infrastructure differences (e.g., ZeRO-3 requirement for larger models, GPU count scaling).
}

\chapter{Additional Experimental Data}
\label{app:data}

\section{Nano Agent Benchmark Results}
\label{app:nano-benchmark}

This section presents baseline performance results for the Nano agent across various language models on a subset of SWE-bench Lite.
The benchmark evaluates models' ability to explore codebases and generate fixes using the Nano agent's minimal terminal-based interface.

\subsection{Methodology}

The benchmark follows this protocol: \begin{itemize}
	\item \textbf{Input}: GitHub repositories containing bugs with known ground truth solutions
	\item \textbf{Task}: Models use Nano's tools to explore codebases and generate fixes
	\item \textbf{Output}: Unified git diffs containing all proposed code changes
	\item \textbf{Evaluation}: Solutions are measured against ground truth using:
	      \begin{itemize}
		      \item Code Similarity: Patch matching with actual bug fix (primary ranking metric)
		      \item Test Similarity: Test change alignment with ground truth test updates
	      \end{itemize} \end{itemize}

\subsection{Results}

\ref{tab:nano-benchmark} presents performance results ranked by code similarity.
Note that these results are subject to noise due to the small test set with limited repetitions.

\begin{table}[htbp]
	\centering
	\caption{Nano Agent Benchmark Performance on subset of SWE-bench Lite}
	\label{tab:nano-benchmark}
	\footnotesize
	\begin{tabular}{@{}cllcccc@{}}
\toprule
\textbf{Rank} & \textbf{Version} & \textbf{Model} & \textbf{Code Sim} & \textbf{Test Sim} & \textbf{Tokens} & \textbf{Tools} \\
\midrule
1 & v3.2.0 & claude-sonnet-4-20250514 & \texttt{0.394} & \texttt{0.188} & \texttt{14,746 / 16,384} & \texttt{41.5 / 100} \\
2 & v3.2.0 & gpt-4.1 & \texttt{0.387} & \texttt{0.092} & \texttt{9,777 / 16,384} & \texttt{35.7 / 100} \\
3 & v4.0.2 & deepseek-chat-v3.1 & \texttt{0.383} & \texttt{0.078} & \texttt{11,762 / 16,384} & \texttt{41.9 / 100} \\
4 & v4.0.1 & kimi-k2 & \texttt{0.382} & \texttt{0.009} & \texttt{5,508 / 16,384} & \texttt{19.7 / 100} \\
5 & v4.0.1 & qwen3-coder & \texttt{0.374} & \texttt{0.042} & \texttt{6,979 / 16,384} & \texttt{26.5 / 100} \\
6 & v3.2.0 & gemini-2.5-pro-preview & \texttt{0.370} & \texttt{0.034} & \texttt{6,008 / 16,384} & \texttt{13.6 / 100} \\
7 & v3.3.0 & gemini-2.5-flash & \texttt{0.363} & \texttt{0.022} & \texttt{4,337 / 16,384} & \texttt{13.2 / 100} \\
8 & v3.2.0 & gemini-2.5-flash-preview-05-20 & \texttt{0.362} & \texttt{0.000} & \texttt{4,547 / 16,384} & \texttt{10.1 / 100} \\
9 & v3.2.0 & gpt-4.1-mini & \texttt{0.350} & \texttt{0.017} & \texttt{7,403 / 16,384} & \texttt{29.7 / 100} \\
10 & v3.2.0 & deepseek-chat & \texttt{0.336} & \texttt{0.011} & \texttt{3,297 / 16,384} & \texttt{7.5 / 100} \\
11 & v4.0.1 & glm-4.5 & \texttt{0.323} & \texttt{0.107} & \texttt{12,477 / 16,384} & \texttt{28.7 / 100} \\
12 & v3.2.0 & qwen-2.5-72b-instruct & \texttt{0.272} & \texttt{0.000} & \texttt{5,873 / 16,384} & \texttt{35.1 / 100} \\
13 & v3.2.0 & qwen3-32b & \texttt{0.255} & \texttt{0.000} & \texttt{5,281 / 16,384} & \texttt{28.3 / 100} \\
14 & v3.2.0 & llama-4-maverick & \texttt{0.255} & \texttt{0.000} & \texttt{4,647 / 16,384} & \texttt{10.4 / 100} \\
15 & v3.2.0 & qwen3-8b & \texttt{0.190} & \texttt{0.000} & \texttt{8,704 / 16,384} & \texttt{56.5 / 100} \\
16 & v3.2.0 & gpt-4.1-nano & \texttt{0.188} & \texttt{0.000} & \texttt{8,536 / 16,384} & \texttt{33.1 / 100} \\
17 & v3.2.0 & qwen3-14b & \texttt{0.176} & \texttt{0.000} & \texttt{10,800 / 16,384} & \texttt{82.6 / 100} \\
18 & v3.2.0 & devstral-small & \texttt{0.092} & \texttt{0.000} & \texttt{14,603 / 16,384} & \texttt{13.0 / 100} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Key Observations}

Several patterns emerge from these baseline results:

\textbf{Frontier Model Performance}: Claude Sonnet 4 and GPT-4.1 achieve the highest code similarity scores of tested models (0.394 and 0.387 respectively), demonstrating superior ability to understand repository structure and generate appropriate fixes through the Nano interface.

\textbf{Token Efficiency}: Models vary significantly in token usage, with some (like DeepSeek Chat) using only 3,297 tokens while others (like Claude Sonnet 4) approach the 16,384 token limit.
This suggests different exploration strategies and verbosity levels.

\textbf{Tool Usage Patterns}: Tool usage rates range from 7.5\% (DeepSeek Chat) to 82.6\% (Qwen3-14B), indicating substantial variation in how models leverage the available terminal commands and file operations.

\textbf{Test Similarity Gap}: Most models achieve low test similarity scores, suggesting that generating appropriate test changes alongside code fixes remains challenging even for frontier models.

\section{Reproducibility and Open-Source Release}
\label{app:reproducibility}

All implementation code, training configurations, evaluation scripts, and infrastructure setup are released as open-source software.
The repository includes complete SLURM job templates, Apptainer container definitions, vLLM server configurations with our worker extensions, TRL trainer modifications for multi-turn agent training, and all plotting and analysis scripts used to generate figures in this thesis.
Detailed documentation provides setup instructions for replicating the training environment on \ac{NAISS} clusters or similar HPC infrastructure.

This section documents configuration details and hyperparameters used throughout experimentation.

\subsection{Sampling parameters}
\label{app:sampling-params}

\subsubsection{Decoding and Exploration Policy}

The temperature parameter controls the exploration-exploitation tradeoff during agent operation.
During training, we decode with temperature 1.0 without top-p or top-k filtering, providing substantial exploration necessary for \ac{RL} learning.
During evaluation, we use temperature 0.2 with top-p 0.9 to exploit the learned policy.

\begin{table}[htbp]
\centering
\caption{Sampling and episode parameters for Nano agent evaluation. Values reflect the main settings used for training and SWE-Bench-Lite evaluation. \todoinline{Fill in concrete values for each model and add additional columns if more models are reported.}}
\label{tab:sampling-params}
\begin{tabular}{lccc}
\toprule
\textbf{Parameter} & \textbf{Qwen3-8B} & \textbf{Qwen3-14B} & \textbf{Qwen3-30B-A3B} \\
\midrule
Temperature & 1.0 (train), 0.2 (eval) & 1.0 (train), 0.2 (eval) & 1.0 (train), 0.2 (eval) \\
Top-k & --- & --- & --- \\
Top-p & 0.9 & 0.9 & 0.9 \\
Min-p & --- & --- & --- \\
Token limit & 12,288 & 12,288 & 12,288 \\
Tool limit & 30 & 30 & 30 \\
Time limit & 60\,s $\pm$20\,s & 60\,s $\pm$20\,s & 60\,s $\pm$20\,s \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Episode Termination Parameters}
\label{app:termination-parameters}

\begin{table}[htbp]
\centering
\caption{Episode termination parameters for the Nano agent. These values are deliberately set on the lower end of typical agent system budgets to encourage efficient exploration and repair strategies during training.}
\label{tab:termination-parameters}
\begin{tabular}{lcc}
\toprule
\textbf{Parameter} & \textbf{Value} & \textbf{Notes} \\
\midrule
Tool calls & 30 & Maximum number of \texttt{shell} or \texttt{apply\_patch} calls \\
Token budget & 12{,}288 & Cumulative generation tokens across all turns \\
Wall-clock timeout & 60 seconds & For 14B reference model \\
Timeout tolerance & \textpm{}20 seconds & Adjusted for smaller/larger models \\
Per-call output & 2{,}000 chars & Truncation limit for individual tool returns \\
\bottomrule
\end{tabular}
\end{table}

\subsection{\ac{LoRA}
Configuration} \label{app:lora-config}

Unless otherwise stated, we use the following \acl{LoRA} settings:

\begin{verbatim}
lora: true
r: 32
lora_alpha: 64   # ~2x the rank per \cite{lora2021}
target_modules:
  - q_proj
  - k_proj
  - v_proj
  - o_proj
  - gate_proj
  - up_proj
  - down_proj
\end{verbatim}

These choices balance adapter capacity and efficiency and allow adaptation of both attention and MLP projections when needed.

\subsection{vLLM Server Configuration}
\label{app:vllm-config}

\todoinline{Document vLLM server flags, batching settings, scheduler adjustments, continuous batching parameters, KV-cache configuration, function-calling enforcement settings, and JSON error handling configuration.}

\subsection{SLURM Configuration}
\label{app:slurm-config}

\todoinline{Document SLURM submission templates, environment modules, ephemeral repository setup, workspace isolation configuration, cgroups settings, rbash configuration, and optional Apptainer/Singularity containerization.}

\subsection{Deepspeed Configuration}
\label{app:deepspeed-config}

\todoinline{Document NCCL environment variables, communication group setup, gradient synchronization configuration, policy broadcasting setup, async broadcast parameters, memory-bounded gathering implementation, and bandwidth optimization settings.}

\subsection{System Prompts}
\label{app:system-prompts}

\todoinline{Document complete system prompts used for agent initialization, including task presentation format, tool documentation, repository context, and termination instructions.
Include both training and evaluation prompt variants.
}

\subsection{Training Hyperparameters}
\label{app:training-hyperparameters}

\todoinline{Document complete training hyperparameters: learning rate, warmup schedule, optimizer settings (AdamW betas, weight decay), gradient accumulation steps, gradient clipping, batch size (per-device and effective), group size for GSPO, clipping epsilon, KL penalty coefficient, number of epochs, checkpoint frequency, evaluation frequency, and random seeds.}

\subsection{Data Processing Pipeline}
\label{app:data-processing}

\todoinline{Document data processing scripts and pipeline: dataset loading, task filtering, repository cloning, diff extraction, canonical diff computation, reward computation implementation, and episode caching strategy.}

\subsection{Logging and Artifact Structure}
\label{app:logging}

\todoinline{Document per-episode logging structure including: tool traces (invocations, arguments, outputs), canonical diffs, reward signals, token counts, timings, RNG states, and training dynamics.
Describe artifact organization and storage format for reproducibility.
}