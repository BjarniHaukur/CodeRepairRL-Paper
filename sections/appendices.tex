\newpage
\appendix
\newpage
\etocdepthtag.toc{mtappendix}
\etocsettagdepth{mtchapter}{none}
\etocsettagdepth{mtappendix}{subsection}
\etoctocstyle{1}{Appendix - Contents}
\tableofcontents
\newpage

\chapter{Interactive Sankey Diagrams}
\label{app:sankey}

This appendix contains interactive Sankey diagrams that visualize command flow patterns during training.

\chapter{Model Comparison: Capacity and Architecture}
\label{app:model-comparison}

This appendix presents limited comparisons to illustrate capacity scaling and architectural effects.
We compare Qwen3-8B vs.
Llama3.1-8B (8B architectural comparison) and conduct limited scaling analysis across Qwen3 model sizes (8B, 14B, 30B-A3B, 32B).
These experiments are not central to the thesis investigation but provide insight into model selection and capacity effects.

\section{8B Architectural Comparison: Qwen3-8B vs.
  Llama3.1-8B} \label{app:8b-comparison}

\todoinline{Present head-to-head comparison of Qwen3-8B and Llama3.1-8B training results, showing substantially lower reward progression for Llama (~3x lower than Qwen3-8B).
Illustrate that tool-calling capability is critical for this training regime.
Discuss implications for model selection in agentic RL settings.
}

\section{Limited Scaling Analysis}
\label{app:scaling}

\todoinline{Present limited scaling analysis across Qwen3 model sizes: 8B, 14B (primary), 30B-A3B, and 32B.
Show capacity scaling trends in tool-call success rates, reward progression, and SWE-Bench-Verified performance if available.
Note infrastructure differences (e.g., ZeRO-3 requirement for larger models, GPU count scaling).
}

\chapter{Additional Experimental Data}
\label{app:data}

\section{Nano Agent Benchmark Results}
\label{app:nano-benchmark}

This section presents baseline performance results for the Nano agent across various language models on a subset of SWE-bench Lite.
The benchmark evaluates models' ability to explore codebases and generate fixes using the Nano agent's minimal terminal-based interface.

\subsection{Methodology}

The benchmark follows this protocol: \begin{itemize}
	\item \textbf{Input}: GitHub repositories containing bugs with known ground truth solutions
	\item \textbf{Task}: Models use Nano's tools to explore codebases and generate fixes
	\item \textbf{Output}: Unified git diffs containing all proposed code changes
	\item \textbf{Evaluation}: Solutions are measured against ground truth using:
	      \begin{itemize}
		      \item Code Similarity: Patch matching with actual bug fix (primary ranking metric)
		      \item Test Similarity: Test change alignment with ground truth test updates
	      \end{itemize} \end{itemize}

\subsection{Results}

\ref{tab:nano-benchmark} presents performance results ranked by code similarity.
Note that these results are subject to noise due to the small test set with limited repetitions.

\begin{table}[htbp]
	\centering
	\caption{Nano Agent Benchmark Performance on subset of SWE-bench Lite}
	\label{tab:nano-benchmark}
	\footnotesize
	\begin{tabular}{@{}cllcccc@{}}
\toprule
\textbf{Rank} & \textbf{Version} & \textbf{Model} & \textbf{Code Sim} & \textbf{Test Sim} & \textbf{Tokens} & \textbf{Tools} \\
\midrule
1 & v3.2.0 & claude-sonnet-4-20250514 & \texttt{0.394} & \texttt{0.188} & \texttt{14,746 / 16,384} & \texttt{41.5 / 100} \\
2 & v3.2.0 & gpt-4.1 & \texttt{0.387} & \texttt{0.092} & \texttt{9,777 / 16,384} & \texttt{35.7 / 100} \\
3 & v4.0.2 & deepseek-chat-v3.1 & \texttt{0.383} & \texttt{0.078} & \texttt{11,762 / 16,384} & \texttt{41.9 / 100} \\
4 & v4.0.1 & kimi-k2 & \texttt{0.382} & \texttt{0.009} & \texttt{5,508 / 16,384} & \texttt{19.7 / 100} \\
5 & v4.0.1 & qwen3-coder & \texttt{0.374} & \texttt{0.042} & \texttt{6,979 / 16,384} & \texttt{26.5 / 100} \\
6 & v3.2.0 & gemini-2.5-pro-preview & \texttt{0.370} & \texttt{0.034} & \texttt{6,008 / 16,384} & \texttt{13.6 / 100} \\
7 & v3.3.0 & gemini-2.5-flash & \texttt{0.363} & \texttt{0.022} & \texttt{4,337 / 16,384} & \texttt{13.2 / 100} \\
8 & v3.2.0 & gemini-2.5-flash-preview-05-20 & \texttt{0.362} & \texttt{0.000} & \texttt{4,547 / 16,384} & \texttt{10.1 / 100} \\
9 & v3.2.0 & gpt-4.1-mini & \texttt{0.350} & \texttt{0.017} & \texttt{7,403 / 16,384} & \texttt{29.7 / 100} \\
10 & v3.2.0 & deepseek-chat & \texttt{0.336} & \texttt{0.011} & \texttt{3,297 / 16,384} & \texttt{7.5 / 100} \\
11 & v4.0.1 & glm-4.5 & \texttt{0.323} & \texttt{0.107} & \texttt{12,477 / 16,384} & \texttt{28.7 / 100} \\
12 & v3.2.0 & qwen-2.5-72b-instruct & \texttt{0.272} & \texttt{0.000} & \texttt{5,873 / 16,384} & \texttt{35.1 / 100} \\
13 & v3.2.0 & qwen3-32b & \texttt{0.255} & \texttt{0.000} & \texttt{5,281 / 16,384} & \texttt{28.3 / 100} \\
14 & v3.2.0 & llama-4-maverick & \texttt{0.255} & \texttt{0.000} & \texttt{4,647 / 16,384} & \texttt{10.4 / 100} \\
15 & v3.2.0 & qwen3-8b & \texttt{0.190} & \texttt{0.000} & \texttt{8,704 / 16,384} & \texttt{56.5 / 100} \\
16 & v3.2.0 & gpt-4.1-nano & \texttt{0.188} & \texttt{0.000} & \texttt{8,536 / 16,384} & \texttt{33.1 / 100} \\
17 & v3.2.0 & qwen3-14b & \texttt{0.176} & \texttt{0.000} & \texttt{10,800 / 16,384} & \texttt{82.6 / 100} \\
18 & v3.2.0 & devstral-small & \texttt{0.092} & \texttt{0.000} & \texttt{14,603 / 16,384} & \texttt{13.0 / 100} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Key Observations}

Several patterns emerge from these baseline results:

\textbf{Frontier Model Performance}: Claude Sonnet 4 and GPT-4.1 achieve the highest code similarity scores of tested models (0.394 and 0.387 respectively), demonstrating superior ability to understand repository structure and generate appropriate fixes through the Nano interface.

\textbf{Token Efficiency}: Models vary significantly in token usage, with some (like DeepSeek Chat) using only 3,297 tokens while others (like Claude Sonnet 4) approach the 16,384 token limit.
This suggests different exploration strategies and verbosity levels.

\textbf{Tool Usage Patterns}: Tool usage rates range from 7.5\% (DeepSeek Chat) to 82.6\% (Qwen3-14B), indicating substantial variation in how models leverage the available terminal commands and file operations.

\textbf{Test Similarity Gap}: Most models achieve low test similarity scores, suggesting that generating appropriate test changes alongside code fixes remains challenging even for frontier models.

\section{Sampling parameters}
\label{app:sampling-params}

\subsection{Decoding and Exploration Policy}

The temperature parameter controls the exploration-exploitation tradeoff during agent operation.
During training, we decode with temperature 1.0 without top-p or top-k filtering, providing substantial exploration necessary for \ac{RL} learning.
During evaluation, we use temperature 0.2 with top-p 0.9 to exploit the learned policy.

\begin{table}[htbp]
\centering
\caption{Sampling and episode parameters for Nano agent evaluation. Values reflect the main settings used for training and SWE-Bench-Lite evaluation. \todoinline{Fill in concrete values for each model and add additional columns if more models are reported.}}
\label{tab:sampling-params}
\begin{tabular}{lccc}
\toprule
\textbf{Parameter} & \textbf{Qwen3-8B} & \textbf{Qwen3-14B} & \textbf{Qwen3-30B-A3B} \\
\midrule
Temperature & 1.0 (train), 0.2 (eval) & 1.0 (train), 0.2 (eval) & 1.0 (train), 0.2 (eval) \\
Top-k & --- & --- & --- \\
Top-p & 0.9 & 0.9 & 0.9 \\
Min-p & --- & --- & --- \\
Token limit & 12,288 & 12,288 & 12,288 \\
Tool limit & 30 & 30 & 30 \\
Time limit & 60\,s $\pm$20\,s & 60\,s $\pm$20\,s & 60\,s $\pm$20\,s \\
\bottomrule
\end{tabular}
\end{table}

\section{Episode Termination Parameters}
\label{app:termination-parameters}

\begin{table}[htbp]
\centering
\caption{Episode termination parameters for the Nano agent. These values are deliberately set on the lower end of typical agent system budgets to encourage efficient exploration and repair strategies during training.}
\label{tab:termination-parameters}
\begin{tabular}{lcc}
\toprule
\textbf{Parameter} & \textbf{Value} & \textbf{Notes} \\
\midrule
Tool calls & 30 & Maximum number of \texttt{shell} or \texttt{apply\_patch} calls \\
Token budget & 12{,}288 & Cumulative generation tokens across all turns \\
Wall-clock timeout & 60 seconds & For 14B reference model \\
Timeout tolerance & \textpm{}20 seconds & Adjusted for smaller/larger models \\
Per-call output & 2{,}000 chars & Truncation limit for individual tool returns \\
\bottomrule
\end{tabular}
\end{table}

\section{\ac{LoRA}
  Configuration} \label{app:lora-config}

Unless otherwise stated, we use the following \acl{LoRA} settings:

\begin{verbatim}
lora: true
r: 32
lora_alpha: 64   # ~2x the rank per \cite{lora2021}
target_modules:
  - q_proj
  - k_proj
  - v_proj
  - o_proj
  - gate_proj
  - up_proj
  - down_proj
\end{verbatim}

These choices balance adapter capacity and efficiency and allow adaptation of both attention and MLP projections when needed.

\section{vLLM Server Configuration}
\label{app:vllm-config}

\todoinline{Document vLLM server flags, batching settings, scheduler adjustments, continuous batching parameters, KV-cache configuration, function-calling enforcement settings, and JSON error handling configuration.}

\section{TRL Modifications}
\label{app:trl-modifications}

\todoinline{Document modifications to TRL trainer and data collator modules: masked loss integration, sequence-level importance ratio computation, variable-turn rollout collection, LoRA-ZeRO reconciliation, gradient accumulation/checkpointing compatibility, and API extensions.}

\section{SLURM Configuration}
\label{app:slurm-config}

\todoinline{Document SLURM submission templates, environment modules, ephemeral repository setup, workspace isolation configuration, cgroups settings, rbash configuration, and optional Apptainer/Singularity containerization.}

\section{Deepspeed Configuration}
\label{app:deepspeed-config}

\todoinline{Document NCCL environment variables, communication group setup, gradient synchronization configuration, policy broadcasting setup, async broadcast parameters, memory-bounded gathering implementation, and bandwidth optimization settings.}

\section{Masked Loss Computation for Tool-Augmented \ac{RL}}
\label{app:masked-loss}

This appendix provides the complete mathematical formulation of our dual-masking strategy for training tool-augmented language models, expanding on the presentation in \S3.4 of Chapter~\cref{ch:method}.

\subsection{Problem Formulation}

Consider an agent trajectory $\tau$ composed of alternating agent actions and tool responses.
Let $\mathbf{x} = [x_1, x_2, \ldots, x_n]$ denote the tokenized trajectory, where tokens originate from three distinct sources: \begin{enumerate}
	\item System prompts and task descriptions (provided by the environment)
	\item Agent-authored content including reasoning text and tool invocations (generated by the model)
	\item Tool outputs including command results and file contents (provided by the environment)
\end{enumerate}

Define $\mathcal{T} \subset \{1, 2, \ldots, n\}$ as the set of indices corresponding to tool-generated tokens, and let $\mathcal{A} \subset \{1, 2, \ldots, n\}$ denote agent-authored token indices.
The remaining tokens belong to system context.

\subsection{Dual-Mask Formulation}

We employ distinct masks for attention computation and loss calculation to preserve contextual information while focusing optimization on agent decisions.

\paragraph{Attention Mask.}
During forward propagation, the model processes the complete sequence with standard causal attention: \[ \mathbf{M}_{\text{att}}[i,j] = \begin{cases}
	1 & \text{if } j \leq i \\
	0 & \text{otherwise}
\end{cases} \] This ensures the model has access to all prior context—including tool outputs—when computing representations and probabilities for agent-authored tokens.

\paragraph{Loss Mask.}
For gradient computation, we mask out non-agent tokens to concentrate optimization exclusively on agent decisions: \[ \mathbf{M}_{\text{loss}}[i] = \begin{cases}
	1 & \text{if } i \in \mathcal{A} \text{ (agent-authored)} \\
	0 & \text{otherwise}
\end{cases} \]

The masked policy gradient loss under \ac{GSPO} becomes: \begin{equation}
	\mathcal{L}_{\text{policy}} = \frac{\sum_{i=1}^{n} \mathbf{M}_{\text{loss}}[i] \cdot \ell_i(a_i; \theta) \cdot A_i}{\sum_{i=1}^{n} \mathbf{M}_{\text{loss}}[i]}
	\label{eq:masked-gspo-loss}
\end{equation} where: \begin{itemize}
	\item $\ell_i(a_i; \theta)$ is the per-token policy loss (negative log-probability with importance weighting)
	\item $A_i$ is the advantage computed via group-relative baseline
	\item The denominator normalizes over agent-authored tokens only
\end{itemize}

\subsection{Algorithmic Properties}

The dual-mask strategy provides several formal guarantees:

\paragraph{Zero Gradient at Tool Tokens.
}
For all $i \in \mathcal{T}$: \[ \frac{\partial \mathcal{L}_{\text{policy}}}{\partial \log p_\theta(x_i | x_{<i})} = 0 \] This follows immediately from $\mathbf{M}_{\text{loss}}[i] = 0$ in Equation~\ref{eq:masked-gspo-loss}.

\paragraph{Preserved Contextual Information.}
Despite zero loss gradient, tool tokens contribute to hidden state computation through the attention mechanism.
For any agent token $j \in \mathcal{A}$: \[ h_j = f_\theta(x_{\leq j}) = f_\theta(x_{\mathcal{S} \cup \mathcal{A} \cup \mathcal{T} : \leq j}) \] where $f_\theta$ denotes the transformer forward pass.
The model thus maintains full observability of tool responses when making subsequent decisions.

\paragraph{Computational Efficiency.}
Masking integrates seamlessly with existing implementations through element-wise multiplication of loss tensors.
No additional forward or backward passes are required; the dual-mask incurs negligible overhead beyond standard training.

\paragraph{Gradient Flow Analysis.}
While tool tokens contribute zero direct gradient, they influence training through their effect on subsequent agent tokens.
For agent token $j$ following tool output at position $i < j$ where $i \in \mathcal{T}$ and $j \in \mathcal{A}$: \[ \frac{\partial \mathcal{L}_{\text{policy}}}{\partial \theta} \propto \frac{\partial \ell_j}{\partial h_j} \cdot \frac{\partial h_j}{\partial h_i} \cdot \frac{\partial h_i}{\partial \theta} \] The chain rule ensures parameters influencing tool token representations still receive gradients through their downstream effect on agent decisions.

\section{Complete Training Metrics}

This section would contain complete training logs and additional metrics that supplement the main results in Chapter~\cref{ch:results}.

\todoinline{Include additional data tables, extended statistical analyses, and supplementary experimental results once available.}
