\newpage
\appendix
\newpage
\etocdepthtag.toc{mtappendix}
\etocsettagdepth{mtchapter}{none}
\etocsettagdepth{mtappendix}{subsection}
\etoctocstyle{1}{Appendix - Contents}
\tableofcontents
\newpage

\chapter{Limited Size and Model Family Ablation}
\label{app:model-comparison}

The choice of Qwen3-14B as the primary model for this thesis emerged from systematic exploration of multiple model families and sizes, balancing computational constraints with task performance.
At the time of experimentation, Qwen3 was widely regarded as the strongest open-weight coding model, particularly for tool-augmented tasks.
Qwen3-14B offered an optimal fit for available resources: Qwen3-8B proved too large for two \acp{GPU} yet too small to justify three, while the 32B and 30B-A3B variants, though promising, imposed prohibitive computational demands for exploratory training runs.

Beyond capacity considerations, the combination of tool-calling capability and sufficient reasoning ability proved critical for agentic \ac{RL} training.
Llama3.1-8B, while competent at formatting tool calls, exhibited near-zero rewards throughout preliminary training, suggesting it lacked the problem-solving intelligence necessary to use tools effectively for repository navigation and bug localization.
Gemma3's tool-calling interface did not adapt well to the structured command sequences required for repository navigation and code editing.
Ministral substantially underperformed compared to more recent releases.
GLM4-9B showed promise but suffered from inadequate support in the Transformers and vLLM ecosystems at the time of experimentation.
Similarly, Llama3.1-Nemotron (Nvidia) demonstrated potential but would have required extensive vLLM refactoring to accommodate its custom tool-calling parser.

These constraints narrowed the viable candidates to the Qwen3 family, where robust tool-calling support, strong baseline coding performance, and flexible scaling across 8B, 14B, 30B-A3B and 32B enabled systematic capacity analysis within consistent infrastructure.

\todoinline{Present reward progression curves across Qwen3 model sizes (8B, 14B, 30B-A3B, 32B) and Llama3.1-8B comparison.
Show capacity scaling trends.
}

\chapter{Nano Agent Benchmark}
\label{app:nano-benchmark}

During Nano development, we established this benchmark to track performance improvements across agent versions and design iterations, initially validating changes against GPT-4.1-mini.
Without systematic evaluation, determining whether alternative tooling strategies or prompt variations improved performance would have been infeasible given the stochastic nature of agent behavior and the lack of deterministic success metrics during development.

The benchmark operates identically to the inference component of our training system: Nano serves as the agent scaffold while hosted language models execute terminal commands to repair buggy GitHub repositories.
The evaluation set consists of a 10-task subset drawn from SWE-Bench-Lite, with each reported metric representing the average of 1 to 8 runs per model, with run counts determined pragmatically by API cost constraints.
Solutions are measured against ground truth using code similarity (patch matching with the actual bug fix, serving as the primary ranking metric) and test similarity (alignment of test changes with ground truth test updates).

Populating the benchmark with frontier model performance provides a useful validation: the resulting rankings align closely with general community consensus on agentic coding performance, with Claude Sonnet 4 achieving the highest code similarity scores (0.394).
Models exhibit substantial variation in token efficiency, with DeepSeek Chat consuming only 3,297 tokens while Claude Sonnet 4 approaches the 16,384-token limit, reflecting different exploration strategies and verbosity patterns.
Most models achieve low test similarity scores, suggesting that generating appropriate test modifications alongside code fixes remains challenging even for frontier systems.
Interestingly, frontier models displayed a tendency to implement test cases despite no such requirement in the task prompt, likely reflecting training practices carefully cultivated in closed-source research labs.

\begin{table}[htbp]
	\centering
	\caption{Nano Agent Benchmark Performance on subset of SWE-bench Lite. Error bars on these results are substantial given the small evaluation set (10 tasks) and variable run counts (1 to 8 per model).}
	\label{tab:nano-benchmark}
	\footnotesize
	\begin{tabular}{@{}cllcccc@{}}
\toprule
\textbf{Rank} & \textbf{Version} & \textbf{Model} & \textbf{Code Sim} & \textbf{Test Sim} & \textbf{Tokens} & \textbf{Tools} \\
\midrule
1 & v3.2.0 & claude-sonnet-4-20250514 & \texttt{0.394} & \texttt{0.188} & \texttt{14,746 / 16,384} & \texttt{41.5 / 100} \\
2 & v3.2.0 & gpt-4.1 & \texttt{0.387} & \texttt{0.092} & \texttt{9,777 / 16,384} & \texttt{35.7 / 100} \\
3 & v4.0.2 & deepseek-chat-v3.1 & \texttt{0.383} & \texttt{0.078} & \texttt{11,762 / 16,384} & \texttt{41.9 / 100} \\
4 & v4.0.1 & kimi-k2 & \texttt{0.382} & \texttt{0.009} & \texttt{5,508 / 16,384} & \texttt{19.7 / 100} \\
5 & v4.0.1 & qwen3-coder & \texttt{0.374} & \texttt{0.042} & \texttt{6,979 / 16,384} & \texttt{26.5 / 100} \\
6 & v3.2.0 & gemini-2.5-pro-preview & \texttt{0.370} & \texttt{0.034} & \texttt{6,008 / 16,384} & \texttt{13.6 / 100} \\
7 & v3.3.0 & gemini-2.5-flash & \texttt{0.363} & \texttt{0.022} & \texttt{4,337 / 16,384} & \texttt{13.2 / 100} \\
8 & v3.2.0 & gemini-2.5-flash-preview-05-20 & \texttt{0.362} & \texttt{0.000} & \texttt{4,547 / 16,384} & \texttt{10.1 / 100} \\
9 & v3.2.0 & gpt-4.1-mini & \texttt{0.350} & \texttt{0.017} & \texttt{7,403 / 16,384} & \texttt{29.7 / 100} \\
10 & v3.2.0 & deepseek-chat & \texttt{0.336} & \texttt{0.011} & \texttt{3,297 / 16,384} & \texttt{7.5 / 100} \\
11 & v4.0.1 & glm-4.5 & \texttt{0.323} & \texttt{0.107} & \texttt{12,477 / 16,384} & \texttt{28.7 / 100} \\
12 & v3.2.0 & qwen-2.5-72b-instruct & \texttt{0.272} & \texttt{0.000} & \texttt{5,873 / 16,384} & \texttt{35.1 / 100} \\
13 & v3.2.0 & qwen3-32b & \texttt{0.255} & \texttt{0.000} & \texttt{5,281 / 16,384} & \texttt{28.3 / 100} \\
14 & v3.2.0 & llama-4-maverick & \texttt{0.255} & \texttt{0.000} & \texttt{4,647 / 16,384} & \texttt{10.4 / 100} \\
15 & v3.2.0 & qwen3-8b & \texttt{0.190} & \texttt{0.000} & \texttt{8,704 / 16,384} & \texttt{56.5 / 100} \\
16 & v3.2.0 & gpt-4.1-nano & \texttt{0.188} & \texttt{0.000} & \texttt{8,536 / 16,384} & \texttt{33.1 / 100} \\
17 & v3.2.0 & qwen3-14b & \texttt{0.176} & \texttt{0.000} & \texttt{10,800 / 16,384} & \texttt{82.6 / 100} \\
18 & v3.2.0 & devstral-small & \texttt{0.092} & \texttt{0.000} & \texttt{14,603 / 16,384} & \texttt{13.0 / 100} \\
\bottomrule
\end{tabular}
\end{table}

\chapter{Reproducibility and Contribution} \label{app:reproducibility}

All training recipes, infrastructure code, and evaluation pipelines are released as open-source software with accessible documentation and straightforward setup procedures.
The main repository at \url{https://github.com/ASSERT-KTH/CodeRepairRL} contains complete training scripts, \ac{SLURM} job templates, and episode orchestration logic.
This thesis repository at \url{https://github.com/BjarniHaukur/CodeRepairRL-Paper} implements all analysis and plotting scripts using the Weights \& Biases API to access logged experimental data and generate figures.

The Nano agent implementation is available at \url{https://github.com/ASSERT-KTH/nano-agent}, providing the minimal terminal-based scaffold used throughout all experiments.
Our TRL fork at \url{https://github.com/ASSERT-KTH/trl/tree/main} extends the upstream library with modifications for multi-turn agent trajectories, \ac{GSPO} support, and asynchronous live weight synchronization with vLLM extensions and \ac{NCCL} channels.
The \ac{GSPO} kernel implementation contributed to Liger-Kernel is documented at \url{https://github.com/linkedin/Liger-Kernel/pull/845}.

\chapter{Use of AI Assistance}
\label{app:ai-assistance}

\ac{AI} language models, primarily Anthropic's Claude Sonnet (3.5, 4 and 4.5), OpenAI's o3, and GPT-5, were used extensively throughout this research as coding assistants, writing aids, and research tools.
\ac{AI} assistance included writing and debugging implementation code, improving prose clarity and academic style, organizing research notes, researching background literature and technical documentation, and formatting LaTeX markup.
All research directions, experimental designs, and scientific interpretations originated with me.
