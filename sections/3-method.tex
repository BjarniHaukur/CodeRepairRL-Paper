\chapter{Methodology}
\label{ch:method}

This chapter presents an execution-free \acs{RL} methodology for repository-level \acs{APR}.
The approach integrates a minimalist terminal agent (Nano) directly into the training loop, using terminal patch-similarity rewards computed deterministically on canonical diffs to enable language-agnostic learning without executable test harnesses.
We first formalize the Nano agent specification (observation space, action space, termination conditions, reward function), then detail training data, policy optimization, and infrastructure requirements.
Evaluation protocols are summarized in \S\ref{sec:eval-brief} and expanded with complete results in Chapter~\ref{ch:results}.

\section{Overview and Scope}

We study whether online \ac{RL} improves a single minimalist coding agent when grounded in real repositories.
The methodology is programming-language agnostic by design: reward depends only on the canonical diff computed from the repository state after the agent's actions, compared against a ground-truth patch.
This removes the need for language-specific runners during training and permits controlled data mixing across languages when ground-truth patches exist.
All results and ablations assume a single-agent paradigm.

\section{Nano Agent}
\label{sec:nano-agent}

We formalize the Nano agent through its observation space, action space, interaction dynamics, termination conditions, and safety constraints.
This specification provides the foundation for the \ac{RL} training methodology described subsequently.

\begin{figure}[H]
	\centering
\includegraphics[width=0.3\textwidth]{plotting/figures/nano_blank.png}
\end{figure}

\subsection{Observations}

All interaction occurs through a terminal transcript maintained as a structured conversation history.
The agent observes system messages, issue descriptions, tool outputs, and error messages as they appear in the dialogue.
Each tool invocation returns at most 2{,}000 characters; outputs exceeding this limit are truncated deterministically with an explicit ``\ldots{} output truncated \ldots{}'' marker appended.
The agent must discover all repository information through its own tool invocations; no file tree summaries, test results, or repository statistics are provided beyond what appears in the visible transcript.

\subsection{Action Space}

The agent has two tools: (i) \texttt{shell(cmd)} executes within a restricted bash (rbash) with a per-call timeout; and (ii) \texttt{apply\_patch(path, old\_content, new\_content)} performs a literal search-and-replace operation without regex.
Patches must target files within the current repository; the specified \texttt{old\_content} must match uniquely in the file at the time of application.

A ``null action'' terminates the episode when the agent produces no further tool calls: if repository changes exist, this constitutes a submission; if no changes exist, the agent is prompted to continue working until changes are made or limits are reached.

\subsection{Formal Interaction Loop}

The agent operates through a structured cycle that formalizes the relationship between observations, actions, and environmental feedback.
At each timestep $t$, the agent maintains a conversation history $h_t$ comprising the system message, issue description, and all prior agent-environment interactions.
Given this history, the model generates an assistant response $y_t$ containing tool invocations expressed as \ac{JSON} function calls.
The inference endpoint extracts the first tool call through a deterministic parser $c_t = \psi(y_t)$, executes it within the isolated repository environment, and returns the resulting output $o_t$—command results, file contents, or error messages—subject to the 2{,}000-character truncation policy.

The history advances to $h_{t+1} = \text{append}(h_t, y_t, o_t)$, incorporating both the assistant's generation and the environment's response.
This cycle continues until a termination condition is met: the agent submits by producing repository changes without further tool calls, the tool-call budget is exhausted, the token limit is exceeded, or the wall-clock timeout is reached.
Upon termination, the episode yields a complete trajectory $\tau = (h_0, y_0, o_0, h_1, \ldots, h_T)$ where $T$ denotes the final timestep.

\begin{figure}[H]
    \centering
\begin{minted}{python}
>>> shell(cmd="ls src/")
drwxr-xr-x  utils/
-rw-r--r--  main.py
-rw-r--r--  config.py

>>> shell(cmd="grep -n 'def process' src/utils.py")
42:def process_data(data):
43:    return data.strip().lower()

>>> apply_patch(
...     file_path="src/utils.py",
...     old_content="return data.strip().lower()",
...     new_content="return data.strip().lower().replace(' ', '_')"
... )
Patch applied successfully.
\end{minted}
\caption{Illustrative rollout of the Nano agent. Strings in red are supplied by the \ac{LLM} as \ac{JSON} tool call arguments. The agent explores repository structure and file contents before applying a targeted edit.}
\label{fig:nano-tools}
\end{figure}

\subsection{Termination and Limits}

Episodes terminate when any of the following conditions is met: (i) the agent produces repository changes and emits no further tool calls (successful submission), (ii) the tool-call budget is exhausted, (iii) the token budget is exceeded, or (iv) the wall-clock timeout is reached.
These limits keep \ac{VRAM} utilization bounded and improve sample efficiency: the agent receives warnings as it approaches limits, encouraging it to produce a submission rather than simply timing out.
Exact values appear in \Cref{tab:termination-parameters}.

\subsection{Sidestepping the Diff Generation Problem}

Generating syntactically valid unified diffs presents a high-dimensional formatting challenge for language models: line numbers must accurately reflect current file state, context lines must match existing content exactly character-for-character, and headers must specify correct paths, offsets, and chunk sizes.
State-of-the-art models frequently produce malformed diffs with misaligned line numbers, incorrect context, or invalid headers that fail during patch application, introducing a brittle failure mode orthogonal to the semantic debugging task.

The Nano agent sidesteps this issue by using semantically clear search-and-replace operations during interaction and deterministically computing the canonical diff via git after termination, effectively eliminating an entire class of diff formatting errors.

\begin{figure}[H]
\centering
\begin{minted}{bash}
$ git diff
diff --git a/src/utils.py b/src/utils.py
index abc123..def456 100644
--- a/src/utils.py
+++ b/src/utils.py
@@ -42,1 +42,1 @@ def process_data(data):
-    return data.strip().lower()
+    return data.strip().lower().replace(' ', '_')
\end{minted}
\caption{The harness computes the canonical diff via git after episode termination which is subsequently used to compute the reward.}
\label{fig:git-diff}
\end{figure}

\section{Training Data}
\label{sec:data-env}

Training employs a 1{,}000-task curriculum combining Python and multilingual data.
SWE-Gym~\cite{sweGym2025} provides approximately 2{,}400 Python bug-fixing tasks extracted from real GitHub repositories; we select 750 of these tasks for the training curriculum.
SWE-Bench-Multilingual~\cite{sweBenchMultilingual2025} offers debugging tasks spanning nine programming languages: Rust, Java, PHP, Ruby, JavaScript, TypeScript, Go, C, and C++.
From this collection, we incorporate 250 tasks into training and reserve 50 as a held-out evaluation set.
Instruction-driven, repository-level multilingual debugging datasets remain substantially scarcer than Python-only resources, constraining the scale of cross-language training currently feasible in the field.

The execution-free training methodology relies solely on ground-truth patches as terminal supervision signals, enabling language-agnostic training through patch-similarity rewards.
This approach permits controlled mixing of programming languages without maintaining language-specific test infrastructure or execution environments.

\section{Reward Design}
\label{sec:reward}

The terminal reward $R(\tau) \in [0,1]$ evaluates the trajectory based on similarity between the canonical diff resulting from the agent's repository modifications and the ground-truth patch.

After an episode terminates, we compute per-file canonical diff hunks for the agent and the ground truth.
Let $F_a$ and $F_g$ denote the sets of files modified by the agent and in the ground-truth patch, and let $p_a(f)$ and $p_g(f)$ denote the canonical diff hunk strings for file $f$.
We aggregate per-file similarity and normalize by the greater number of affected files: \begin{equation}
R(\tau) = \frac{1}{\max\big(|F_a|,\,|F_g|\big)} \sum_{f \in F_a \cup F_g} \operatorname{similarity}\!\big(p_a(f),\, p_g(f)\big) \in [0,1].
\end{equation} Here $\operatorname{similarity}(\cdot,\cdot)$ is the string-similarity score of the canonical diff hunks computed with Python's \texttt{difflib.SequenceMatcher.ratio()}.
Files that appear in only the set of agent affected files or ground truth affected files contribute zero.

This design keeps training simple, avoids brittle intermediate reward engineering, and permits diverse problem-solving trajectories while targeting the ultimate objective directly: producing the correct changes to the repository.

\section{Policy Optimization} \label{sec:rl}

We adopt \ac{GSPO}~\cite{gspo2025} as our policy optimization algorithm.
\ac{GSPO} extends the group-relative baseline principle from \ac{GRPO} with sequence-level importance weighting, offering improved stability over earlier group-relative methods.
This stability is essential for long-context, multi-turn coding agent training where episodes vary substantially in length and structure.

Our training objective combines \ac{GSPO} with a modest \ac{KL} penalty to counteract gradient variance from limited batch sizes: \begin{equation}
J(\theta) = J_{\mathrm{GSPO}}(\theta) - \beta_{\text{KL}} \cdot D_{\text{KL}}(\pi_\theta \| \pi_{\theta_{\text{ref}}}),
\end{equation} where the \ac{KL} divergence is computed on the logits of the rollouts used for training, $\beta_{\text{KL}}$ controls regularization strength, and $\pi_{\theta_{\text{ref}}}$ is the initial pre-trained checkpoint.

\section{Model Choice}
\label{sec:models}

We base our experiments on Qwen3-14B, a hybrid reasoning model combining strong coding capabilities with native tool-calling support.
This selection stems from superior tool-calling performance and competitive performance on coding benchmarks, making it well-suited for multi-turn agent applications.
Initial experiments included Llama3.1-8B, but it was excluded from the study after demonstrating insuficcient performance to facilitate learning.
Limited comparisons across the Qwen3 series (8B, 14B, 30B-A3B, 32B) appear in Appendix~\ref{app:model-comparison}.

\section{Evaluation Protocol}
\label{sec:eval-brief}

Our evaluation protocol addresses the three research questions established in Chapter~\ref{ch:introduction}.

\textbf{RQ1: How does \ac{GSPO} training improve Nano harness adaptation?}
We measure tool-call success rate, invalid-call reduction, action efficiency, and command usage evolution, comparing pre-training and post-training behavior.
Command usage patterns are analyzed qualitatively across training runs to identify shifts in debugging strategies.

\textbf{RQ2: Does execution-free patch-similarity \ac{RL} training improve SWE-Bench-Verified performance?}
We measure test-verified success rates on SWE-Bench-Verified, comparing pre-training baseline to post-training performance using identical evaluation protocol.

\textbf{RQ3: Does execution-free \ac{RL} enable effective multilingual training without language-specific engineering?}
We measure per-language reward improvements on the 50-task SWE-Bench-Multilingual held-out set spanning nine programming languages, comparing pre-training and post-training checkpoints with bootstrap confidence intervals.

Baselines consist of the Qwen3-14B checkpoint with Nano before \ac{RL} training.
