\chapter{Method}
\label{ch:method}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=1.0\textwidth]{plotting/figures/diagram.png}
	\caption{Simultaneously too busy and too simplified, I'll make another version}
	\label{fig:method-diagram}
\end{figure}

This chapter presents our novel approach to training large language models for automated code repair through online \ac{RL}.
We detail the experimental design, technical implementation, and evaluation methodology used to investigate whether embedding coding agents directly into the \ac{RL} training loop improves bug-fixing performance.

\todoinline{Add a concise informal agentic setup: define observations (repository-local context and tool I/O), actions (tool calls and termination), and episode-level, execution-free reward.}

\section{Overview of our approach}
\label{sec:method-overview}

Traditional approaches to training \acp{LLM} for code repair rely on \ac{SFT} with static datasets of code-patch pairs.
In contrast, our method pioneers online \ac{RL}, where coding agents actively interact with real software repositories during training.
This paradigm shift transforms models from passive learners observing fixed examples into active agents that learn through environmental interaction and experiential feedback.
% We retain the same outcome-driven similarity principle as SWE-RL~\cite{wei2025swerladvancingllmreasoning} while embedding it within a multi-step setting that rewards successful end-to-end repair after autonomous context acquisition.

The core innovation lies in our integration of existing coding agent frameworks directly into the \ac{RL} training pipeline.
Rather than constraining models to single-pass generation, we enable multi-step interactions where agents can navigate repository structures using terminal commands, examine multiple files to understand code context, iteratively refine solutions based on environmental feedback, and learn from the outcomes of their actions rather than merely imitating examples.

Unlike previous approaches that relied on synchronous batched generation, our implementation provides an asynchronous OpenAI-compatible \ac{API} server that enables true multi-turn interaction rather than single-pass generation.

% Our Nano coding agent can therefore engage in natural back-and-forth interaction with repositories through basic terminal commands while remaining fully compatible with the RL training loop.

% Figure~\ref{fig:agent-loop-architecture} illustrates the complete training architecture, highlighting the continuous feedback cycle between language model policy, agent implementation, repository environment, and reward computation.
% This diagram demonstrates how traditional \ac{RL} training is enhanced by embedding coding agents directly within the optimization loop.

\section{Nano Coding Agent}
\label{sec:nano-agent}
\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.6\textwidth]{plotting/figures/nano_blank.png}
	\label{fig:nano-sprite}
\end{figure}

Our Nano coding agent adopts a minimalist design: rather than engineering sophisticated tools and guided workflows, we provide only essential capabilities and allow effective behaviors to emerge through \ac{RL}.
This isolates the contribution of learning under a simple action space and avoids confounds introduced by heavyweight scaffolding.

\subsection{Decoding and Exploration Policy}
\label{subsec:decoding-exploration}

The temperature parameter in language model sampling directly controls the exploration-exploitation trade-off during inference.
Higher temperatures increase randomness and exploration of diverse solutions, while lower temperatures favor exploitation of high-probability actions.
Critically, deterministic sampling (temperature=0) prevents any exploration, making it unsuitable for reinforcement learning where the agent must discover new strategies through trial and error.
During \ac{RL} training, we typically use temperature values between 0.7 and 1.0 to balance exploration of novel debugging approaches with exploitation of learned patterns (see \cref{app:sampling-params} for detailed sampling parameters).

\subsection{Tool Interface}

Nano agent has access to two streamlined tools for terminal based repository interaction:

\textbf{Shell Command Execution} (\texttt{shell(cmd)}): This tool allows the agent to execute unix commands within a restricted bash environment (rbash), such as exploration commands like \texttt{ls}, \texttt{rg}, \texttt{grep}, and \texttt{find}, as well as content inspection commands such as \texttt{cat}, \texttt{head}, \texttt{tail}, and \texttt{sed}.

\textbf{File Patching} (\texttt{apply\_patch}): This tool enables precise code modifications through a search-and-replace mechanism.
The agent specifies the target file path where changes should be made, the exact text that needs to be replaced (old\_content), the new replacement text (new\_content).

This minimal toolset encourages models to develop their own strategies for code understanding and modification.

\subsection{Sidestepping the Diff Generation Problem}

A critical design decision in the Nano agent architecture demonstrates how thoughtful tool design can eliminate entire classes of errors.
Generating valid unified diffs represents one of the most challenging output formatting tasks for language models.
The unified diff format requires:

\begin{verbatim}
--- a/src/utils.py
+++ b/src/utils.py
@@ -45,7 +45,7 @@ class DataProcessor:
     def process(self, data):
         if not data:
             return None
-        return data.strip().lower()
+        return data.strip().lower().replace(' ', '_')
     
     def validate(self, data):
         return len(data) > 0
\end{verbatim}

This format demands precise coordination across multiple dimensions.
The line numbers must accurately reflect the current file state, while context lines need to exactly match the existing content.
The diff must properly handle whitespace, indentation, and special characters throughout.
Additionally, it requires consistent header formatting with correct file paths and accurate chunk headers containing proper line counts.
This intricate coordination makes diff generation particularly challenging for language models.

Even state-of-the-art models frequently produce malformed diffs with misaligned line numbers, incorrect context, or formatting errors that prevent patch application.
\todoinline{Back this up}

The Nano agent completely sidesteps this challenge through an elegant architectural choice.
Instead of requiring diff generation, it provides a simple \texttt{apply\_patch} interface:

\begin{verbatim}
apply_patch(
    file_path="src/utils.py",
    old_content="return data.strip().lower()",
    new_content="return data.strip().lower().replace(' ', '_')"
)
\end{verbatim}

This format proves naturally conducive to language model generation for several reasons.
The model can specify changes in semantically clear terms without requiring numerical coordination such as line numbers or offsets.
String matching provides robust handling of minor formatting variations, while the explicit specification of old and new content makes the transformation intent unambiguous.

After the model applies changes using this simple interface, the actual diff is computed using \texttt{git diff}.
This architectural choice transforms what is typically a major source of technical failures into a reliable, automated process.

\subsection{Safety and Resource Constraints}

\todoinline{This will always look bad...
We can say we attempt "safety" with rbash, readonly access and timeouts}

\subsection{Integration with RL Training}

\begin{itemize}
	\item \textbf{History} $h_t$: The conversation transcript containing the issue description and all prior tool interactions
	\item \textbf{Assistant chunk} $y_t$: Model output containing \texttt{shell} commands or \texttt{apply\_patch} operations
	\item \textbf{Tool call} $c_t$: The parsed command extracted by $\psi(y_t)$ 
	\item \textbf{Tool output} $o_t$: Command results, file contents, or error messages
	\item \textbf{Return} $R(\tau)$: Terminal reward based on patch similarity to ground truth
\end{itemize}

The agent operates through the following cycle: \begin{enumerate}
	\item Receives initial history $h_0$ containing the issue description
	\item Generates assistant chunk $y_t$ with tool calls to explore the codebase
	\item Executes first tool call $c_t = \psi(y_t)$ to produce output $o_t$
	\item Updates history $h_{t+1} = \text{append}(h_t, y_t, o_t)$
	\item Continues until patch submission or resource limits
	\item Receives terminal return $R(h_T)$ based on patch quality
\end{enumerate}

This cycle allows the model to learn from both successful and unsuccessful repair attempts, gradually improving its ability to navigate codebases and identify correct fixes.
% Security is paramount when allowing language models to execute arbitrary commands.% The Nano agent employs several safety mechanisms:% \textbf{Restricted Bash (rbash)}: All shell commands execute within a restricted bash environment that prevents:% \begin{itemize}% 	\item Network access and external communication% 	\item File system access outside the designated workspace% 	\item Process spawning beyond allowed utilities% 	\item Modification of system files or configurations% \end{itemize}
The model applies an arbitrary sequence of apply\_patch operations in simplified search-replace format, then after task completion we compute the final diff instead of requiring the model to generate properly formatted patches directly.

This learning-centric approach avoids overfitting to tooling idiosyncrasies.
Rather than engineering away the difficulty of code repair through sophisticated tooling, we provide a stable, minimal interface and train the model to develop its own solutions through experience.

\subsection{Compute-Aware Agent Design}

Nano is designed to be compute-aware and operates within well-defined token, tool and time budgets.
This compute-aware design ensures predictable resource utilization across multiple dimensions.
Token limits guarantee that conversation histories remain within \ac{GPU} memory constraints during both training and inference, while tool call limits prevent runaway episodes that could consume excessive computational resources.
Additionally, timeout mechanisms provide an upper bound on total time allocation per episode.

With the agent architecture established, we now turn to the experimental framework that enables systematic evaluation of our approach.

\section{Experimental Setup}\label{sec:experimental-setup}

\subsection{Datasets and Benchmarks}

Our experimental design carefully separates training and evaluation data to demonstrate true generalization rather than memorization.
We employ the following datasets:

\textbf{Training Dataset:} We use SWE-Gym, a curated collection of approximately 2,400 Python bug-fixing tasks derived from real GitHub repositories.
Each task provides a snapshot of the repository at the time of the bug report, a problem statement describing the bug, and the ground-truth patch that resolved the issue.

SWE-Gym's containerized design makes it ideal for \ac{RL} training, as agents can freely explore and modify code without risk while receiving deterministic feedback based on their actions.

\textbf{Primary Evaluation:}
SWE-Bench-Verified serves as our main evaluation benchmark, containing approximately 500 carefully validated Python bugs from popular open-source projects.
These bugs are notably more challenging than the training set, often requiring multi-file modifications and deep understanding of project structure.
This dataset tests whether our agent-trained models can generalize beyond their training distribution.

\textbf{Generalization Testing:}
To assess transfer of learned repair skills beyond the training domain, we evaluate on HumanEval, a standard code generation benchmark.
This tests whether debugging strategies learned through online \ac{RL} improve general programming capabilities.

\subsection{Multilingual Training Protocol}

Our multilingual training protocol explores whether execution-free training enables effective cross-language debugging without language-specific test infrastructure.
We employ a data mixing strategy using the multilingual debugging datasets available in the open-source community.

\subsubsection{Dataset Composition}

We construct our training dataset from available multilingual resources: \begin{itemize}
	\item \textbf{Python}: 700 samples from SWE-Gym
	\item \textbf{Other languages}: 300 samples from SWE-Bench-Multilingual
	\item Languages represented: Rust, Java, PHP, Ruby, JavaScript, TypeScript, Go, C, and C++
\end{itemize}

This dataset composition provides exposure to a diverse range of programming paradigms and syntactic structures while maintaining sufficient samples for stable training convergence.

\subsubsection{Training Configurations}

To rigorously evaluate the impact of multilingual training, we implement two configurations:

\textbf{Baseline (Python-only)}: Training exclusively on 2,500 Python samples provides a strong monolingual baseline against which to measure the impact of language diversity.

\textbf{Multilingual}: Training on the mixed dataset of 1,300 samples (1,000 Python + 300 multilingual) tests whether exposure to multiple languages improves or degrades debugging performance.

Both configurations train for multiple epochs until convergence, with identical hyperparameters to ensure fair comparison.
The key experimental question is whether the multilingual configuration's exposure to diverse syntax and programming patterns enhances the model's ability to identify universal debugging principles.

\subsubsection{Evaluation Strategy}

Our evaluation protocol assesses both within-language and cross-language generalization: \begin{itemize}
	\item \textbf{Primary metrics}: Performance on Python holdout sets (does multilingual training affect majority language performance?)
	\item \textbf{Transfer metrics}: Performance on non-Python languages in both seen (Java, JavaScript) and unseen contexts
	\item \textbf{Zero-shot evaluation}: Testing on GitBug-Java and other monolingual benchmarks not seen during training
\end{itemize}

\todoinline{Write about evaluations on GitBug-Java, SWE-Bench, and TerminalBench (possibly etc.)
}

\todoinline{Consider adding: Partial observability challenge - repositories have thousands of files but LLM context windows are limited.
Context management as learned skill.
Cross-benchmark evaluation prevents overfitting - train on SWE-Gym, test on SWE-Bench.
}

This comprehensive evaluation enables us to determine whether execution-free multilingual training achieves its promise of universal debugging capability or introduces problematic interference between languages.

\subsection{Pretrained Model Selection: Qwen for Tool Calling Excellence}

\section{Pretrained Language Models}
\label{sec:pretrained-llms}

% We treat instruction-tuned code \acp{LLM} as pretrained policies that already exhibit reliable structured output (e.g., tool-call formatting), long-context handling, and baseline repository reasoning.
% Two efficiency profiles are particularly relevant for online \ac{RL} with coding agents: dense Transformers activate all parameters per token (simple and stable), whereas sparse Mixture-of-Experts (MoE) models route each token to a small subset of experts, decoupling total parameter count from per-token compute and memory.
% This makes MoE attractive for multi-turn, long-context training and evaluation.
% Parameter-efficient adaptation (e.g., \ac{LoRA}) complements both families by keeping trainable state small and enabling clean policy-reference separation.
% These properties matter for agent training dynamics without requiring architectural specificity in this thesis.

\todoinline{rework, integrate MoE, the 2-3 different models we are targetting etc.}
We base our experiments on the Qwen2.5-Coder family, specifically targeting both 7B and 32B parameter variants.
Qwen was selected based on extensive empirical evidence from recent literature demonstrating its superior tool-calling capabilities compared to other open-source alternatives.

\subsubsection{Tool Calling Performance}

Qwen models exhibit exceptional structured output generation, which proves critical for reliable agent behavior.
These models consistently generate valid \ac{JSON} function calls with correct parameter types while maintaining superior context retention across multi-step tool interactions.
They demonstrate robust error recovery through effective handling of tool execution failures and adaptive strategy adjustment, coupled with reliable adherence to function signatures and parameter constraints.

\subsubsection{Code Understanding Capabilities}

Beyond tool calling, Qwen demonstrates strong foundational coding abilities through state-of-the-art performance on HumanEval, MBPP, and CodeContests benchmarks.
The model exhibits superior Python code generation capabilities, excellent repository-level understanding with cross-file reasoning, and strong performance on debugging and code repair tasks in baseline evaluations.

The model undergoes continued training rather than traditional fine-tuning, preserving its general capabilities while acquiring specialized agent skills through \ac{RL}.
This approach maintains Qwen's robust tool-calling foundation while enabling learning of complex debugging behaviors.

\section{Training-Inference Duality in Online RL for LLMs}
\label{sec:training-inference}

A fundamental innovation in our approach is the collapse of the traditional training-inference boundary.
Unlike conventional \ac{RL} where trajectory collection and policy updates occur in separate phases, our system performs both simultaneously through continuous serving and real-time weight updates.

\subsection{The Unified Training-Inference Paradigm}

The \ac{GRPO} process operates as a continuous cycle where: \begin{enumerate}
	\item \textbf{Live inference}: vLLM serves the current policy to multiple agent workers simultaneously
	\item \textbf{Trajectory streaming}: Completed agent episodes stream to the training pipeline in real-time
	\item \textbf{Immediate updates}: \ac{GRPO} computes gradients and updates weights continuously
	\item \textbf{Live synchronization}: Updated weights propagate to the inference server without interruption
\end{enumerate}

This unified paradigm enables true online learning where the policy improves continuously throughout the training session, rather than in discrete update cycles.

\section{Trainer Implementation}
\label{sec:rl-algorithm}

\todoinline{Write short explanation of TRL choice: why we selected TRL over alternatives like rLLM/verl, what extensions we made for multi-turn agents, integration with HuggingFace ecosystem}

Our trainer implementation builds upon the Transformers Reinforcement Learning (TRL) library~\cite{vonwerra2022trl}, which provides foundational infrastructure for applying reinforcement learning to language models.
We extend this framework with custom components specifically designed for multi-turn agent interactions and online training dynamics.

\subsection{\ac{GRPO}}

\ac{GRPO} offers several key advantages over traditional actor-critic methods.
Unlike conventional approaches, \ac{GRPO} estimates advantages using relative performance within a batch, eliminating the need for a separate value network.
This design choice reduces variance by normalizing rewards across groups of trajectories, providing more stable training signals.
The absence of value estimation also simplifies the training pipeline, reducing both computational overhead and implementation complexity.

\subsection{Masked Loss Computation for Tool-Augmented RL} \label{sec:masked-loss-computation}

A key design consideration in training tool-augmented language models involves the treatment of externally-generated content during optimization.
Interactive agents must process and reason about tool outputs (shell command results, file contents, search results) to maintain coherent conversations, yet including these external tokens in the loss computation shifts optimization pressure away from the primary learning objective: making appropriate tool calls based on context.

Consider an agent trajectory $\tau = (s_1, a_1, o_1, s_2, a_2, o_2, \ldots, s_T)$ where $a_t$ represents agent actions (tool calls) and $o_t$ represents corresponding tool outputs.
While tool outputs are partially predictable given sufficient context, training the model to predict them deemphasizes the critical skill of contextual tool selection and invocation.
Traditional sequence modeling treats the entire trajectory as a prediction target, diluting the learning signal for agent reasoning and tool usage patterns.

\subsubsection{Dual-Mask Strategy for Loss Isolation}

We introduce a dual-masking approach that preserves contextual information during forward computation while focusing optimization on agent-authored content.
Let $\mathbf{x} = [x_1, x_2, \ldots, x_n]$ represent a tokenized trajectory and $\mathcal{T} \subset \{1, 2, \ldots, n\}$ denote the indices corresponding to tool-generated tokens.

During the forward pass, the model processes the complete sequence with full attention using a complete attention mask $\mathbf{M}_{\text{att}} = \mathbf{1}_{n \times n}$, where the transformer processes the input sequence $\mathbf{x}$ to produce hidden representations $\mathbf{h} = \text{Transformer}(\mathbf{x}, \mathbf{M}_{\text{att}})$ and final logits $\mathbf{L} = \text{LinearHead}(\mathbf{h})$.
Crucially, when computing log probabilities for agent-authored tokens, the model maintains full access to tool response content through the attention mechanism, ensuring that contextual information remains available for reasoning.

For loss computation, we apply selective masking to focus optimization exclusively on agent-authored tokens: \begin{align}
	\mathbf{M}_{\text{loss}}[i] & = \begin{cases}
		                                1 & \text{if } i \notin \mathcal{T} \text{ (agent-authored)} \\
		                                0 & \text{if } i \in \mathcal{T} \text{ (tool-generated)}
	                                \end{cases}                                                         \\
	\mathcal{L}_{\text{GRPO}}   & = \frac{\sum_{i=1}^{n} \mathbf{M}_{\text{loss}}[i] \cdot \ell_i \cdot A_i}{\sum_{i=1}^{n} \mathbf{M}_{\text{loss}}[i]}
\end{align}

where $\ell_i$ represents the per-token policy loss and $A_i$ denotes advantages computed via \ac{GRPO}.

\subsubsection{Perplexity Analysis and Training Focus}

Figure~\ref{fig:tool-perplexity-analysis} demonstrates the importance of masked loss computation through perplexity decomposition across agent trajectories.
Tool-generated tokens consistently exhibit higher perplexity as the model's predicted distributions often diverge from actual tool outputs—reflecting the difficulty of precisely predicting external system responses without complete environmental state.

% TODO: Create figure showing perplexity breakdown
\begin{figure}[ht]
	\centering
	\fbox{\parbox{0.8\textwidth}{\centering
			\textit{[Figure placeholder: Perplexity decomposition across agent trajectories showing tool-generated tokens contribute disproportionately to sequence-level perplexity.
						Left panel: trajectory timeline with color-coded regions (agent reasoning, tool calls, tool outputs).
						Right panel: per-token perplexity highlighting the spike at tool response boundaries where model predictions diverge from external content that, while partially predictable, is not the target learning objective.
					]}
		}}
	\caption{Perplexity analysis revealing the dominance of tool-generated tokens in sequence-level prediction difficulty.
		The majority of perplexity arises from tool responses, which while not entirely unpredictable, represent a different learning objective than the desired focus on contextual tool selection and agent reasoning.
	}
	\label{fig:tool-perplexity-analysis}
\end{figure}

Without selective masking, the optimization objective allocates significant capacity to predicting tool outputs rather than focusing on the primary training goal: learning to make appropriate tool calls and reason effectively about their results.

This dual-mask strategy provides several algorithmic benefits that directly address the challenges of training tool-augmented language models.
By zeroing out tool response contributions to the gradient, we target optimization exclusively on the tokens that matter most for agent development.
Tool tokens contribute zero gradient magnitude, ensuring optimization pressure concentrates on agent reasoning and tool invocation patterns rather than external content prediction.
Simultaneously, the forward pass maintains full attention over tool outputs, enabling coherent reasoning about external information while directing learning toward appropriate tool usage.
The masking approach integrates seamlessly with existing kernel implementations, avoiding computational overhead, and ensures that as tool outputs grow in length and complexity, training remains focused on core agent capabilities rather than becoming diluted across content prediction tasks.

This masked loss formulation represents an important design choice for tool-augmented \ac{RL}, enabling models to leverage external information for reasoning while maintaining clear optimization objectives focused on contextual tool selection and effective agent behavior.

Building on these foundational training principles, we now detail the complete training process that transforms static language models into dynamic debugging agents.

\subsection{Training Process}

Our training follows an iterative process where the agent attempts to solve bugs from the SWE-Gym dataset through four key stages.
Initially, trajectory generation occurs where the agent produces complete repair attempts from initial exploration through final patch submission for each bug in a training batch.
Subsequently, reward computation assigns each trajectory a score based on the similarity between the generated patch and the ground-truth solution.
The advantage estimation phase then employs \ac{GRPO} to compute advantages by comparing each trajectory's reward to the mean reward within its group.
Finally, policy updates modify model parameters to increase the likelihood of high-advantage actions while maintaining proximity to the reference policy through KL regularization.

\subsection{Hyperparameters}

Our \ac{GRPO} implementation employs carefully selected hyperparameters optimized for coding agent training.
We utilize a group size of 8 trajectories with gradients accumulated over 4 groups for an effective batch size that balances training stability with computational efficiency.
The learning rate is set to $1 \times 10^{-4}$, which is elevated due to \ac{LoRA}'s parameter efficiency, and maintained constant without decay to preserve the online nature of our training approach.
Stability is ensured through a KL coefficient of $\beta = 0.2$, as omitting KL divergence constraints often leads to model collapse in our experiments.
The maximum trajectory length is constrained to 12,288 tokens, representing an optimal balance that maximizes \ac{GPU} throughput while accommodating the multi-turn nature of debugging interactions.
Training spans 5,000 episodes, providing comprehensive coverage of the SWE-Gym dataset.

\todoinline{Consider adding: Reward normalization equation for proper credit assignment.
Sparse reward handling through GRPO's group-relative advantages.
Compositional action space challenge - infinite possible code modifications vs discrete game actions.
}

These values were selected through preliminary experiments to ensure stable convergence while allowing sufficient exploration of the action space.

\subsection{Large-Scale Compute Management}

Training and serving 8B to 32B parameter models for online \ac{RL} requires sophisticated compute optimization across the entire pipeline.

\subsubsection{Training Optimizations}

\textbf{LoRA:}
\todoinline{Move discussions on what LoRA buys us efficiency-wise here, discuss them tersely}

\paragraph{Note on LoRA in KL-controlled training.}
When KL control to a reference policy is used, sharing the frozen $W_0$ and differing only in adapters avoids duplicating full parameters and simplifies bookkeeping; we do not employ an explicit KL term in this work but note the general advantage.

\textbf{FlashAttention 2:} \cite{dao2023flashattention2fasterattentionbetter}
3 only available on >H100

\textbf{DeepSpeed ZeRO Integration:}
\todoinline{briefly discuss both stage 2 and 3}
Complete model parameter, gradient, and optimizer state sharding enables training of 32B models across multiple \acp{GPU} with minimal memory overhead per device.

\textbf{Gradient Checkpointing:}
Selective activation checkpointing reduces memory usage by 50\% during backward passes, trading compute for memory to enable larger batch sizes and longer sequence lengths.

\textbf{Mixed Precision Training:}
BF16 training.

\textbf{\ac{KL}
Divergence Computational Efficiency:} \ac{GRPO} training often requires computing \ac{KL} divergence between the current policy and a reference model to ensure training stability—preventing the model from deviating excessively from its original training objectives while optimizing for the new task.
This stability constraint typically necessitates maintaining two complete model instances in memory to compute $D_{\ac{KL}}(\pi_{\theta}(a|s) \| \pi_{\text{ref}}(a|s))$, effectively doubling \ac{GPU} memory requirements.
\ac{LoRA}'s additive parameter structure provides a natural solution to this computational burden: since base model weights $\mathbf{W}_0$ remain frozen while only low-rank adaptations $\mathbf{W}_0 + \mathbf{BA}$ are trained, the reference policy corresponds exactly to the base model without adapters.
We can therefore compute reference policy outputs by temporarily disabling the \ac{LoRA} adapters on the same model instance, eliminating the need for a separate reference model.
This approach reduces memory consumption by approximately 50\% while preserving the distributional constraints essential for stable policy optimization.

\subsubsection{Cost-Performance Trade-offs}

The combination of these optimizations enables training of state-of-the-art agent behaviors while maintaining practical computational budgets:

\todoinline{Make this a table with empirical metrics like: "backprop time", "rollout time"} \begin{itemize}
	\item \textbf{~8B models:} Trainable on 2-3x A100 \acsp{GPU} with DeepSpeed ZeRO 2 and \ac{LoRA}
	\item \textbf{~14B models:} Trainable on 3x A100 \acsp{GPU} with DeepSpeed ZeRO 2 and \ac{LoRA}
	\item \textbf{~32B models:} Trainable on 6x A100 \acsp{GPU} DeepSpeed ZeRO 3 and \ac{LoRA}
\end{itemize}

Stacking these optimizations make \ac{RL} practically feasible for academic research, democratizing access to advanced coding agent training previously available only to industry labs with unlimited compute budgets.

\section{Reward Design}
\label{sec:method-reward-design}

The design of reward functions in reinforcement learning fundamentally shapes the behaviors that agents learn to exhibit.
For coding agents operating in complex software environments, this design choice becomes particularly critical as it must balance computational tractability with meaningful signals that guide learning toward effective debugging strategies.

Our reward design process involved extensive experimentation with various approaches, including execution-based rewards that run test suites, intermediate progress rewards that credit partial solutions, and composite metrics that combine multiple evaluation criteria.
Through systematic evaluation, we converged on patch-similarity rewards as the optimal balance of signal quality, computational efficiency, and alignment with our training objectives.

Crucially, the execution-free nature of patch-similarity rewards enables our key innovation: seamless multilingual training.
Unlike test-based approaches that require language-specific infrastructure (test runners, build systems, package managers), our reward function operates identically across all programming languages.
This design choice transforms multilingual training from an engineering nightmare into a straightforward data mixing problem, enabling us to train universal debugging agents that would be prohibitively complex with execution-based rewards.

\todoinline{Consider adding from deleted background section: Test-based rewards offer correctness guarantees but require maintaining executable environments for each language.
Patch-based rewards may miss semantically equivalent solutions but enable language-agnostic operation.
Reward sparsity challenge - most trajectories fail, GRPO extracts learning signals through group-relative advantages.
}

\subsection{Outcome-Based Patch Similarity}

Our reward function, inspired by the method proposed in SWE-RL~\cite{wei2025swerladvancingllmreasoning}, employs patch similarity as the primary signal for training.
This approach compares the agent-generated diff with the ground truth using Python's SequenceMatcher, which implements a sophisticated sequence matching algorithm based on the Ratcliff-Obershelp pattern recognition method.
The algorithm identifies the longest common subsequences between two sequences and computes a similarity ratio that ranges from 0 (completely different) to 1 (identical patches).

This approach offers several advantages for training coding agents.
Evaluating final patches proves straightforward and deterministic, avoiding the complexity of intermediate reward shaping.
The method provides flexibility by allowing agents to discover diverse problem-solving strategies without being constrained by process-specific rewards.
Most importantly, the reward directly measures what we ultimately care about: correct bug fixes.

\subsection{Multi-Component Reward Computation}
\todoinline{We tried this and found it to behave poorly}
Our reward function decomposes the complex task of bug fixing into three distinct components, each capturing a different aspect of repair quality:

\begin{equation}
	R_{\text{total}} = 0.2 \cdot R_{\text{files}} + 0.4 \cdot R_{\text{functional}} + 0.4 \cdot R_{\text{testing}}
\end{equation}

where each component evaluates a specific dimension of the agent's solution:

The file targeting component ($R_{\text{files}} \in [0,1]$) evaluates whether the agent modified the correct files that should be changed to fix the bug, rewarding the agent for identifying appropriate locations in the codebase regardless of specific changes made.
The functional similarity component ($R_{\text{functional}} \in [0,1]$) measures how similar the agent's changes are to the ground-truth solution in terms of functional bug-fixing aspects, focusing on whether core logic changes align with expected repair strategies.
Finally, the testing alignment component ($R_{\text{testing}} \in [0,1]$) assesses how well the agent's changes align with testing suite additions or modifications in the ground truth, which monitor the bug and ensure correctness while preventing regression.

This decomposition reflects the multi-faceted nature of software debugging: successful repair requires not only identifying the right locations and implementing functional fixes, but also considering how changes interact with the broader testing infrastructure that validates correctness.

This reward computation approach addresses a fundamental challenge in \ac{LLM}-based code modification: the notorious difficulty of generating syntactically valid diffs.
Traditional approaches require models to produce unified diff format with precise line numbering, context coordination, and complex formatting rules—a task that frequently results in parsing errors and invalid patches.

Our approach elegantly separates semantic understanding from syntactic formatting.
During interaction, the agent performs any number of \texttt{apply\_patch} operations using a simplified search-replace format that aligns naturally with \ac{LLM} capabilities.
Each operation specifies exact text to replace and its substitution, without requiring knowledge of line numbers or diff syntax.

Only after the complete interaction do we execute \texttt{git diff} to compute the actual patch for evaluation.
This separation of concerns offers several key advantages in training robust debugging agents.
Models cannot fail due to diff formatting issues since patches are computed automatically, allowing agents to concentrate entirely on identifying and fixing bugs rather than wrestling with syntax requirements.
Git automatically ensures all patches are properly formatted and applicable, while every intended modification is captured accurately for reward computation.

This architectural choice transforms a major source of technical failures into a reliable, automated process while preserving full fidelity of the agent's debugging intentions.

\subsection{Addressing Sparse Rewards}

The sparse nature of exact patch matching presents challenges for RL training.

\subsection{Future Extensions}

While our current approach uses patch similarity for computational efficiency, the framework naturally extends to test-based evaluation.
Future work could incorporate execution of project test suites to verify functional correctness and multi-objective rewards that train on diverse task sets to improve generalization capabilities.

These extensions would require significant infrastructure investment but could lead to more robust and generalizable repair capabilities.

\section{Long context}
\label{sec:long-context}
Agent interactions present significant computational scaling challenges, being orders of magnitude more expensive than simple text generation.
Each training step involves multiple model calls, often ranging from 10-30 per trajectory, while memory requirements scale directly with trajectory length, necessitating careful resource management throughout the training process.

\section{High-Performance Training Infrastructure}\label{sec:method-training-infrastructure}

\todoinline{Consider adding: The dual computation problem - online RL for LLMs requires maintaining two distinct workloads: inference (low latency, high throughput, KV caching) and training (gradient accumulation, optimizer states, memory checkpointing).
}

\section{vLLM}

\todoinline{Write short explanation of vLLM choice: why vLLM over SGLang, key features that matter for online RL (batching, memory optimization), extensibility for our NCCL modifications}

Our inference layer leverages vLLM's powerful inference engine to handle the demanding requirements of online \ac{RL} workloads.
vLLM's KV-cache management, continuous batching, and PagedAttention~\cite{kwon2023efficientmemorymanagementlarge} collectively enable fast, efficient serving of asynchronous multi-turn agents.
Crucially, between model weight updates, prefix caches deterministically hit, providing substantial performance boosts during the intervals when agents operate with stable model parameters.

To support online training requirements, we extended vLLM's OpenAI-compatible \ac{API} server with live weight synchronization capabilities, enabling seamless model updates without service interruption through our \ac{NCCL}-based synchronization system.

\subsection{Logit divergence problem}
A critical technical challenge in our online training approach involves logit divergence between inference and training computations, which can destabilize learning and degrade model performance.
This phenomenon, where logits computed during fast inference differ from those generated during training, becomes particularly problematic in multi-turn agent interactions.
This challenge motivates our adoption of \ac{GSPO}, which addresses these stability issues through improved sequence-level importance sampling mechanisms.

\subsection{\ac{NCCL}-Based Live Weight Synchronization} \label{sec:nccl-sync}

A critical technical breakthrough enabling our training-inference duality is real-time weight synchronization between the training pipeline and inference servers using NVIDIA Collective Communications Library (\ac{NCCL}).

\subsection{Technical Architecture}

The weight synchronization system operates through several components:

Training nodes broadcast updated weights to inference servers using optimized \ac{NCCL} collective communication primitives, ensuring minimal latency and maximum bandwidth utilization across the distributed system.

The system implements differential updates through sequential parameter gathering, which selectively broadcasts only modified parameters (specifically, the LoRA adapter weights) rather than entire model states.
This approach dramatically reduces network traffic and update latency while minimizing peak \ac{GPU} memory requirements by approximately 500-fold compared to full model synchronization, making the system practical even with limited hardware resources.

\subsubsection{Performance Characteristics}

The performance characteristics of our \ac{NCCL}-based weight synchronization system demonstrate significant engineering achievements in distributed training infrastructure.
Weight broadcasts complete in 150-300ms for LoRA adapters with ranks between 8 and 64, enabling near real-time policy improvements throughout the training process.
Differential updates reduce communication overhead by 95\% compared to full weight synchronization, utilizing only 100-500MB per update versus the 50GB+ required for complete model transfers.

Several technical challenges required careful engineering solutions to achieve these performance levels.
Custom \ac{CUDA} memory management through coordinated memory pools prevents out-of-memory conditions during concurrent training and inference operations.
The system demonstrates linear scalability to multiple inference nodes through optimized broadcast tree architectures that minimize update latency as cluster size increases, with production deployments achieving stable operation across up to 8 inference nodes receiving synchronized updates.

This infrastructure represents a significant advance in making online RL practical for large language models, reducing the traditional barrier between training and deployment while maintaining production-grade reliability and performance.

This \ac{NCCL}-based approach represents a significant engineering achievement, enabling truly continuous learning where model improvements benefit ongoing inference within seconds of gradient computation.

\subsection{Integrated System Architecture}

The complete online \ac{RL} system consists of several interconnected components operating in parallel:

The complete system architecture comprises distinct computational clusters optimized for their respective workloads.
The vLLM inference cluster employs a single 80GB A100 \ac{GPU} for 8B and 14B parameter models, scaling to two \acp{GPU} for 32B parameter models.
The TRL training cluster utilizes two 80GB A100 GPUs for smaller models and four GPUs for 32B parameter models, managing both agent trajectory generation and weight synchronization across the distributed system.

\subsection{Implementation Details}

Several key implementation choices enable efficient and safe operation of the distributed system.
Collective RPC facilitates efficient weight sharing across distributed processes, while LoRA adaptation provides optional use of low-rank adapters to reduce communication overhead.
Each agent runs in an isolated Docker container for safety, and request batching processes multiple agent requests concurrently to maximize system efficiency.

Figure~\ref{fig:system-architecture} presents the complete system architecture, illustrating data flow between the vLLM inference cluster, distributed agent worker pool, and GRPO training pipeline.
The diagram highlights the \ac{NCCL} communication layer that enables real-time weight synchronization, supporting the training-inference duality that distinguishes our approach from conventional RL systems.

\section{Evaluation Methodology}
\label{sec:evaluation}

\subsection{Evaluation Metrics}

We assess model performance using several complementary metrics that capture different aspects of debugging capability.
Success rate measures the percentage of bugs for which the agent generates an exactly correct patch, while partial success rate includes fixes that target the correct location with minor differences.
Time to solution tracks the average wall-clock time required to generate a fix, and exploration efficiency quantifies the number of files examined and commands executed per bug, providing insights into the agent's investigation strategies.

\subsection{Baseline Comparisons}

To contextualize our results, we establish baselines through systematic comparison with both the same models before training and state-of-the-art systems.
Our primary comparison focuses on the performance delta achieved through our RL training approach, measured against the base Qwen models without any task-specific fine-tuning.
Additionally, we compare against contemporary frontier coding assistants to establish the relative performance of our approach within the broader landscape of automated programming tools.

\subsection{Generalization Testing}

Beyond in-domain performance on SWE-Bench, we evaluate generalization through cross-dataset evaluation on Multi-SWE-Bench, which tests transfer to different programming languages and problem domains, and cross-task evaluation on TauBench, which assesses whether debugging skills learned in our training regime transfer to broader software engineering tasks.

\subsection{Statistical Significance}
\todoinline{Should we have something like this?}
