\chapter{Method}
\label{ch:method}

This chapter presents our novel approach to training large language models for automated code repair through agent-in-the-loop reinforcement learning. We detail the experimental design, technical implementation, and evaluation methodology used to investigate whether embedding coding agents directly into the RL training loop improves bug-fixing performance.

\section{Overview of Agent-in-the-Loop Reinforcement Learning}
\label{sec:method-overview}

Traditional approaches to training LLMs for code repair rely on supervised fine-tuning with static datasets of code-patch pairs. In contrast, our method pioneers \textit{agent-in-the-loop reinforcement learning}, where coding agents actively interact with real software repositories during training. This paradigm shift transforms models from passive learners observing fixed examples into active agents that learn through environmental interaction and experiential feedback.

The core innovation lies in our integration of existing coding agent frameworks directly into the RL training pipeline. Rather than constraining models to single-pass generation, we enable multi-step interactions where agents can:
\begin{itemize}
    \item Navigate repository structures using terminal commands
    \item Examine multiple files to understand code context
    \item Iteratively refine solutions based on environmental feedback
    \item Learn from the outcomes of their actions rather than just imitating examples
\end{itemize}

By implementing an OpenAI-compatible API server with asynchronous token streaming capabilities, we bridge the gap between standard RL training frameworks and agent scaffolding. This enables our custom Nano coding agent to interact naturally with repositories through basic terminal commands while maintaining compatibility with the RL training loop.

Figure~\ref{fig:agent-loop-architecture} illustrates the complete agent-in-the-loop training architecture, highlighting the continuous feedback cycle between language model policy, agent scaffold, repository environment, and reward computation. This diagram demonstrates how traditional RL training is enhanced by embedding interactive agents directly within the optimization loop.

\section{Experimental Setup}
\label{sec:experimental-setup}

\subsection{Datasets and Benchmarks}

Our experimental design carefully separates training and evaluation data to demonstrate true generalization rather than memorization. We employ the following datasets:

\textbf{Training Dataset:} We use SWE-Gym, a curated collection of approximately 2,400 Python bug-fixing tasks derived from real GitHub repositories. Each task includes:
\begin{itemize}
    \item A containerized repository snapshot at the time of the bug report
    \item The original issue description
    \item The ground-truth patch that resolved the issue
    \item Isolated execution environments for safe agent interaction
\end{itemize}

SWE-Gym's containerized design makes it ideal for RL training, as agents can freely explore and modify code without risk while receiving deterministic feedback based on their actions.

\textbf{Primary Evaluation:} SWE-Bench-Verified serves as our main evaluation benchmark, containing approximately 500 carefully validated Python bugs from popular open-source projects. These bugs are notably more challenging than the training set, often requiring multi-file modifications and deep understanding of project structure. This dataset tests whether our agent-trained models can generalize beyond their training distribution.

\textbf{Generalization Testing:} To assess cross-language transfer of learned repair skills, we evaluate on Defects4J v2.0, which contains 835 real Java bugs from mature projects. This tests whether the code repair strategies learned in Python environments transfer to syntactically different languages—directly addressing our research question about the generality of agent-learned skills.

\subsection{Pretrained Model Selection: Qwen for Tool Calling Excellence}

We base our experiments on the Qwen2.5-Coder family, specifically targeting both 7B and 32B parameter variants. Qwen was selected based on extensive empirical evidence from recent literature demonstrating its superior tool-calling capabilities compared to other open-source alternatives.

\subsubsection{Tool Calling Performance}

Qwen models exhibit exceptional structured output generation, critical for reliable agent behavior:
\begin{itemize}
    \item \textbf{Function call accuracy}: Qwen consistently generates valid JSON function calls with correct parameter types
    \item \textbf{Context retention}: Superior ability to maintain conversation state across multi-step tool interactions
    \item \textbf{Error recovery}: Robust handling of tool execution failures and adaptive strategy adjustment
    \item \textbf{Schema adherence}: Reliable conformance to function signatures and parameter constraints
\end{itemize}

\subsubsection{Code Understanding Capabilities}

Beyond tool calling, Qwen demonstrates strong foundational coding abilities:
\begin{itemize}
    \item State-of-the-art performance on HumanEval, MBPP, and CodeContests benchmarks
    \item Superior multi-language support including Python, JavaScript, Java, and C++
    \item Excellent repository-level understanding and cross-file reasoning capabilities
    \item Strong performance on debugging and code repair tasks in baseline evaluations
\end{itemize}

The model undergoes continued training rather than traditional fine-tuning, preserving its general capabilities while acquiring specialized agent skills through reinforcement learning. This approach maintains Qwen's robust tool-calling foundation while enabling learning of complex debugging behaviors.

\section{The Nano Coding Agent: Implementing the Bitter Lesson}
\label{sec:nano-agent}

Our Nano coding agent implements the bitter lesson philosophy in automated code repair: rather than engineering sophisticated tools and guided workflows, we provide only essential capabilities and let effective behaviors emerge through reinforcement learning. This approach tests whether computation and learning can overcome the apparent disadvantage of minimal tooling.

\subsection{Core Capabilities}

The Nano agent provides a streamlined set of tools for repository interaction:

\begin{itemize}
    \item \texttt{bash}: Execute shell commands for navigation and file system operations
    \item \texttt{str\_replace}: Perform precise string replacements for code modification
    \item \texttt{view\_file}: Examine file contents with optional line range specification
    \item \texttt{write\_file}: Create new files when necessary
\end{itemize}

This minimal toolset encourages models to develop their own strategies for code understanding and modification rather than relying on pre-engineered heuristics. The agent must learn to:
\begin{itemize}
    \item Navigate unfamiliar codebases using standard Unix commands
    \item Identify relevant files through grep searches and directory exploration
    \item Understand code context by examining multiple related files
    \item Apply targeted fixes using precise string replacements
\end{itemize}

\subsection{The Bitter Lesson Applied to Code Repair}

The Nano agent's design directly applies Rich Sutton's bitter lesson to automated software engineering. This lesson, learned from decades of AI research, teaches us that methods leveraging computation and learning consistently outperform approaches relying on human knowledge and engineering—even when the latter initially seem more promising.

Applied to coding agents, this principle suggests:

\textbf{Learning Over Engineering:} Rather than pre-programming sophisticated debugging heuristics, we provide minimal tools and let effective strategies emerge through extensive training.

\textbf{Computation Over Complexity:} Complex behaviors should arise from simple primitives and large-scale learning rather than from engineering complex tool interfaces.

\textbf{Generality Over Specialization:} Basic, general-purpose tools should ultimately prove more effective than specialized, engineered solutions as models learn to use them creatively.

\textbf{Scalability Focus:} The design prioritizes scalability to massive training regimens rather than immediate performance optimization through sophisticated tooling.

\subsection{Integration with RL Training}

The Nano agent integrates seamlessly with our RL training pipeline through a structured action-observation loop:

\begin{enumerate}
    \item The agent receives an issue description and repository state
    \item It generates a sequence of tool calls to explore and understand the codebase
    \item Each action produces observations (command outputs, file contents, etc.)
    \item The agent iteratively refines its understanding and proposes fixes
    \item The final patch is evaluated against the ground truth for reward computation
\end{enumerate}

This cycle allows the model to learn from both successful and unsuccessful repair attempts, gradually improving its ability to navigate codebases and identify correct fixes. The model applies an arbitrary sequence of apply\_patch operations in simplified search-replace format, then after task completion we compute the final diff instead of requiring the model to generate properly formatted patches directly.

This learning-centric approach embodies the bitter lesson: rather than engineering away the difficulty of code repair through sophisticated tooling, we embrace the challenge and let the model develop its own solutions through extensive experience.

\section{Training-Inference Duality in Agent-in-the-Loop RL}
\label{sec:training-inference}

A fundamental innovation in our approach is the collapse of the traditional training-inference boundary. Unlike conventional RL where trajectory collection and policy updates occur in separate phases, our system performs both simultaneously through continuous serving and real-time weight updates.

\subsection{The Unified Training-Inference Paradigm}

In traditional RL, models alternate between:
\begin{enumerate}
    \item \textbf{Inference phase}: Collecting trajectories with a frozen policy
    \item \textbf{Training phase}: Computing gradients and updating parameters offline
\end{enumerate}

Our agent-in-the-loop system eliminates this separation. The GRPO process operates as a continuous cycle where:
\begin{enumerate}
    \item \textbf{Live inference}: vLLM serves the current policy to multiple agent workers simultaneously
    \item \textbf{Trajectory streaming}: Completed agent episodes stream to the training pipeline in real-time
    \item \textbf{Immediate updates}: GRPO computes gradients and updates weights continuously
    \item \textbf{Live synchronization}: Updated weights propagate to the inference server without interruption
\end{enumerate}

This unified paradigm enables true online learning where the policy improves continuously throughout the training session, rather than in discrete update cycles.

\subsection{Benefits of Training-Inference Integration}

\textbf{Reduced Training Time}: Eliminating the inference-training separation dramatically reduces wall-clock training time, as trajectory collection and policy updates overlap completely.

\textbf{Better Sample Efficiency}: The policy benefits from improvements immediately, enabling more efficient exploration as training progresses.

\textbf{Resource Utilization}: GPU resources are maximally utilized since inference and training computations run concurrently on different hardware.

\textbf{Stability}: Continuous small updates prove more stable than large batch updates, reducing training instability.

\section{Reinforcement Learning Training Algorithm}
\label{sec:rl-algorithm}

\subsection{Group Relative Policy Optimization (GRPO)}

We employ Group Relative Policy Optimization (GRPO) as our primary RL algorithm. GRPO offers several advantages for our agent-in-the-loop setting:

\begin{itemize}
    \item \textbf{No value model required:} Unlike traditional actor-critic methods, GRPO estimates advantages using relative performance within a batch, eliminating the need for a separate value network
    \item \textbf{Reduced variance:} By normalizing rewards across groups of trajectories, GRPO provides more stable training signals
    \item \textbf{Simplified pipeline:} The absence of value estimation reduces computational overhead and implementation complexity
\end{itemize}

\subsection{Training Process}

Our training follows an iterative process where the agent attempts to solve bugs from the SWE-Gym dataset:

\begin{enumerate}
    \item \textbf{Trajectory Generation:} For each bug in a training batch, the agent generates a complete trajectory—from initial exploration through final patch submission
    \item \textbf{Reward Computation:} Each trajectory receives a reward based on the similarity between the generated patch and the ground-truth solution (detailed in Section~\ref{sec:reward-design})
    \item \textbf{Advantage Estimation:} GRPO computes advantages by comparing each trajectory's reward to the mean reward within its group
    \item \textbf{Policy Update:} The model parameters are updated to increase the likelihood of high-advantage actions while maintaining proximity to the reference policy through KL regularization
\end{enumerate}

\subsection{Hyperparameters}

Key hyperparameters for our GRPO implementation include:

\begin{itemize}
    \item \textbf{Batch size:} 32 trajectories per update
    \item \textbf{Learning rate:} $1 \times 10^{-5}$ with cosine annealing
    \item \textbf{KL coefficient:} $\beta = 0.1$ to balance exploration and stability
    \item \textbf{Maximum trajectory length:} 8192 tokens to accommodate multi-step agent interactions
    \item \textbf{Training iterations:} 5,000 episodes covering the SWE-Gym dataset
\end{itemize}

These values were selected through preliminary experiments to ensure stable convergence while allowing sufficient exploration of the action space.

\subsection{Large-Scale Compute Management}

Training and serving 8B to 32B parameter models for agent-in-the-loop RL requires sophisticated compute optimization across the entire pipeline.

\subsubsection{Training Optimizations}

\textbf{DeepSpeed ZeRO-3 Integration:} Complete model parameter, gradient, and optimizer state sharding enables training of 32B models across multiple GPUs with minimal memory overhead per device.

\textbf{Gradient Checkpointing:} Selective activation checkpointing reduces memory usage by 50\% during backward passes, trading compute for memory to enable larger batch sizes and longer sequence lengths.

\textbf{Mixed Precision Training:} FP16 training with automatic loss scaling reduces memory usage and increases throughput while maintaining numerical stability.

\textbf{Communication Optimization:} Overlapped gradient communication during backward passes minimizes the impact of all-reduce operations on training throughput.

\subsubsection{LoRA+ Training Enhancements}

Beyond basic LoRA, we implement several advanced parameter-efficient training techniques:

\textbf{Rank-Adaptive LoRA:} Dynamic rank adjustment based on gradient norms allows optimal parameter efficiency while maintaining expressiveness for complex agent behaviors.

\textbf{LoRA+:} Enhanced initialization and scaling strategies specifically designed for RL workloads, improving convergence stability and final performance.

\textbf{Selective Layer Adaptation:} Strategic selection of which transformer layers receive LoRA adapters based on analysis of gradient magnitudes and learning dynamics.

\subsubsection{Memory Management Strategies}

\textbf{Activation Partitioning:} Custom memory management splits large activations across devices to minimize peak memory usage during both forward and backward passes.

\textbf{Dynamic Batch Construction:} Intelligent batching algorithms pack variable-length agent trajectories to maximize GPU utilization while respecting memory constraints.

\textbf{Cache-Aware Scheduling:} Coordinated scheduling between training and inference processes minimizes memory contention and enables stable dual-mode operation.

\subsubsection{Cost-Performance Trade-offs}

The combination of these optimizations enables training of state-of-the-art agent behaviors while maintaining practical computational budgets:

\begin{itemize}
    \item \textbf{8B models:} Trainable on 4x A100 GPUs with full agent interaction
    \item \textbf{32B models:} Scalable to 8x A100 GPUs with maintained training throughput
    \item \textbf{Memory efficiency:} 90\%+ memory utilization through careful optimization
    \item \textbf{Training speed:} Agent trajectory processing at near real-time rates
\end{itemize}

These innovations make large-scale agent-in-the-loop RL practically feasible for academic research, democratizing access to advanced coding agent training previously available only to industry labs with unlimited compute budgets.

\section{Reward Design}
\label{sec:reward-design}

\subsection{Outcome-Based Patch Similarity}

Our reward function focuses on the final outcome—the generated patch—rather than intermediate steps. This design choice offers several benefits:

\begin{itemize}
    \item \textbf{Simplicity:} Evaluating final patches is straightforward and deterministic
    \item \textbf{Flexibility:} Agents can discover diverse problem-solving strategies without being constrained by process-specific rewards
    \item \textbf{Alignment:} The reward directly measures what we care about—correct bug fixes
\end{itemize}

\subsection{Reward Computation}

For each generated patch, we compute rewards based on similarity to the ground-truth solution:

\begin{equation}
R(p_{\text{gen}}, p_{\text{true}}) = \begin{cases}
1.0 & \text{if } p_{\text{gen}} = p_{\text{true}} \\
\text{sim}(p_{\text{gen}}, p_{\text{true}}) & \text{if partial match} \\
0.0 & \text{otherwise}
\end{cases}
\end{equation}

Where $\text{sim}$ measures line-level similarity between patches, awarding partial credit for fixes that modify the correct locations but with imperfect changes.

This reward computation approach addresses a fundamental challenge in LLM-based code modification: the notorious difficulty of generating syntactically valid diffs. Traditional approaches require models to produce unified diff format with precise line numbering, context coordination, and complex formatting rules—a task that frequently results in parsing errors and invalid patches.

Our approach elegantly separates semantic understanding from syntactic formatting. During interaction, the agent performs any number of \texttt{apply\_patch} operations using a simplified search-replace format that aligns naturally with LLM capabilities. Each operation specifies exact text to replace and its substitution, without requiring knowledge of line numbers or diff syntax.

Only after the complete interaction do we execute \texttt{git diff} to compute the actual patch for evaluation. This separation of concerns offers several advantages:

\begin{itemize}
\item \textbf{Error Elimination}: Models cannot fail due to diff formatting issues, as the patch is computed automatically
\item \textbf{Semantic Focus}: Agents concentrate entirely on identifying and fixing bugs rather than wrestling with syntax requirements
\item \textbf{Perfect Formatting}: Git ensures all patches are properly formatted and applicable
\item \textbf{Complete Information}: Every intended modification is captured accurately for reward computation
\end{itemize}

This architectural choice transforms a major source of technical failures into a reliable, automated process while preserving full fidelity of the agent's debugging intentions.

\subsection{Addressing Sparse Rewards}

The sparse nature of exact patch matching presents challenges for RL training. We address this through:

\begin{itemize}
    \item \textbf{Curriculum learning:} Starting with simpler bugs where rewards are more attainable
    \item \textbf{Partial credit:} Rewarding patches that target correct files and functions even if the exact fix differs
    \item \textbf{Large batch sizes:} Ensuring sufficient positive examples within each training batch
\end{itemize}

\subsection{Future Extensions}

While our current approach uses patch similarity for computational efficiency, the framework naturally extends to test-based evaluation. Future work could incorporate:

\begin{itemize}
    \item Execution of project test suites to verify functional correctness
    \item Multi-objective rewards balancing correctness, code quality, and efficiency
    \item Human preference learning for subjective aspects of code style
\end{itemize}

These extensions would require significant infrastructure investment but could lead to more robust and generalizable repair capabilities.

\section{High-Performance Training Infrastructure}
\label{sec:training-infrastructure}

\subsection{vLLM Serving Optimizations}

Our system leverages vLLM's advanced serving capabilities, enhanced with additional optimizations for RL training workloads:

\subsubsection{Core vLLM Features}

\textbf{KV-Cache Management:} Intelligent key-value cache reuse across agent conversations dramatically reduces memory usage and latency. For multi-step debugging sessions, context prefixes are cached and reused, providing up to 3x speedup in token generation.

\textbf{Continuous Batching:} Dynamic request batching allows optimal GPU utilization by processing multiple agent requests simultaneously, with automatic load balancing across available compute resources.

\textbf{Memory-Efficient Attention:} PagedAttention implementation reduces memory fragmentation and enables serving larger models with the same hardware footprint.

\subsubsection{RL-Specific Enhancements}

\textbf{Trajectory Streaming:} Custom modifications enable real-time streaming of completed agent trajectories to the training pipeline without blocking inference for ongoing requests.

\textbf{Model Hot-Swapping:} Live weight updates occur without service interruption through careful state management and request routing.

\textbf{Multi-Model Serving:} Simultaneous serving of both the current policy and reference policy (required for GRPO) with shared base weights and separate adapters.

\subsection{NCCL-Based Live Weight Synchronization}

A critical technical breakthrough enabling our training-inference duality is real-time weight synchronization between the training pipeline and inference servers using NVIDIA Collective Communications Library (NCCL).

\subsubsection{Technical Architecture}

The weight synchronization system operates through several components:

\textbf{NCCL Broadcast Groups:} Training nodes broadcast updated weights to inference servers using optimized collective communication primitives. This ensures minimal latency and maximum bandwidth utilization.

\textbf{Asynchronous Updates:} Weight broadcasts occur asynchronously with respect to inference requests, preventing service interruption during model updates.

\textbf{Version Control:} Sophisticated versioning ensures request consistency—all tokens within a generation use the same model weights, even if updates occur mid-generation.

\textbf{Differential Updates:} Only changed parameters (typically LoRA adapters) are broadcast, dramatically reducing network traffic and update latency.

\subsubsection{Performance Characteristics}

The performance characteristics of our NCCL-based weight synchronization system demonstrate significant engineering achievements:

\textbf{Update Latency}: Weight broadcasts complete in 150-300ms for LoRA adapters (8-64 rank), enabling near real-time policy improvements. Full model updates for smaller models (8B parameters) complete in 2-4 seconds.

\textbf{Throughput Impact}: Inference throughput degrades by less than 5\% during weight updates, achieved through careful scheduling and asynchronous communication patterns.

\textbf{Memory Overhead}: Dual-model serving (current policy + reference policy) requires only 15\% additional memory through shared base weights and separate adapters.

\textbf{Network Utilization}: Differential updates reduce communication by 95\% compared to full weight synchronization, utilizing only 100-500MB per update versus 50GB+ for complete models.

Several technical challenges were overcome to achieve these performance levels:

\textbf{CUDA Memory Management}: Custom memory pools coordinate between training and inference processes, preventing out-of-memory conditions during concurrent operation.

\textbf{Partial Generation Handling}: Sophisticated request tracking ensures that ongoing generations complete with consistent weights, while new requests use updated parameters.

\textbf{Numerical Consistency}: Careful attention to floating-point precision and operation ordering maintains bit-exact reproducibility across distributed updates.

\textbf{Scalability}: The system scales linearly to multiple inference nodes, with broadcast trees minimizing update latency as cluster size increases. Production deployments have demonstrated stable operation with up to 8 inference nodes receiving synchronized updates.

This infrastructure represents a significant advance in making online RL practical for large language models, reducing the traditional barrier between training and deployment while maintaining production-grade reliability and performance.

This NCCL-based approach represents a significant engineering achievement, enabling truly continuous learning where model improvements benefit ongoing inference within seconds of gradient computation.

\subsection{Integrated System Architecture}

The complete agent-in-the-loop RL system consists of several interconnected components operating in parallel:

\begin{enumerate}
    \item \textbf{vLLM Inference Cluster:} Multi-GPU serving infrastructure with KV-cache optimization and real-time weight updates
    \item \textbf{Agent Worker Pool:} Distributed Nano agent instances operating in parallel across containerized environments
    \item \textbf{GRPO Training Pipeline:} Continuous gradient computation and parameter updates with advanced memory management
    \item \textbf{NCCL Communication Layer:} High-bandwidth weight synchronization enabling training-inference duality
    \item \textbf{Resource Orchestration:} Dynamic load balancing and memory management across training and inference workloads
\end{enumerate}

\subsection{Real-Time Weight Updates}

Unlike traditional RL setups where inference and training are separate phases, our system enables continuous learning:

\begin{itemize}
    \item Model weights are updated after each batch of trajectories
    \item Updates are immediately synchronized to all inference processes
    \item Agents benefit from improvements within the same training session
\end{itemize}

This tight integration between serving and training represents a significant departure from conventional approaches, enabling more efficient exploration and faster convergence.

\subsection{Implementation Details}

Key implementation choices include:

\begin{itemize}
    \item \textbf{Collective RPC:} For efficient weight sharing across distributed processes
    \item \textbf{LoRA adaptation:} Optional use of low-rank adapters to reduce communication overhead
    \item \textbf{Containerized environments:} Each agent runs in an isolated Docker container for safety
    \item \textbf{Request batching:} Multiple agent requests are processed concurrently for efficiency
\end{itemize}

Figure~\ref{fig:system-architecture} presents the complete system architecture, illustrating data flow between the vLLM inference cluster, distributed agent worker pool, and GRPO training pipeline. The diagram highlights the NCCL communication layer that enables real-time weight synchronization, supporting the training-inference duality that distinguishes our approach from conventional RL systems.

\section{Evaluation Methodology}
\label{sec:evaluation}

\subsection{Evaluation Metrics}

We assess model performance using several complementary metrics:

\begin{itemize}
    \item \textbf{Success Rate:} Percentage of bugs for which the agent generates an exactly correct patch
    \item \textbf{Partial Success Rate:} Including fixes that target the correct location with minor differences
    \item \textbf{Time to Solution:} Average wall-clock time required to generate a fix
    \item \textbf{Exploration Efficiency:} Number of files examined and commands executed per bug
\end{itemize}

\subsection{Baseline Comparisons}

To contextualize our results, we compare against several baselines:

\begin{enumerate}
    \item \textbf{Base Model:} The pretrained Qwen2.5-Coder without any bug-fixing training
    \item \textbf{Supervised Fine-Tuning:} The same model fine-tuned on bug-fix pairs using standard supervised learning
    \item \textbf{Direct Generation:} Models prompted to generate fixes without agent scaffolding
    \item \textbf{State-of-the-art Systems:} Published results from other automated repair approaches
\end{enumerate}

\subsection{Generalization Testing}

Beyond in-domain performance, we evaluate generalization through:

\begin{itemize}
    \item \textbf{Cross-dataset evaluation:} Testing on SWE-Bench-Verified after training on SWE-Gym
    \item \textbf{Cross-language evaluation:} Applying Python-trained models to Java bugs
    \item \textbf{Temporal generalization:} Testing on bugs from time periods not covered in training
\end{itemize}

\subsection{Ablation Studies}

To understand the contribution of different components, we conduct ablations:

\begin{itemize}
    \item \textbf{Without RL:} Using only supervised fine-tuning on the same data
    \item \textbf{Without agent scaffold:} Direct patch generation without repository interaction
    \item \textbf{Varying trajectory lengths:} Impact of allowing more or fewer exploration steps
    \item \textbf{Different reward functions:} Comparing patch similarity vs. binary success rewards
\end{itemize}

\subsection{Statistical Significance}

All reported improvements are tested for statistical significance using:

\begin{itemize}
    \item Bootstrap confidence intervals for success rate differences
    \item McNemar's test for paired comparisons on the same test set
    \item Effect size measurements (Cohen's d) to quantify practical significance
\end{itemize}

This comprehensive evaluation framework ensures that our conclusions about agent-in-the-loop RL are well-supported and reproducible.
