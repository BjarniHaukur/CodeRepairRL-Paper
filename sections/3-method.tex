\chapter{Methodology}
\label{ch:method}

This chapter presents an execution-free \acs{RL} methodology for repository-level \acs{APR}.
The approach integrates a minimalist terminal agent (Nano) directly into the training loop, using terminal patch-similarity rewards computed deterministically on canonical diffs to enable language-agnostic learning without executable test harnesses.
We first formalize the Nano agent specification (observation space, action space, termination conditions, reward function), then detail training data, policy optimization, and infrastructure requirements.
Evaluation protocols are summarized in \cref{sec:eval-brief} and expanded with complete results in Chapter~\ref{ch:results}.

\section{Overview and Scope}

We study whether online \ac{RL} improves a single minimalist coding agent when grounded in real repositories.
The methodology is programming-language agnostic by design: reward depends only on the canonical diff computed from the repository state after the agent's actions, compared against a ground-truth patch.
This removes the need for language-specific runners during training and permits controlled data mixing across languages when ground-truth patches exist.

\section{Training Data}
\label{sec:data-env}

Unlike traditional supervised learning that trains on static datasets, our \ac{RL} coding agent approach trains on problem instances where the agent interactively debugs repositories through the terminal environment.
Each training instance consists of a GitHub repository at a specific buggy commit paired with an issue description and a ground-truth patch.
The agent does not observe the repository contents or ground-truth solution during training; instead, it must learn effective debugging strategies through repeated interaction with these problem instances, receiving terminal rewards based on the correctness of its proposed fixes.

Training employs a 1{,}000-task curriculum combining Python and multilingual repositories.
SWE-Gym~\cite{sweGym2025} provides approximately 2{,}400 Python bug-fixing tasks extracted from real GitHub repositories; we select 750 of these for the training curriculum.
SWE-Bench-Multilingual~\cite{sweBenchMultilingual2025} offers debugging tasks spanning nine programming languages: Rust, Java, PHP, Ruby, JavaScript, TypeScript, Go, C, and C++.
From this collection, we incorporate 250 tasks into training and reserve 50 as a held-out evaluation set.
Instruction-driven, repository-level multilingual debugging datasets remain substantially scarcer than Python-only resources, constraining the scale of cross-language training currently feasible.

\section{Nano Agent}
\label{sec:nano-agent}

We formalize the Nano agent through its observation space, action space, interaction dynamics, termination conditions, and safety constraints.
This specification provides the foundation for the \ac{RL} training methodology described subsequently.

\begin{figure}[H]
	\centering
\includegraphics[width=0.3\textwidth]{plotting/figures/nano_blank.png}
\end{figure}

\subsection{Episode Initialization}

Each episode begins with the agent placed within an isolated clone of a Git repository containing a known bug.
The agent receives a task prompt in the form of a issue description that specifies the problem to be solved, typically via a bug report.
This issue serves as the sole high-level specification of the task; the agent must autonomously navigate the repository, locate relevant code, and implement appropriate fixes using only the provided tools.
The repository starts in a clean working state at a specific commit corresponding to the buggy version, and the agent operates entirely through terminal interactions without access to external documentation, pre-computed file listings, or repository metadata beyond what it can discover through exploration.

\subsection{Observations}

All interaction occurs through a terminal transcript maintained as a structured conversation history.
The agent observes system messages, issue descriptions, tool outputs, and error messages as they appear in the dialogue.
Each tool invocation returns at most 2{,}000 characters; outputs exceeding this limit are truncated deterministically with an explicit ``\ldots{} output truncated \ldots{}'' marker appended.
The agent must discover all repository information through its own tool invocations; no file tree summaries, test results, or repository statistics are provided beyond what appears in the visible transcript.

\subsection{Action Space}

The agent has two tools: (i) \texttt{shell(cmd)} executes within a restricted bash (rbash) with a per-call timeout; and (ii) \texttt{apply\_patch(path, old\_content, new\_content)} performs a literal search-and-replace operation without regex.
Patches must target files within the current repository; the specified \texttt{old\_content} must match uniquely in the file at the time of application.

A ``null action'' terminates the episode when the agent produces no further tool calls: if repository changes exist, this constitutes a submission; if no changes exist, the agent is prompted to continue working until changes are made or limits are reached.

\subsection{Formal Interaction Loop}

The agent operates through a structured cycle that formalizes the relationship between observations, actions, and environmental feedback.
At each timestep $t$, the agent maintains a conversation history $h_t$ comprising the system message, issue description, and all prior agent-environment interactions.
Given this history, the model generates an assistant response $y_t$ containing tool invocations expressed as \ac{JSON} function calls.
The inference endpoint extracts the first tool call through a deterministic parser $c_t = \psi(y_t)$, executes it within the isolated repository environment, and returns the resulting output $o_t$—command results, file contents, or error messages—subject to the 2{,}000-character truncation policy.

The history advances to $h_{t+1} = \text{append}(h_t, y_t, o_t)$, incorporating both the assistant's generation and the environment's response.
This cycle continues until a termination condition is met: the agent submits by producing repository changes without further tool calls, the tool-call budget is exhausted, the token limit is exceeded, or the wall-clock timeout is reached.
Upon termination, the episode yields a complete trajectory $\tau = (h_0, y_0, o_0, h_1, \ldots, h_T)$ where $T$ denotes the final timestep.

\begin{figure}[H]
    \centering
\begin{minted}{python}
>>> shell(cmd="ls src/")
drwxr-xr-x  utils/
-rw-r--r--  main.py
-rw-r--r--  config.py

>>> shell(cmd="grep -n 'def process' src/utils.py")
42:def process_data(data):
43:    return data.strip().lower()

>>> apply_patch(
...     file_path="src/utils.py",
...     old_content="return data.strip().lower()",
...     new_content="return data.strip().lower().replace(' ', '_')"
... )
Patch applied successfully.
\end{minted}
\caption{Illustrative rollout of the Nano agent. Strings in red are supplied by the \ac{LLM} as \ac{JSON} tool call arguments. The agent explores repository structure and file contents before applying a targeted edit.}
\label{fig:nano-tools}
\end{figure}

\subsection{Sidestepping the Diff Generation Problem}

Generating syntactically valid unified diffs presents a high-dimensional formatting challenge for language models: line numbers must accurately reflect current file state, context lines must match existing content exactly character-for-character, and headers must specify correct paths, offsets, and chunk sizes.
State-of-the-art models frequently produce malformed diffs with misaligned line numbers, incorrect context, or invalid headers that fail during patch application, introducing a brittle failure mode orthogonal to the semantic debugging task.

The Nano agent sidesteps this issue by using semantically clear search-and-replace operations during interaction and deterministically computing the canonical diff via git after termination, effectively eliminating an entire class of diff formatting errors.

\begin{figure}[H]
\centering
\begin{minted}{bash}
$ git diff
diff --git a/src/utils.py b/src/utils.py
index abc123..def456 100644
--- a/src/utils.py
+++ b/src/utils.py
@@ -42,1 +42,1 @@ def process_data(data):
-    return data.strip().lower()
+    return data.strip().lower().replace(' ', '_')
\end{minted}
\caption{The harness computes the canonical diff via git after episode termination which is subsequently used to compute the reward.}
\label{fig:git-diff}
\end{figure}

\subsection{Termination and Limits}

Episodes terminate when any of the following conditions is met: (i) the agent produces repository changes and emits no further tool calls (successful submission), (ii) the tool-call budget is exhausted, (iii) the token budget is exceeded, or (iv) the wall-clock timeout is reached.
Throughout all experiments, we enforce a maximum of 30 tool calls, a cumulative token budget of 10,240 generated tokens, and a wall-clock timeout of approximately 60 seconds.
Individual tool outputs are truncated to 2,000 characters to prevent excessively verbose responses from consuming the token budget.
These limits keep \ac{VRAM} utilization bounded and improve sample efficiency: the agent receives warnings as it approaches limits, encouraging it to produce a submission rather than simply timing out.

\subsection{Sampling and Exploration}

Training and evaluation employ distinct sampling configurations that reflect the fundamental exploration-exploitation trade-off in \ac{RL}.
During training rollouts, we configure sampling with temperature 1.0 and top-$p$ 1.0 to encourage exploration of diverse debugging strategies and tool-usage patterns.
This maximally stochastic sampling enables the policy to discover varied solution trajectories and prevents premature convergence to suboptimal behaviors.
Conversely, evaluation rollouts use temperature 0.2 and top-$p$ 0.9 to exploit the learned policy, producing more deterministic and reliable behavior when assessing performance on held-out benchmarks.
This exploration-during-training, exploitation-during-evaluation paradigm ensures that the model explores broadly while learning but executes confidently when deployed.

\section{Reward Design}
\label{sec:reward}

The terminal reward $R(\tau) \in [0,1]$ evaluates the trajectory based on similarity between the canonical diff resulting from the agent's repository modifications and the ground-truth patch.

After an episode terminates, we compute per-file canonical diff hunks for the agent and the ground truth.
Let $F_a$ and $F_g$ denote the sets of files modified by the agent and in the ground-truth patch, and let $p_a(f)$ and $p_g(f)$ denote the canonical diff hunk strings for file $f$.
We aggregate per-file similarity and normalize by the greater number of affected files: \begin{equation}
R(\tau) = \frac{1}{\max\big(|F_a|,\,|F_g|\big)} \sum_{f \in F_a \cup F_g} \operatorname{similarity}\!\big(p_a(f),\, p_g(f)\big) \in [0,1].
\end{equation} Here $\operatorname{similarity}(\cdot,\cdot)$ is the string-similarity score of the canonical diff hunks computed with Python's \texttt{difflib.SequenceMatcher.ratio()}.
Files that appear in only the set of agent affected files or ground truth affected files contribute zero.

This design keeps training simple, avoids brittle intermediate reward engineering, and permits diverse problem-solving trajectories while targeting the ultimate objective directly: producing the correct changes to the repository.
By rewarding patch similarity rather than test passage, this execution-free approach enables language-agnostic training without language-specific test infrastructure, build systems, or execution environments.
The trade-off exchanges direct functional verification for infrastructure simplicity and determinism, making large-scale multilingual training tractable.

\section{Policy Optimization} \label{sec:rl}

We adopt \ac{GSPO}~\cite{gspo2025} as our policy optimization algorithm.
\ac{GSPO} extends the group-relative baseline principle from \ac{GRPO} with sequence-level importance weighting, offering improved stability over earlier group-relative methods.
This stability is essential for long-context, multi-turn coding agent training where episodes vary substantially in length and structure.

Our training objective combines \ac{GSPO} with a modest \ac{KL} penalty to counteract gradient variance from limited batch sizes: \begin{equation}
J(\theta) = J_{\mathrm{GSPO}}(\theta) - \beta_{\text{KL}} \cdot D_{\text{KL}}(\pi_\theta \| \pi_{\theta_{\text{ref}}}),
\end{equation} where the \ac{KL} divergence is computed on the logits of the rollouts used for training, $\beta_{\text{KL}}$ controls regularization strength, and $\pi_{\theta_{\text{ref}}}$ is the initial pre-trained checkpoint.

\section{Model Choice}
\label{sec:models}

We base our experiments on Qwen3-14B, a hybrid reasoning model combining strong coding capabilities with native tool-calling support.
Qwen3 was selected based on widespread consensus that it was the strongest open-weight coding model for tool-augmented tasks at the time of experimentation.
The 14B variant offered an optimal fit for available resources: Qwen3-8B proved too large for two \acp{GPU} yet too small to justify three, while larger variants (30B-A3B, 32B) imposed prohibitive computational demands.

Several alternative models including Llama3.1-8B, Gemma3, Ministral, GLM4-9B, and Llama3.1-Nemotron were explored but ultimately excluded.
Limited comparisons across Qwen3 sizes and Llama3.1-8B, along with more detailed justification of model selection constraints, appear in \cref{app:model-comparison}.

\section{Evaluation Protocol}
\label{sec:eval-brief}

Our evaluation protocol addresses the three research questions established in Chapter~\ref{ch:introduction}.

\textbf{RQ1: How does \ac{GSPO} training improve Nano harness adaptation?}
We measure tool-call success rate, invalid-call reduction, action efficiency, and command usage evolution, comparing pre-training and post-training behavior.
Command usage patterns are analyzed qualitatively across training runs to identify shifts in debugging strategies.

\textbf{RQ2: Does execution-free patch-similarity \ac{RL} training improve SWE-Bench-Verified performance?}
We measure test-verified success rates on SWE-Bench-Verified, comparing pre-training baseline to post-training performance using identical evaluation protocol.

\textbf{RQ3: Does execution-free \ac{RL} enable effective multilingual training without language-specific engineering?}
We measure per-language reward improvements on the 50-task SWE-Bench-Multilingual held-out set spanning nine programming languages, comparing pre-training and post-training checkpoints with bootstrap confidence intervals.

Baselines consist of the Qwen3-14B checkpoint with Nano before \ac{RL} training.
