\chapter{Method}
\label{ch:method}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=1.0\textwidth]{plotting/figures/diagram.png}
	\caption{Simultaneously too busy and too simplified, I'll make another version}
	\label{fig:method-diagram}
\end{figure}

This chapter presents our novel approach to training large language models for automated code repair through online \ac{RL}.
We detail the experimental design, technical implementation, and evaluation methodology used to investigate whether embedding coding agents directly into the \ac{RL} training loop improves bug-fixing performance.

\section{Overview of our approach}
\label{sec:method-overview}

Traditional approaches to training \acp{LLM} for code repair rely on \ac{SFT} with static datasets of code-patch pairs.
In contrast, our method pioneers online \ac{RL}, where coding agents actively interact with real software repositories during training.
This paradigm shift transforms models from passive learners observing fixed examples into active agents that learn through environmental interaction and experiential feedback.
% We retain the same outcome-driven similarity principle as SWE-RL~\cite{wei2025swerladvancingllmreasoning} while embedding it within a multi-step setting that rewards successful end-to-end repair after autonomous context acquisition.

The core innovation lies in our integration of existing coding agent frameworks directly into the \ac{RL} training pipeline.
Rather than constraining models to single-pass generation, we enable multi-step interactions where agents can:
\begin{itemize}
	\item Navigate repository structures using terminal commands
	\item Examine multiple files to understand code context
	\item Iteratively refine solutions based on environmental feedback
	\item Learn from the outcomes of their actions rather than just imitating examples
\end{itemize}

Unlike previous approaches that relied on synchronous batched generation, our implementation provides an asynchronous OpenAI-compatible \ac{API} server that enables true multi-turn interaction rather than single-pass generation.

% Our Nano coding agent can therefore engage in natural back-and-forth interaction with repositories through basic terminal commands while remaining fully compatible with the RL training loop.

% Figure~\ref{fig:agent-loop-architecture} illustrates the complete training architecture, highlighting the continuous feedback cycle between language model policy, agent implementation, repository environment, and reward computation.
% This diagram demonstrates how traditional \ac{RL} training is enhanced by embedding coding agents directly within the optimization loop.

\section{Nano Coding Agent}
\label{sec:nano-agent}
\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.6\textwidth]{plotting/figures/nano_blank.png}
	\label{fig:nano-sprite}
\end{figure}

Our Nano coding agent adopts a minimalist design: rather than engineering sophisticated tools and guided workflows, we provide only essential capabilities and allow effective behaviors to emerge through \ac{RL}.
This isolates the contribution of learning under a simple action space and avoids confounds introduced by heavyweight scaffolding.

\subsection{Tool Interface}

Nano agent has access to two streamlined tools for terminal based repository interaction:

\textbf{Shell Command Execution} (\texttt{shell(cmd)}): This tool allows the agent to execute unix commands within a restricted bash environment (rbash), such as exploration commands like \texttt{ls}, \texttt{rg}, \texttt{grep}, and \texttt{find}, as well as content inspection commands such as \texttt{cat}, \texttt{head}, \texttt{tail}, and \texttt{sed}.

\textbf{File Patching} (\texttt{apply\_patch}): This tool enables precise code modifications through a search-and-replace mechanism.
The agent specifies the target file path where changes should be made, the exact text that needs to be replaced (old\_content), the new replacement text (new\_content).

This minimal toolset encourages models to develop their own strategies for code understanding and modification.

\subsection{Sidestepping the Diff Generation Problem}

A critical design decision in the Nano agent architecture demonstrates how thoughtful tool design can eliminate entire classes of errors.
Generating valid unified diffs represents one of the most challenging output formatting tasks for language models.
The unified diff format requires:

\begin{verbatim}
--- a/src/utils.py
+++ b/src/utils.py
@@ -45,7 +45,7 @@ class DataProcessor:
     def process(self, data):
         if not data:
             return None
-        return data.strip().lower()
+        return data.strip().lower().replace(' ', '_')
     
     def validate(self, data):
         return len(data) > 0
\end{verbatim}

This format demands precise coordination across multiple dimensions.
The line numbers must accurately reflect the current file state, while context lines need to exactly match the existing content.
The diff must properly handle whitespace, indentation, and special characters throughout.
Additionally, it requires consistent header formatting with correct file paths and accurate chunk headers containing proper line counts.
This intricate coordination makes diff generation particularly challenging for language models.

Even state-of-the-art models frequently produce malformed diffs with misaligned line numbers, incorrect context, or formatting errors that prevent patch application.
\todoinline{Back this up}

The Nano agent completely sidesteps this challenge through an elegant architectural choice.
Instead of requiring diff generation, it provides a simple \texttt{apply\_patch} interface:

\begin{verbatim}
apply_patch(
    file_path="src/utils.py",
    old_content="return data.strip().lower()",
    new_content="return data.strip().lower().replace(' ', '_')"
)
\end{verbatim}

This format is naturally conducive to language model generation because:
\begin{itemize}
	\item \textbf{Semantic clarity}: The model specifies what to change in natural terms
	\item \textbf{No numerical coordination}: No line numbers or offsets to calculate
	\item \textbf{Robust matching}: String matching handles minor formatting variations
	\item \textbf{Clear intent}: The transformation is explicit and unambiguous
\end{itemize}

After the model applies changes using this simple interface, the actual diff is computed using \texttt{git diff}—a battle-tested tool that handles all formatting complexities correctly.

\subsection{Sidestepping Setup Constraints}

\todoinline{This will always look bad... We can say we attempt "safety" with rbash, readonly access and timeouts}

% Security is paramount when allowing language models to execute arbitrary commands.
% The Nano agent employs several safety mechanisms:

% \textbf{Restricted Bash (rbash)}: All shell commands execute within a restricted bash environment that prevents:
% \begin{itemize}
% 	\item Network access and external communication
% 	\item File system access outside the designated workspace
% 	\item Process spawning beyond allowed utilities
% 	\item Modification of system files or configurations
% \end{itemize}

\subsection{Integration with RL Training}

The Nano agent instantiates the history-based MDP formulation from Section~\ref{sec:mdp-history} as follows:

\begin{itemize}
	\item \textbf{History} $h_t$: The conversation transcript containing the issue description and all prior tool interactions
	\item \textbf{Assistant chunk} $y_t$: Model output containing \texttt{shell} commands or \texttt{apply\_patch} operations
	\item \textbf{Tool call} $c_t$: The parsed command extracted by $\psi(y_t)$ 
	\item \textbf{Tool output} $o_t$: Command results, file contents, or error messages
	\item \textbf{Return} $R(\tau)$: Terminal reward based on patch similarity to ground truth
\end{itemize}

The agent operates through the following cycle:
\begin{enumerate}
	\item Receives initial history $h_0$ containing the issue description
	\item Generates assistant chunk $y_t$ with tool calls to explore the codebase
	\item Executes first tool call $c_t = \psi(y_t)$ to produce output $o_t$
	\item Updates history $h_{t+1} = \text{append}(h_t, y_t, o_t)$
	\item Continues until patch submission or resource limits
	\item Receives terminal return $R(h_T)$ based on patch quality
\end{enumerate}

This cycle allows the model to learn from both successful and unsuccessful repair attempts, gradually improving its ability to navigate codebases and identify correct fixes.
The model applies an arbitrary sequence of apply\_patch operations in simplified search-replace format, then after task completion we compute the final diff instead of requiring the model to generate properly formatted patches directly.

This learning-centric approach avoids overfitting to tooling idiosyncrasies.
Rather than engineering away the difficulty of code repair through sophisticated tooling, we provide a stable, minimal interface and train the model to develop its own solutions through experience.

\subsection{Compute-Aware Agent Design}

Nano is designed to be compute-aware and operates within well-defined token, tool and time budgets.
This approach ensures:

\begin{itemize}
	\item \textbf{Predictable Memory Usage}: Token limits guarantee that conversation histories remain within \ac{GPU} memory constraints during training and inference
	\item \textbf{Predictable Rollout Time}: Tool call limits prevent runaway episodes
	\item \textbf{Timeout}: Upper limit on total time allotted 
\end{itemize}

\section{Experimental Setup}
\label{sec:experimental-setup}

\subsection{Datasets and Benchmarks}

Our experimental design carefully separates training and evaluation data to demonstrate true generalization rather than memorization.
We employ the following datasets:

\textbf{Training Dataset:} We use \ac{SWE-Gym}, a curated collection of approximately 2,400 Python bug-fixing tasks derived from real GitHub repositories.
Each task includes:
\begin{itemize}
	\item A containerized repository snapshot at the time of the bug report
	\item The original issue description
	\item The ground-truth patch that resolved the issue
	\item Isolated execution environments for safe agent interaction
\end{itemize}

\ac{SWE-Gym}'s containerized design makes it ideal for \ac{RL} training, as agents can freely explore and modify code without risk while receiving deterministic feedback based on their actions.

\textbf{Primary Evaluation:}
\ac{SWE-Bench-Verified} serves as our main evaluation benchmark, containing approximately 500 carefully validated Python bugs from popular open-source projects.
These bugs are notably more challenging than the training set, often requiring multi-file modifications and deep understanding of project structure.
This dataset tests whether our agent-trained models can generalize beyond their training distribution.

\textbf{Generalization Testing:}
To assess transfer of learned repair skills beyond the training domain, we evaluate on HumanEval, a standard code generation benchmark.
This tests whether debugging strategies learned through online \ac{RL} improve general programming capabilities.

\subsection{Pretrained Model Selection: Qwen for Tool Calling Excellence}

We base our experiments on the Qwen2.5-Coder family, specifically targeting both 7B and 32B parameter variants.
Qwen was selected based on extensive empirical evidence from recent literature demonstrating its superior tool-calling capabilities compared to other open-source alternatives.

\subsubsection{Tool Calling Performance}

Qwen models exhibit exceptional structured output generation, critical for reliable agent behavior:
\begin{itemize}
	\item \textbf{Function call accuracy}: Qwen consistently generates valid \ac{JSON} function calls with correct parameter types
	\item \textbf{Context retention}: Superior ability to maintain conversation state across multi-step tool interactions
	\item \textbf{Error recovery}: Robust handling of tool execution failures and adaptive strategy adjustment
	\item \textbf{Schema adherence}: Reliable conformance to function signatures and parameter constraints
\end{itemize}

\subsubsection{Code Understanding Capabilities}

Beyond tool calling, Qwen demonstrates strong foundational coding abilities:
\begin{itemize}
	\item State-of-the-art performance on HumanEval, MBPP, and CodeContests benchmarks
	\item Superior Python code generation and tool-calling capabilities
	\item Excellent repository-level understanding and cross-file reasoning capabilities
	\item Strong performance on debugging and code repair tasks in baseline evaluations
\end{itemize}

The model undergoes continued training rather than traditional fine-tuning, preserving its general capabilities while acquiring specialized agent skills through \ac{RL}.
This approach maintains Qwen's robust tool-calling foundation while enabling learning of complex debugging behaviors.

\section{Training-Inference Duality in Online RL for LLMs}
\label{sec:training-inference}

A fundamental innovation in our approach is the collapse of the traditional training-inference boundary.
Unlike conventional \ac{RL} where trajectory collection and policy updates occur in separate phases, our system performs both simultaneously through continuous serving and real-time weight updates.

\subsection{The Unified Training-Inference Paradigm}

The \ac{GRPO} process operates as a continuous cycle where:
\begin{enumerate}
	\item \textbf{Live inference}: vLLM serves the current policy to multiple agent workers simultaneously
	\item \textbf{Trajectory streaming}: Completed agent episodes stream to the training pipeline in real-time
	\item \textbf{Immediate updates}: \ac{GRPO} computes gradients and updates weights continuously
	\item \textbf{Live synchronization}: Updated weights propagate to the inference server without interruption
\end{enumerate}

This unified paradigm enables true online learning where the policy improves continuously throughout the training session, rather than in discrete update cycles.

\section{Trainer Implementation}
\label{sec:rl-algorithm}

We fork this \cite{vonwerra2022trl} \todoinline{write}

Parallel to our work, repositories like rLLM~\cite{rllm2025} have gotten very good \todoinline{write}

\subsection{Group Relative Policy Optimization (GRPO)}

We employ Group Relative Policy Optimization (\ac{GRPO})~\cite{shao2024deepseekmathpushinglimitsmathematical} as our primary \ac{RL} algorithm.
\ac{GRPO} offers several advantages:

\begin{itemize}
	\item \textbf{No value model required:} Unlike traditional actor-critic methods, \ac{GRPO} estimates advantages using relative performance within a batch, eliminating the need for a separate value network
	\item \textbf{Reduced variance:} By normalizing rewards across groups of trajectories, \ac{GRPO} provides more stable training signals
	\item \textbf{Simplified pipeline:} The absence of value estimation reduces computational overhead and implementation complexity
\end{itemize}

\subsection{Masked Loss Computation for Tool-Augmented RL} \label{sec:masked-loss-computation}

A key design consideration in training tool-augmented language models involves the treatment of externally-generated content during optimization.
Interactive agents must process and reason about tool outputs (shell command results, file contents, search results) to maintain coherent conversations, yet including these external tokens in the loss computation shifts optimization pressure away from the primary learning objective: making appropriate tool calls based on context.

Consider an agent trajectory $\tau = (s_1, a_1, o_1, s_2, a_2, o_2, \ldots, s_T)$ where $a_t$ represents agent actions (tool calls) and $o_t$ represents corresponding tool outputs.
While tool outputs are partially predictable given sufficient context, training the model to predict them deemphasizes the critical skill of contextual tool selection and invocation.
Traditional sequence modeling treats the entire trajectory as a prediction target, diluting the learning signal for agent reasoning and tool usage patterns.

\subsubsection{Dual-Mask Strategy for Loss Isolation}

We introduce a dual-masking approach that preserves contextual information during forward computation while focusing optimization on agent-authored content.
Let $\mathbf{x} = [x_1, x_2, \ldots, x_n]$ represent a tokenized trajectory and $\mathcal{T} \subset \{1, 2, \ldots, n\}$ denote the indices corresponding to tool-generated tokens.

\textbf{Forward Pass:}
The model processes the complete sequence with full attention: \begin{align}
	\mathbf{M}_{\text{att}} & = \mathbf{1}_{n \times n} \quad \text{(full attention mask)} \\
	\mathbf{h}              & = \text{Transformer}(\mathbf{x}, \mathbf{M}_{\text{att}})    \\
	\mathbf{L}              & = \text{LinearHead}(\mathbf{h})
\end{align}

where $\mathbf{h}$ contains representations informed by all tokens, including tool outputs.
Crucially, when computing log probabilities for agent-authored tokens, the model maintains full access to tool response content through the attention mechanism.

\textbf{Loss Computation:}
We apply selective masking to focus optimization exclusively on agent-authored tokens: \begin{align}
	\mathbf{M}_{\text{loss}}[i] & = \begin{cases}
		                                1 & \text{if } i \notin \mathcal{T} \text{ (agent-authored)} \\
		                                0 & \text{if } i \in \mathcal{T} \text{ (tool-generated)}
	                                \end{cases}                                                         \\
	\mathcal{L}_{\text{GRPO}}   & = \frac{\sum_{i=1}^{n} \mathbf{M}_{\text{loss}}[i] \cdot \ell_i \cdot A_i}{\sum_{i=1}^{n} \mathbf{M}_{\text{loss}}[i]}
\end{align}

where $\ell_i$ represents the per-token policy loss and $A_i$ denotes advantages computed via \ac{GRPO}.

\subsubsection{Perplexity Analysis and Training Focus}

Figure~\ref{fig:tool-perplexity-analysis} demonstrates the importance of masked loss computation through perplexity decomposition across agent trajectories.
Tool-generated tokens consistently exhibit higher perplexity as the model's predicted distributions often diverge from actual tool outputs—reflecting the difficulty of precisely predicting external system responses without complete environmental state.

% TODO: Create figure showing perplexity breakdown
\begin{figure}[ht]
	\centering
	\fbox{\parbox{0.8\textwidth}{\centering
			\textit{[Figure placeholder: Perplexity decomposition across agent trajectories showing tool-generated tokens contribute disproportionately to sequence-level perplexity.
						Left panel: trajectory timeline with color-coded regions (agent reasoning, tool calls, tool outputs).
						Right panel: per-token perplexity highlighting the spike at tool response boundaries where model predictions diverge from external content that, while partially predictable, is not the target learning objective.
					]}
		}}
	\caption{Perplexity analysis revealing the dominance of tool-generated tokens in sequence-level prediction difficulty.
		The majority of perplexity arises from tool responses, which while not entirely unpredictable, represent a different learning objective than the desired focus on contextual tool selection and agent reasoning.
	}
	\label{fig:tool-perplexity-analysis}
\end{figure}

Without selective masking, the optimization objective allocates significant capacity to predicting tool outputs rather than focusing on the primary training goal: learning to make appropriate tool calls and reason effectively about their results.

\subsubsection{Algorithmic Benefits}

\todoinline{by zeroing out the tool responses, we target the optimization on the tokens that matter.}

\textbf{Training Focus:}
Tool tokens contribute zero gradient magnitude, ensuring optimization pressure concentrates exclusively on agent reasoning and tool invocation patterns—the core competencies we seek to develop.

\textbf{Contextual Preservation:}
The forward pass maintains full attention over tool outputs, enabling coherent reasoning about external information while directing optimization toward appropriate tool usage rather than content prediction.

\textbf{Computational Efficiency:}
Loss masking integrates seamlessly with fused kernel implementations, avoiding additional forward passes or memory overhead during training.

\textbf{Objective Clarity:}
As tool outputs grow in length and complexity, the masking strategy ensures training remains focused on agent capabilities rather than becoming diluted across external content prediction tasks.

This masked loss formulation represents an important design choice for tool-augmented \ac{RL}, enabling models to leverage external information for reasoning while maintaining clear optimization objectives focused on contextual tool selection and effective agent behavior.

\subsection{Training Process}

Our training follows an iterative process where the agent attempts to solve bugs from the \ac{SWE-Gym} dataset:

\begin{enumerate}
	\item \textbf{Trajectory Generation:}
	      For each bug in a training batch, the agent generates a complete trajectory—from initial exploration through final patch submission \item \textbf{Reward Computation:} Each trajectory receives a reward based on the similarity between the generated patch and the ground-truth solution (detailed in Section~\ref{sec:reward-design}) \item \textbf{Advantage Estimation:} \ac{GRPO} computes advantages by comparing each trajectory's reward to the mean reward within its group \item \textbf{Policy Update:} The model parameters are updated to increase the likelihood of high-advantage actions while maintaining proximity to the reference policy through \ac{KL} regularization \end{enumerate}

\subsection{Hyperparameters}

Key hyperparameters for our \ac{GRPO} implementation include:

\todoinline{Cite GSPO, Dr.GRPO, DeepSWE, DAPO?
	for how hparams are selected. Can also "mine" the trainer/config.py for relevant stuff to add}

\begin{itemize}
	\item \textbf{Group Size:} 8
	\item \textbf{Effective Batch size:} gradients accumulated over 4 groups
	\item \textbf{Learning rate:} $1 \times 10^{-4}$, higher due to LoRA, no decay since that counteracts the "online" aspect of our training
	\item \textbf{\ac{KL} coefficient:} $\beta = 0.2$ for stability, omitting \acs{KL}-Div often leads to model collapse
	\item \textbf{Maximum trajectory length:} 12288 token sweet spot that maximizes GPU throughput
	\item \textbf{Training iterations:} 5,000 episodes covering the \ac{SWE-Gym} dataset
\end{itemize}

These values were selected through preliminary experiments to ensure stable convergence while allowing sufficient exploration of the action space.

\subsection{Large-Scale Compute Management}

Training and serving 8B to 32B parameter models for online \ac{RL} requires sophisticated compute optimization across the entire pipeline.

\subsubsection{Training Optimizations}

\textbf{LoRA:}
\todoinline{Move discussions on what LoRA buys us efficiency-wise here, discuss them tersely}

\textbf{FlashAttention 2:} \cite{dao2023flashattention2fasterattentionbetter}
3 only available on >H100

\textbf{DeepSpeed ZeRO Integration:}
\todoinline{briefly discuss both stage 2 and 3}
Complete model parameter, gradient, and optimizer state sharding enables training of 32B models across multiple \acp{GPU} with minimal memory overhead per device.

\textbf{Gradient Checkpointing:}
Selective activation checkpointing reduces memory usage by 50\% during backward passes, trading compute for memory to enable larger batch sizes and longer sequence lengths.

\textbf{Mixed Precision Training:}
BF16 training.

\textbf{\ac{KL}
	Divergence Computational Efficiency:} \ac{GRPO} training often requires computing \ac{KL} divergence between the current policy and a reference model to ensure training stability—preventing the model from deviating excessively from its original training objectives while optimizing for the new task.
This stability constraint typically necessitates maintaining two complete model instances in memory to compute $D_{\ac{KL}}(\pi_{\theta}(a|s) \| \pi_{\text{ref}}(a|s))$, effectively doubling \ac{GPU} memory requirements.
\ac{LoRA}'s additive parameter structure provides a natural solution to this computational burden: since base model weights $\mathbf{W}_0$ remain frozen while only low-rank adaptations $\mathbf{W}_0 + \mathbf{BA}$ are trained, the reference policy corresponds exactly to the base model without adapters.
We can therefore compute reference policy outputs by temporarily disabling the \ac{LoRA} adapters on the same model instance, eliminating the need for a separate reference model.
This approach reduces memory consumption by approximately 50\% while preserving the distributional constraints essential for stable policy optimization.

\subsubsection{Cost-Performance Trade-offs}

The combination of these optimizations enables training of state-of-the-art agent behaviors while maintaining practical computational budgets:

\todoinline{Make this a table with empirical metrics like: "backprop time", "rollout time"}
\begin{itemize}
	\item \textbf{~8B models:} Trainable on 2-3x A100 \acsp{GPU} with DeepSpeed ZeRO 2 and LoRA
	\item \textbf{~14B models:} Trainable on 3x A100 \acsp{GPU} with DeepSpeed ZeRO 2 and LoRA
	\item \textbf{~32B models:} Trainable on 6x A100 \acsp{GPU} DeepSpeed ZeRO 3 and LoRA
\end{itemize}

Stacking these optimizations make \ac{RL} practically feasible for academic research, democratizing access to advanced coding agent training previously available only to industry labs with unlimited compute budgets.

\section{Reward Design}
\label{sec:method-reward-design}

\todoinline{Add discussion on the problem / necessity of reward sparsity / std}
\todoinline{Say that we tried a range of different rewards but converged on doing only patch similarity}

\subsection{Outcome-Based Patch Similarity}

Our reward function inspired by the method proposed in SWE-RL~\cite{wei2025swerladvancingllmreasoning} compares the generated diff to the repository with the ground truth using \textit{Python's SequenceMatcher}.
\todoinline{elaborate / improve}

\begin{itemize}
	\item \textbf{Simplicity:}
	      Evaluating final patches is straightforward and deterministic \item \textbf{Flexibility:} Agents can discover diverse problem-solving strategies without being constrained by process-specific rewards \item \textbf{Alignment:} The reward directly measures what we care about—correct bug fixes \end{itemize}

\subsection{Multi-Component Reward Computation}

Our reward function decomposes the complex task of bug fixing into three distinct components, each capturing a different aspect of repair quality:

\begin{equation}
	R_{\text{total}} = 0.2 \cdot R_{\text{files}} + 0.4 \cdot R_{\text{functional}} + 0.4 \cdot R_{\text{testing}}
\end{equation}

where each component evaluates a specific dimension of the agent's solution:

\textbf{File Targeting Component} ($R_{\text{files}} \in [0,1]$): Evaluates whether the agent modified the correct files that should be changed to fix the bug.
This component rewards the agent for identifying the appropriate locations in the codebase, regardless of the specific changes made.

\textbf{Functional Similarity Component} ($R_{\text{functional}} \in [0,1]$): Measures how similar the agent's changes are to the ground-truth solution in terms of the functional aspects of fixing the bug.
This component focuses on whether the core logic changes align with the expected repair strategy.

\textbf{Testing Alignment Component} ($R_{\text{testing}} \in [0,1]$): Assesses how well the agent's changes align with the testing suite additions or modifications in the ground truth, which monitor the bug and ensure correctness while preventing regression.

This decomposition reflects the multi-faceted nature of software debugging: successful repair requires not only identifying the right locations and implementing functional fixes, but also considering how changes interact with the broader testing infrastructure that validates correctness.

This reward computation approach addresses a fundamental challenge in \ac{LLM}-based code modification: the notorious difficulty of generating syntactically valid diffs.
Traditional approaches require models to produce unified diff format with precise line numbering, context coordination, and complex formatting rules—a task that frequently results in parsing errors and invalid patches.

Our approach elegantly separates semantic understanding from syntactic formatting.
During interaction, the agent performs any number of \texttt{apply\_patch} operations using a simplified search-replace format that aligns naturally with \ac{LLM} capabilities.
Each operation specifies exact text to replace and its substitution, without requiring knowledge of line numbers or diff syntax.

Only after the complete interaction do we execute \texttt{git diff} to compute the actual patch for evaluation.
This separation of concerns offers several advantages:

\begin{itemize}
	\item \textbf{Error Elimination}: Models cannot fail due to diff formatting issues, as the patch is computed automatically
	\item \textbf{Semantic Focus}: Agents concentrate entirely on identifying and fixing bugs rather than wrestling with syntax requirements
	\item \textbf{Perfect Formatting}: Git ensures all patches are properly formatted and applicable
	\item \textbf{Complete Information}: Every intended modification is captured accurately for reward computation
\end{itemize}

This architectural choice transforms a major source of technical failures into a reliable, automated process while preserving full fidelity of the agent's debugging intentions.

\subsection{Addressing Sparse Rewards}

The sparse nature of exact patch matching presents challenges for RL training.
We address this through:

\begin{itemize}
	\item \textbf{Curriculum learning:}
	      Starting with simpler bugs where rewards are more attainable \item \textbf{Partial credit:} Rewarding patches that target correct files and functions even if the exact fix differs \item \textbf{Large batch sizes:} Ensuring sufficient positive examples within each training batch \end{itemize}

\subsection{Future Extensions}

While our current approach uses patch similarity for computational efficiency, the framework naturally extends to test-based evaluation.
Future work could incorporate:

\begin{itemize}
	\item Execution of project test suites to verify functional correctness
	\item Multi-objective rewards, training on a wide set of tasks to improve generalization
\end{itemize}

These extensions would require significant infrastructure investment but could lead to more robust and generalizable repair capabilities.

\section{Long context}
\label{sec:long-context}
\textbf{Computational Scaling Challenges}: Agent interactions are orders of magnitude more expensive than simple text generation:
\begin{itemize}
	\item Each training step involves multiple model calls (often 10-50 per trajectory)
	\item Memory requirements scale with trajectory length
\end{itemize}

\section{High-Performance Training Infrastructure}
\label{sec:method-training-infrastructure}

\subsection{vLLM}

Our inference layer leverages \ac{vLLM}'s powerful inference engine to handle the demanding requirements of online \ac{RL} workloads.
\ac{vLLM}'s KV-cache management, continuous batching, and PagedAttention~\cite{kwon2023efficientmemorymanagementlarge} collectively enable fast, efficient serving of asynchronous multi-turn agents.
Crucially, between model weight updates, prefix caches deterministically hit, providing substantial performance boosts during the intervals when agents operate with stable model parameters.

To support online training, we extended \ac{vLLM}'s OpenAI-compatible \ac{API} server with live weight synchronization (hot-swapping) so updates take effect without service interruption (see \ref{sec:nccl-sync}).

\subsubsection{Logit divergence problem}
Motivates GSPO

\subsection{NCCL-Based Live Weight Synchronization} \label{sec:nccl-sync}

A critical technical breakthrough enabling our training-inference duality is real-time weight synchronization between the training pipeline and inference servers using NVIDIA Collective Communications Library (NCCL).

\subsubsection{Technical Architecture}

The weight synchronization system operates through several components:

\textbf{NCCL Broadcast Groups:} Training nodes broadcast updated weights to inference servers using optimized collective communication primitives.
This ensures minimal latency and maximum bandwidth utilization.

\textbf{Differential Updates:}
\todoinline{Write more here about "sequential gathering", reducing peak GPU VRAM reqs by ~$500$x}
Only changed parameters (the LoRA adapter weights) are broadcast, dramatically reducing network traffic and update latency.

\subsubsection{Performance Characteristics}

The performance characteristics of our NCCL-based weight synchronization system demonstrate significant engineering achievements:

\textbf{Update Latency}: Weight broadcasts complete in 150-300ms for LoRA adapters (8-64 rank), enabling near real-time policy improvements.

\textbf{Network Utilization}: Differential updates reduce communication by 95\% compared to full weight synchronization, utilizing only 100-500MB per update versus 50GB+ for complete models.

Several technical challenges were overcome to achieve these performance levels:

\textbf{\ac{CUDA} Memory Management}: Custom memory pools coordinate between training and inference processes, preventing out-of-memory conditions during concurrent operation.

\textbf{Scalability}: The system scales linearly to multiple inference nodes, with broadcast trees minimizing update latency as cluster size increases.
Production deployments have demonstrated stable operation with up to 8 inference nodes receiving synchronized updates.

This infrastructure represents a significant advance in making online RL practical for large language models, reducing the traditional barrier between training and deployment while maintaining production-grade reliability and performance.

This NCCL-based approach represents a significant engineering achievement, enabling truly continuous learning where model improvements benefit ongoing inference within seconds of gradient computation.

\subsection{Integrated System Architecture}

The complete online \ac{RL} system consists of several interconnected components operating in parallel:

\textbf{vLLM Inference Cluster:} 1 $80$GB A100 for $8$B/$14$B models, 2 for $~32$B
\textbf{TRL Training Cluster:} 2 $80$GB A100 for $8$B/$14$B models, 4 for $~32$B, launches agents, posts weight synchronizations

\subsection{Implementation Details}

Key implementation choices include:

\begin{itemize}
	\item \textbf{Collective RPC:}
	      For efficient weight sharing across distributed processes \item \textbf{LoRA adaptation:} Optional use of low-rank adapters to reduce communication overhead \item \textbf{Containerized environments:} Each agent runs in an isolated Docker container for safety \item \textbf{Request batching:} Multiple agent requests are processed concurrently for efficiency \end{itemize}

Figure~\ref{fig:system-architecture} presents the complete system architecture, illustrating data flow between the vLLM inference cluster, distributed agent worker pool, and GRPO training pipeline.
The diagram highlights the NCCL communication layer that enables real-time weight synchronization, supporting the training-inference duality that distinguishes our approach from conventional RL systems.

\section{Evaluation Methodology}
\label{sec:evaluation}

\subsection{Evaluation Metrics}

We assess model performance using several complementary metrics:

\begin{itemize}
	\item \textbf{Success Rate:}
	      Percentage of bugs for which the agent generates an exactly correct patch \item \textbf{Partial Success Rate:} Including fixes that target the correct location with minor differences \item \textbf{Time to Solution:} Average wall-clock time required to generate a fix \item \textbf{Exploration Efficiency:} Number of files examined and commands executed per bug \end{itemize}

\subsection{Baseline Comparisons}

To contextualize our results, we compare:

\textbf{Base Model:} The same models before and after training

\textbf{State-of-the-art Models:} See how frontier models instead

\subsection{Generalization Testing} Beyond in-domain performance, we evaluate generalization through: \todoinline{to MultiSwe?
to Tau?}

\subsection{Statistical Significance}
\todoinline{Should we have something like this?}
