\chapter{Introduction}
\label{ch:introduction}

Automated program repair has rapidly transformed from a nascent research area to one of the most promising applications of agentic \ac{AI} systems.
By ``agentic,'' we refer to systems that can autonomously interact with environments through tool use, make sequential decisions based on observations, and iteratively refine their approach based on feedback, as opposed to single-pass generation models that produce outputs without environmental interaction.
Recent advances in autonomous software engineering have achieved remarkable performance milestones, with systems like Claude~4 now exceeding 70\% success rates on SWE-Bench-Verified \cite{claude4_2025}.
This rapid progress underscores the critical importance of maintaining open research capable of understanding frontier capabilities.
A broad consensus has emerged among researchers: competitive access to training methodologies is essential to ensure that critical insights remain within the scientific community rather than concentrated in proprietary systems.
Recent open weight releases such as Kimi-K2~\cite{kimiK2_2025} and Qwen3-Coder~\cite{qwen3CoderBlog2025} follow closely behind frontier performance, but do not include full training recipes.
This reinforces the need for open, reproducible methods and analysis.
\todoinline{Verify SWE-Bench-Verified SOTA numbers at submission time as frontier performance continues to evolve rapidly.}

This thesis directly addresses this need by investigating how a single, minimalist agent—Nano—enables a model to acquire agentic behavior through experiential \ac{RL} feedback occurring entirely within the agent's interactive loop.
By situating all learning and adaptation within the Nano harness, we provide clear, reproducible insights into the mechanisms by which \ac{RL} cultivates autonomy and effective tool use in repository-level code repair.

An important contemporaneous effort is SWE-RL~\cite{wei2025swerladvancingllmreasoning}, which applies \ac{GRPO} with patch-similarity rewards (using Python's \texttt{difflib.SequenceMatcher}) in a static, single-turn setting: full file contexts are provided, the model outputs a patch directly, and similarity to ground truth defines the reward signal.
Our work is complementary: we also use execution-free patch-similarity rewards but combine them with \ac{GSPO}—a more robust sequence-level variant—in an agentic, multi-turn regime where the model must first acquire relevant context through tool use before proposing edits.
This distinction between static one-shot patch generation and interactive context acquisition motivates our focus on agent-integrated training within a single minimalist harness rather than prompt-time scaffolding engineering.

The necessity and transformative potential of agentic training emerge from examining the historical limitations of supervised learning for code repair and the foundational role of \acp{LLM} in enabling \ac{RL} for complex tasks.

\section{Background}
\label{sec:intro-background}

Traditional systems for automated code repair have relied predominantly on supervised learning from static datasets of bug-fix pairs.
In this paradigm, models passively observe input-output mappings between broken and corrected code, treating bug fixing as a pattern recognition task rather than engaging with the full interactive software engineering process.
While computationally efficient, this approach fundamentally misrepresents the nature of repository-level software debugging, which involves iterative hypothesis refinement, strategic exploration of entire codebases, execution of diagnostic commands, and contextual interpretation of error signals across multiple rounds of investigation.
Moreover, the information needed to fix a bug in real-world repositories is rarely contained within the bug-fix pair itself, necessitating the interactive capabilities that coding agents provide to navigate and understand the broader codebase context.
\todoinline{Add citation to traditional APR survey or seminal work (e.g., GenProg, Prophet, or APR survey paper).}

The question becomes: how do we train such agentic systems when the very complexity that necessitates their existence also makes traditional training approaches intractable?
\ac{RL} has achieved remarkable results in well-structured domains like Go, where clear success metrics exist and every action represents a valid move within the game's rules \cite{alphaGo2016}.
However, \ac{RL} fundamentally requires that learning systems can meaningfully participate in the task from the beginning.
\todoinline{OR that we can heuristically reward them.
Not feasible for SWE.
}
They must be capable of generating valid actions, interpreting feedback, and understanding the basic structure of the problem space.
For complex reasoning tasks like software debugging, this prerequisite participation threshold has historically been insurmountable.

\acp{LLM} have altered this fundamental constraint by providing the world knowledge and reasoning capabilities necessary to attempt tasks of unprecedented complexity.
When equipped with appropriate interfaces, these models can navigate file systems, interpret error messages, execute diagnostic commands, and generate syntactically valid code modifications.
This capability foundation transforms previously intractable problems into domains where \ac{RL} becomes feasible.
In effect, \acp{LLM} provide the first rung on the ladder that makes applying \ac{RL} to software engineering feasible.

However, unlike traditional \ac{RL} agents that directly produce discrete actions from observations (e.g., chess moves, joystick inputs), \acp{LLM} fundamentally operate as text generators.
They cannot directly perceive environments or execute actions; they require an intermediary translation layer that converts generated text into executable actions and translates environmental responses into textual observations.

This architectural distinction necessitates a different approach to \ac{RL}.
In this paradigm, a lightweight harness interprets tool calls from the \ac{LLM}, executes corresponding actions in the environment, and returns textual observations for processing.
The entire system—\ac{LLM} plus harness—forms the \ac{RL} agent, with learning occurring through this mediated interaction cycle.
This thesis instantiates exactly one such harness (the Nano agent) and conducts all training strictly within this single-agent setting.

When equipped with appropriate harnesses, \acp{LLM} can navigate file systems, execute terminal commands, interpret error messages, and generate syntactically correct code modifications.
This capability transformation makes it practical to apply \ac{RL} to real-world software engineering tasks, as models can now generate the rich interaction trajectories required for policy gradient estimation.
Terminal-oriented agency benchmarks such as TauBench~\cite{taubench2024} provide standardized evaluation frameworks for such capabilities; our cross-scaffold transfer experiments (\cref{sec:rq3-scaffold}) further explore generalization across different harness interfaces.

Training adopts an execution-free approach: the agent executes shell commands to explore repositories but does not run test suites or build systems.
Rewards derive solely from comparing the agent's final repository modifications to ground-truth patches via static string similarity.
This trades direct functional verification for infrastructure simplicity and determinism, enabling scalable multilingual training while maintaining alignment with valid tool use, efficiency, and ultimately, repair success.
Concrete observation, action, and reward definitions are specified in \cref{ch:method}.

The significance of this paradigm shift extends beyond technical feasibility.
Human software developers do not repair bugs through single-shot pattern matching; they engage in exploratory processes involving hypothesis formation, incremental information gathering, and iterative refinement.
They navigate complex codebases, examine interconnected components, execute diagnostic commands, and progressively construct mental models of system behavior.
This active learning process differs fundamentally from passive pattern recognition, suggesting that training models through interactive experience may yield qualitatively superior debugging capabilities.

%The current landscape of automated program repair reflects an artificial fragmentation: separate models for Python, distinct approaches for Java, isolated systems for JavaScript.
%This language-specific isolation stems not from fundamental differences in debugging strategies but from the practical constraints of execution-based evaluation.
%Test-based rewards require language-specific test runners, build systems, package managers, and dependency resolution, creating insurmountable engineering barriers to unified training.
%Our execution-free approach dissolves these barriers, enabling the first practical path toward universal debugging agents that operate across language boundaries.

\todoinline{Consider expanding: Our execution-free problem setup has significantly lower engineering friction compared to test-based approaches.
No need for language-specific test runners, build systems, dependency management, or containerized execution environments.
This enables seamless multilingual training that would be prohibitively complex with execution-based rewards.
}

This direction aligns with widely observed advances at the frontier: agentic capabilities are clearly improving, with public reports of stronger coding performance and enhanced interactive capabilities.
These developments heighten the need for open, reproducible understanding of how agentic training works, rather than reliance on closed systems.
Our work addresses this need by studying \ac{RL} with interactive coding agents in an open setting.
\todoinline{Add citations to public system cards or technical reports where appropriate.}

\section{Problem}
\label{sec:intro-problem}

Tool-using coding agents already exist; what remains largely unavailable to the open scientific community is a reproducible training pipeline that develops agentic behavior through interaction.
We target a minimalist, execution-free \ac{RL} recipe for repository-level \ac{APR} that (i) optimizes shell-based repository interaction using static patch-similarity rewards, (ii) scales across programming languages without test infrastructure or language-specific engineering, and (iii) demonstrates transfer from training benchmarks to held-out evaluations and across base model families.

Execution-free \ac{RL} represents a pragmatic trade-off: by replacing test execution with deterministic patch comparison, large-scale multilingual experiments become feasible with controlled variance and full reproducibility.
The scientific question becomes whether such training can improve repository navigation, reduce invalid tool calls, and enhance repair efficiency while transferring across datasets and model architectures.

\section{Purpose}
\label{sec:intro-purpose}

This thesis develops and evaluates an open, execution-free \ac{RL} training recipe for a single minimalist coding agent (Nano).
The emphasis lies on training practice and empirical evidence rather than novel scaffolding or algorithmic contributions: we document a complete, transparent training procedure and analyze its effects on interactive behavior.
We demonstrate that static patch-similarity rewards can improve repository navigation and repair attempts without test execution, and that the resulting behaviors transfer across base models and from training benchmarks to held-out evaluations.
Methodological clarity and reproducibility guide all design choices: complete configurations, harness specifications, and evaluation protocols are provided, with multilingual scalability achieved through language-agnostic reward formulation rather than per-language engineering.

\todoinline{Tighten scope/claims to match finalized results; add pointers to Sections/Figures once numbers are fixed.}

% Despite growing evidence that leading \ac{AI} labs employ experiential training—OpenAI's o1~\cite{openAI_o1_2024}, Anthropic's Claude~\cite{anthropic2024}, and Cognition's Devin~\cite{cognition2024} all exhibit behaviors suggesting such training—no open-source implementation has been available to the research community.

% By open-sourcing this infrastructure, we aim to democratize access to advanced training techniques previously available only to well-resourced industry labs, enabling broader research into agentic \ac{AI} systems.

% The significance extends beyond code repair: online training could transform how models learn any task requiring environmental interaction, from scientific experimentation to robotic control.
% This thesis demonstrates its viability in the well-scoped domain of automated debugging, paving the way for broader applications.

% Current research in this area is primarily conducted by industry labs (OpenAI, Anthropic, Cognition Labs) with limited open-source replication, creating a significant knowledge gap in the academic community.

\section{Goal}
\label{sec:intro-goal}

% This thesis makes several significant contributions to the field of automated program repair and \ac{RL} for code generation:

% \textbf{Novel Training Paradigm}: We introduce and implement \ac{RL} for code repair, wherein autonomous agents interact with real software environments during training.
% This approach represents a fundamental departure from static supervised learning, enabling models to learn debugging strategies through direct experience.
% Our implementation demonstrates the feasibility of this paradigm at scale, handling the computational and engineering challenges of simultaneous training and environment interaction.

% \textbf{Nano Agent Implementation}: We develop a minimalist "Nano agent" that utilizes only essential terminal commands and file operations, demonstrating that sophisticated debugging behaviors can emerge from simple interfaces when combined with \ac{RL}.
% This clean-slate approach validates that effective code repair capabilities arise from learning rather than engineered complexity.

% \textbf{Technical Infrastructure}: We develop a comprehensive training infrastructure that enables efficient \ac{RL} with coding agents.
% This includes a custom integration layer between vLLM inference servers and \ac{RL} trainers, real-time weight synchronization via \ac{NCCL} for live policy updates, and optimizations enabling training of models up to 32B parameters with limited computational resources.
% The infrastructure supports both Group Relative Policy Optimization (\ac{GRPO})~\cite{grpo2024} and can be extended to other policy gradient methods.

% \textbf{Empirical Validation}: Through extensive experiments on the \ac{SWE-Gym} training environment and \ac{SWE-Bench-Verified} evaluation benchmark, we demonstrate that this \ac{RL} approach produces monotonic improvements in bug-fixing performance.
% This result, achieved in an open-source setting, represents significant progress toward practical autonomous debugging systems and validates the potential of experiential training approaches.

% \textbf{Open Science Contribution}: All code, training configurations, and evaluation protocols are released as open-source contributions to the research community.
% This includes the Nano agent implementation and the \ac{RL} training pipeline.

The overarching goal is to develop and empirically validate an online \ac{RL} training paradigm for automated program repair, organized around three research questions that structure the investigation:

\textbf{RQ1: How does \ac{GSPO} training improve Nano harness adaptation?
}
\begin{itemize}
	\item Primary indicators: tool-call success rate, invalid-call reduction, action efficiency, and command usage evolution over training.
	\item Analysis: pre-training vs. post-training comparison of harness-level efficiency metrics and qualitative examination of changes in command usage patterns.
	\item Hypothesis: \ac{RL} training improves harness interaction efficiency and tool-use reliability, with observable shifts in command patterns toward more focused debugging strategies.
\end{itemize}

\textbf{RQ2: Does execution-free patch-similarity \ac{RL} training improve SWE-Bench-Verified performance?}
\begin{itemize}
	\item Primary metric: test-verified success rate on SWE-Bench-Verified (approximately 500 Python debugging tasks).
	\item Comparison: pre-training baseline vs. post-training performance using identical evaluation protocol.
	\item Hypothesis: training on static patch-similarity rewards improves functional bug-fixing success despite not directly optimizing for test passage.
\end{itemize}

\textbf{RQ3: Does the execution-free multilingual curriculum generalize beyond Python?}
\begin{itemize}
	\item Primary evaluation: per-language reward improvements on a 50-task SWE-Bench-Multilingual holdout spanning nine programming languages.
	\item Analysis: bootstrap confidence intervals for per-language reward deltas, comparing pre-training and post-training checkpoints.
	\item Hypothesis: execution-free patch-similarity rewards generalize across programming languages, with measurable improvements on held-out multilingual tasks despite limited non-Python training data.
\end{itemize}

\section{Benefits, Ethics and Sustainability}
\label{sec:intro-benefits}

This work contributes to open research in a field dominated by closed frontier labs by releasing methods, code, and evaluation protocols in an open format.
All implementation details needed to reproduce results are documented, including configuration files, prompts, and harness interfaces.
We use standard datasets with clear licensing and provide exact commit hashes for benchmarks and dependencies.
To enable critical scrutiny, we specify evaluation scripts and random seeds, and we will archive artifacts to ensure long-term accessibility.
By reducing barriers to entry and facilitating independent replication, the project strengthens transparency and community-driven progress in automated program repair.

Ethical considerations include potential misuse of agentic systems and training data provenance.
Experiments use only permissively licensed repositories, avoiding exposure of sensitive credentials or private data.
Episodes execute with restricted file access and process-level isolation mechanisms to prevent host system compromise, though future work should explore stronger containerization for enhanced security.

Compute utilization per experiment is monitored and reported (tokens processed, \ac{GPU}-hours) to support reproducibility and minimize unnecessary energy consumption.
Efficiency practices include mixed-precision training (BF16), checkpoint reuse, shared inference servers through training-inference duality, and targeted ablations to eliminate redundant experiments.

\section{Methodology}
\label{sec:intro-methodology}
This research employs a rigorous experimental methodology to systematically evaluate \ac{RL} for repository-level bug repair through online agent training.

We employ \ac{GSPO}~\cite{gspo2025}, a sequence-level policy optimization algorithm that extends the group-relative baseline principle with improved importance weighting for variable-length sequences.
\ac{GSPO} demonstrates superior robustness compared to earlier group-relative methods (\ac{GRPO}, DAPO), proving particularly effective for small effective batch sizes and high-variance rewards characteristic of multi-turn agent training under academic compute constraints.

Training proceeds through iterative online interaction: agents explore repositories, attempt repairs, and receive rewards based on static patch-similarity comparison between their repository modifications and ground-truth patches.

Primary comparisons report pre-training baseline vs.
post-training performance on SWE-Bench-Verified using the same Nano harness for both evaluations, eliminating confounds from supervised fine-tuning or prompt engineering.
Measured outcomes include repair success rates, reward progression, tool-call validity, action efficiency, and command usage patterns.

Data collection captures complete agent-environment interactions: tool invocations, repository states, reward signals computed via patch-similarity comparison, training dynamics, and aggregated evaluation metrics.
Training uses a 1,000-task curriculum (750 SWE-Gym Python tasks + 250 SWE-Bench-Multilingual tasks), with primary evaluation on SWE-Bench-Verified and multilingual generalization assessed on a 50-task held-out subset.

This methodology enables quantitative validation and mechanistic insights into how \ac{RL} shapes interactive debugging behavior through the Nano agent.

\section{Stakeholders}
\label{sec:intro-stakeholders}

Primary stakeholders include researchers investigating \ac{RL} training for coding agents, those seeking reproducible methodologies for agentic \ac{LLM} training, and the open-source community advancing alternatives to proprietary training recipes.
Secondary stakeholders include infrastructure providers supporting large-scale academic training, and benchmark maintainers whose repositories enable evaluation.
Methods and releases prioritize utility for open research while addressing privacy, security, and sustainability considerations.

\section{Delimitations}
\label{sec:intro-delimitations}

This investigation operates within carefully defined boundaries that balance scientific rigor with practical constraints.
Understanding these delimitations is essential for interpreting our findings and their applicability to broader contexts.

\textbf{Task Scope}: This work addresses automated bug repair where known errors exist and the objective is patch generation.
Evaluation focuses on the Nano agent operating within its native harness, while broader code-generation benchmarks (HumanEval, MBPP) remain out of scope.
This focused investigation enables deep analysis of interactive debugging behavior while acknowledging that specification-based program synthesis constitutes orthogonal research.

\textbf{Language and Environment Constraints}: Training uses a 1,000-task curriculum (750 SWE-Gym Python issues + 250 SWE-Bench-Multilingual issues) curated to take advantage of our programming language agnostic approach.

\textbf{Compute Environment Constraints}: Experiments execute on Berzelius and Alvis Swedish national \ac{GPU} clusters using SLURM scheduling without Docker-based container orchestration support.
This infrastructure constraint shaped both Nano agent design and training pipeline architecture, favoring SLURM-compatible parallelization and process-level isolation over container-based approaches.

\textbf{Model Architecture Scope}: Our primary development and analysis focuses on Qwen3-14B—a hybrid reasoning model with strong tool-calling capabilities—with reasoning explicitly disabled to conserve context window budget in multi-turn episodes.
Limited comparisons with Qwen3-8B, Qwen3-30B-A3B, Qwen3-32B, and Llama3.1-8B appear in the Appendix to illustrate capacity and architecture effects, but full model ablation studies are left for future work.

\textbf{Evaluation Protocol Boundaries}: Rewards derive from automated patch-similarity comparison rather than test suite execution, trading functional verification for computational tractability and reproducibility.
This approach aligns with established benchmarks and enables large-scale multilingual training, though it may undervalue functionally equivalent patches with syntactic divergence from ground truth.
Evaluation emphasizes single-commit bug repairs; multi-stage refactoring and architectural changes remain out of scope.

\textbf{Agent Implementation}: The Nano agent adopts a deliberately minimalist design, exposing only essential shell commands and file patching operations (two tools: \texttt{shell}, \texttt{apply\_patch}).
This simplicity enables clean experimental isolation of \ac{RL} effects, though richer tool sets might exhibit different performance-complexity trade-offs.

\textbf{Methodological Boundaries}: We briefly explored distillation from proprietary models via \ac{SFT} but abandoned this direction due to limited empirical gains and philosophical misalignment with open research objectives focused on training from environmental interaction rather than imitating closed systems.

\section{Outline}
\label{sec:intro-outline}

The remainder of this thesis progresses from theoretical foundations through empirical validation to conclusions:

\textbf{Chapter 2: Background and Related Work} reviews policy optimization algorithms for language models (\ac{PPO}, \ac{GRPO}, DAPO, \ac{GSPO}), situates \ac{APR} within repository-level evaluation paradigms, and contrasts agentless vs.
agentic approaches to motivate our execution-free, agent-integrated training methodology.

\textbf{Chapter 3: Methodology} formalizes the Nano agent (observation/action spaces, interaction loop, termination), specifies training data (SWE-Gym, SWE-Bench-Multilingual), defines the patch-similarity reward formulation, describes \ac{GSPO} policy optimization with masked loss, and outlines the evaluation protocol addressing RQ1-RQ3.

\textbf{Chapter 4: Implementation} documents the practical realization: vLLM-based serving with tool calling, live adapter synchronization via \ac{NCCL}, trainer extensions for multi-turn \ac{GSPO}, SLURM orchestration, compute-efficient infrastructure (\ac{LoRA}, DeepSpeed ZeRO, gradient checkpointing, BF16), and training protocol execution.

\textbf{Chapter 5: Results} presents experimental findings: harness adaptation metrics and behavioral changes (RQ1), performance comparisons on SWE-Bench-Verified (RQ2), multilingual generalization on the held-out subset (RQ3), training dynamics and convergence, and qualitative trajectory examination with statistical analyses.

\textbf{Chapter 6: Conclusions} synthesizes findings, discusses design philosophy and lessons learned (system complexity, reward misspecification, infrastructure challenges), examines broader implications for agentic \ac{AI}, identifies limitations, and proposes future research directions for \ac{RL}-trained coding agents.
