\chapter{Introduction}

Automated program repair represents one of the most formidable challenges in software engineering, with profound implications for development productivity and software reliability. Despite remarkable advances in large language models (LLMs), state-of-the-art systems achieve less than 20\% success rates on rigorous benchmarks such as SWE-Bench \cite{sweBench2024}, highlighting a significant gap between current capabilities and practical requirements. This limitation persists despite the availability of extensive training data and increasingly sophisticated model architectures, suggesting that fundamental methodological innovations are necessary to achieve meaningful progress.

The traditional paradigm for training code repair models relies on supervised learning from static datasets of bug-fix pairs, wherein models passively observe input-output mappings without experiencing the interactive, exploratory process that characterizes human debugging. This approach, while computationally efficient, fails to capture the essential dynamics of software debugging: the iterative refinement of hypotheses, strategic exploration of codebases, and contextual interpretation of error signals. Consequently, models trained through passive observation struggle to generalize beyond pattern matching to genuine problem-solving.

The recent convergence of large language models and reinforcement learning presents an unprecedented opportunity to transcend these limitations. AlphaZero's superhuman game performance \cite{alphaZero2018} initially seemed like a significant step toward artificial general intelligence, but it exposed a critical constraint: games provide reward signals for every legal move, while real-world tasks like programming require deep conceptual understanding merely to participate meaningfully. Before LLMs, applying RL to tasks like coding was futile. Our models couldn't even attempt such problems. The issue wasn't that RL algorithms were broken; they simply had no gradient to climb.

LLMs cracked this problem by providing the prerequisite knowledge to attempt real tasks coherently. Now when an RL agent tries to fix a bug, it can at least run valid commands and read error messages, generating the rich learning signals that RL algorithms need. For the first time, we can apply decades of RL research to problems that actually matter, because LLMs have given us that crucial first rung on the ladder.

However, unlike traditional reinforcement learning agents—such as those trained for game playing—that directly produce actions from observations (e.g., selecting chess moves or joystick inputs), LLMs fundamentally operate as token generators. They cannot directly perceive environments or execute actions; instead, they require an intermediary translation layer that converts their generated tokens into executable actions and translates environmental responses back into text.

This critical distinction necessitates what we term "tool-mediated" reinforcement learning. In this paradigm, a scaffold or harness interprets structured token outputs from the LLM (such as function calls or commands), executes these as actions in the environment, and returns textual observations that the LLM can process. The entire system—LLM plus scaffold—forms the RL agent, with learning occurring through this mediated interaction cycle.

This approach has gained particular prominence following dramatic improvements in model agency capabilities observed throughout 2024. Systems like Claude 3.5 Sonnet demonstrated unprecedented coding performance (improving from 33.4\% to 49.0\% on SWE-Bench Verified) and introduced revolutionary "computer use" capabilities, while OpenAI's o1 showed remarkable reasoning improvements through reinforcement learning techniques. These breakthroughs likely reflect large-scale deployment of similar "tool-mediated" training approaches by major AI laboratories, validating the potential of this paradigm for developing genuinely agentic AI systems.

When equipped with appropriate scaffolds, LLMs can navigate file systems, execute terminal commands, interpret error messages, and generate syntactically correct code modifications. This capability transformation enables, for the first time, the application of reinforcement learning to real-world software engineering tasks, as models can now generate the rich interaction trajectories required for policy gradient estimation.

The significance of this paradigm shift extends beyond mere technical feasibility. Human software developers do not repair bugs through single-shot pattern matching; rather, they engage in sophisticated exploratory processes involving hypothesis formation, incremental information gathering, and iterative solution refinement. They navigate complex codebases, examine multiple interconnected components, execute diagnostic commands, and progressively construct mental models of system behavior. This active learning process, fundamentally different from passive pattern recognition, suggests that training models through interactive experience could yield qualitatively superior debugging capabilities.

Recent investigations into agent-based approaches have demonstrated the potential of tool-augmented LLMs for software engineering tasks \cite{openHands2024, aider2024}. However, these systems typically employ a two-stage approach: first training models on static datasets through supervised learning, then deploying them as agents with access to various tools and interfaces. This training-deployment mismatch potentially limits their effectiveness, as models never learn to optimize their tool usage or develop exploration strategies during training. The disconnect between the passive training regime and the active deployment environment represents a fundamental limitation that our work addresses.

\section{Problem Statement}

The central challenge addressed in this thesis concerns the fundamental disconnect between the training methodologies employed for code repair models and the interactive, exploratory nature of real-world debugging. Contemporary approaches to automated program repair exhibit a critical limitation: models trained on static datasets of bug-fix pairs are subsequently expected to perform dynamic, multi-step reasoning in complex software environments. This paradigm mismatch constrains their effectiveness and generalization capabilities.

We identify three specific limitations that characterize current approaches:

\textbf{Exploration Capability Deficit}: Models trained exclusively on static data lack the ability to develop effective exploration strategies. Unlike human developers who actively navigate codebases, execute diagnostic commands, and iteratively gather contextual information, these models never experience the exploratory aspects of debugging during training. This results in agents that, when deployed, exhibit suboptimal information-gathering behaviors and fail to leverage available tools effectively.

\textbf{Temporal Abstraction Mismatch}: The predominant training paradigm assumes bug repair can be modeled as a single-step transformation from buggy code to correct code. However, empirical observation of developer workflows reveals that debugging is inherently a multi-step process involving hypothesis formation, incremental testing, and solution refinement. Models trained for single-shot generation lack the temporal reasoning capabilities necessary for effective iterative problem-solving.


These limitations converge to a fundamental research question: How can we develop training methodologies that enable language models to acquire genuine debugging skills through interactive experience, and what architectural choices maximize the effectiveness of such training? This question motivates our investigation into "tool-mediated" reinforcement learning as a paradigm for training more capable code repair systems.

\section{Research Objectives}

This thesis investigates a novel training paradigm termed "tool-mediated" reinforcement learning for automated program repair. The fundamental premise is that language models can acquire more sophisticated debugging capabilities when trained through direct interaction with software environments, rather than passive observation of static bug-fix examples. By embedding autonomous coding agents within the reinforcement learning optimization process, we enable models to learn from the consequences of their exploratory actions and develop effective debugging strategies through experience.

The primary objective is to demonstrate that this interactive training approach yields models with superior bug-fixing capabilities compared to those trained through conventional supervised learning. We hypothesize that the ability to explore codebases, execute commands, observe error messages, and iteratively refine solutions during training will result in agents that better capture the problem-solving patterns characteristic of human debugging.

Through rigorous empirical evaluation, this research aims to establish "tool-mediated" reinforcement learning as a viable and superior alternative to static supervised learning for training code repair systems, demonstrating that interactive training with real environments leads to more capable debugging agents.

\section{Contributions}

This thesis makes several significant contributions to the field of automated program repair and reinforcement learning for code generation:

\textbf{Novel Training Paradigm}: We introduce and implement "tool-mediated" reinforcement learning for code repair, wherein autonomous agents interact with real software environments during training. This approach represents a fundamental departure from static supervised learning, enabling models to learn debugging strategies through direct experience. Our implementation demonstrates the feasibility of this paradigm at scale, handling the computational and engineering challenges of simultaneous training and environment interaction.

\textbf{Nano-Agent Implementation}: We develop a minimalist "nano-agent" that utilizes only essential terminal commands and file operations, demonstrating that sophisticated debugging behaviors can emerge from simple interfaces when combined with reinforcement learning. This clean-slate approach validates that effective code repair capabilities arise from learning rather than engineered complexity.

\textbf{Technical Infrastructure}: We develop a comprehensive training infrastructure that enables efficient reinforcement learning with interactive agents. This includes a custom integration layer between vLLM inference servers and reinforcement learning trainers, real-time weight synchronization via NCCL for live policy updates, and optimizations enabling training of models up to 32B parameters with limited computational resources. The infrastructure supports both Group Relative Policy Optimization (GRPO) and can be extended to other policy gradient methods.

\textbf{Empirical Validation}: Through extensive experiments on the SWE-Gym training environment and SWE-Bench-Verified evaluation benchmark, we demonstrate that "tool-mediated" reinforcement learning produces monotonic improvements in bug-fixing performance. This result, achieved in an open-source setting, represents significant progress toward practical autonomous debugging systems and validates the potential of interactive training approaches.

\textbf{Open Science Contribution}: All code, training configurations, and evaluation protocols are released as open-source contributions to the research community. This includes the nano-agent implementation, the reinforcement learning training pipeline, and the infrastructure for integrating arbitrary agent scaffolds into the training process. By providing these resources, we lower the barrier to entry for future research in this domain.

\section{Research Questions}

This investigation is structured around one carefully formulated research question that probes the core aspects of "tool-mediated" reinforcement learning for code repair:

\textbf{RQ1: To what extent does "tool-mediated" reinforcement learning improve automated program repair performance compared to models trained through conventional supervised learning?}

This foundational question examines the core hypothesis that interactive training through reinforcement learning yields superior debugging capabilities. We investigate not only whether improvements occur, but also their magnitude, consistency across different bug types, and the learning dynamics that produce them. By comparing against both pretrained models and those fine-tuned through supervised learning on identical data, we isolate the specific contribution of the reinforcement learning paradigm. We hypothesize that the ability to learn from environmental feedback and develop exploration strategies will result in statistically significant performance improvements on held-out bug-fixing tasks.

\section{Methodology Overview}

This research employs a rigorous experimental methodology designed to systematically evaluate "tool-mediated" reinforcement learning for automated program repair. Our approach combines theoretical insights from reinforcement learning with practical engineering solutions to create a comprehensive experimental framework.

The methodology centers on a two-stage training pipeline that progressively refines model capabilities. The first stage employs supervised fine-tuning (SFT) on curated datasets of high-quality bug fixes, establishing a foundation of code understanding and basic repair patterns. This stage ensures that models begin reinforcement learning with sufficient capability to generate meaningful actions within the environment. The second stage implements Group Relative Policy Optimization (GRPO), a variant of proximal policy optimization that forgoes value function estimation in favor of group-relative reward baselines. During this stage, agents actively interact with software repositories, attempting repairs and receiving rewards based on patch quality.

Our experimental design implements careful controls to ensure valid comparisons between different training paradigms. We maintain identical model architectures (Qwen3 family models chosen for their superior tool-calling abilities), training data sources, and computational budgets across all experimental conditions. The primary independent variable is the training methodology (supervised learning versus reinforcement learning). Dependent variables include repair success rates, patch quality metrics, and generalization performance.

Data collection encompasses multiple complementary sources: detailed logs of agent-environment interactions capturing exploration patterns and decision-making processes, reward signals computed through automated comparison with ground-truth patches, and comprehensive evaluation metrics across diverse benchmarks. We employ both the SWE-Gym environment for training, which provides thousands of real-world Python bug-fixing tasks, and SWE-Bench-Verified for evaluation, ensuring robust assessment on human-verified bug fixes.

Statistical analysis includes significance testing for performance differences, learning curve analysis to understand training dynamics, and qualitative examination of agent behaviors to identify emergent strategies. This multi-faceted approach enables both quantitative validation of our hypotheses and deeper insights into the mechanisms driving performance improvements.

\section{Scope and Limitations}

This investigation operates within carefully defined boundaries that balance scientific rigor with practical constraints. Understanding these delimitations is essential for interpreting our findings and their applicability to broader contexts.

\textbf{Task Scope}: We focus specifically on automated bug repair tasks where a known error exists in the codebase and the goal is to generate a corrective patch. While we conduct limited evaluation on general code generation benchmarks to assess skill transfer, our primary emphasis remains on debugging and repair scenarios. This focus allows deep investigation of the debugging process while acknowledging that code generation from specifications represents a related but distinct challenge.

\textbf{Language and Environment Constraints}: Our experiments focus exclusively on Python repositories due to the availability of high-quality benchmarks and the language's popularity in open-source development. We specifically exclude bugs requiring complex runtime environments, external service interactions, or hardware-specific behaviors, as these cannot be reliably simulated in our containerized training environment.

\textbf{Model Architecture Limitations}: While our approach is theoretically model-agnostic, computational constraints necessitate focusing on the Qwen3 model family (8B and 32B variants). These models were selected for their exceptional tool-calling capabilities and open availability, but our findings may not directly generalize to models with different architectural characteristics or training objectives. The reinforcement learning approach demands substantial computational resources for both training and inference, limiting the scope of hyperparameter exploration and model scaling experiments.

\textbf{Evaluation Protocol Boundaries}: Our reward function relies on automated patch comparison rather than full test suite execution, trading perfect functional correctness assessment for computational tractability. While this approach aligns with established benchmarks and enables large-scale experiments, it may occasionally miss functionally equivalent but syntactically different solutions. Additionally, our evaluation emphasizes single-commit bug fixes rather than complex multi-stage refactoring or architectural changes.

\textbf{Agent Implementation}: Our nano-agent represents a deliberately minimalist approach to agent design, providing only essential terminal commands and file operations. While this simplicity enables clean experimental validation of the reinforcement learning approach, more sophisticated tool sets could potentially yield different performance characteristics.

\section{Thesis Organization}

The remainder of this thesis is structured to provide a comprehensive treatment of "tool-mediated" reinforcement learning for automated program repair, progressing from theoretical foundations through empirical validation.

\textbf{Chapter 2: Background and Theoretical Foundations} establishes the theoretical context for our work, reviewing reinforcement learning algorithms for language models with particular emphasis on Group Relative Policy Optimization. We examine the evolution of automated program repair approaches, from rule-based systems through modern neural methods, and analyze the emergence of tool-augmented language models. The chapter introduces key concepts including agent scaffolding, reward design for code repair, and the training-inference duality that characterizes our approach.

\textbf{Chapter 3: Methodology and System Design} presents our technical approach in detail, beginning with the "tool-mediated" training paradigm and its implementation. We describe the architecture of our nano-agent, explaining design decisions and rationale for the minimalist approach. The chapter details our two-stage training pipeline, reward formulation, and the engineering innovations that enable efficient large-scale training. Particular attention is given to the integration of vLLM inference servers with reinforcement learning trainers and the NCCL-based weight synchronization system.

\textbf{Chapter 4: Related Work} contextualizes our contributions within the broader landscape of code generation and repair research. We analyze prior work on reinforcement learning for programming tasks, agent-based software engineering tools, and the evolution of benchmarks for code repair evaluation. This chapter highlights how our approach synthesizes insights from multiple research threads while addressing limitations in existing methods.

\textbf{Chapter 5: Experimental Results and Analysis} presents comprehensive empirical evaluation addressing our research question. We report performance metrics on SWE-Bench-Verified, analyze learning curves and training dynamics, and examine the effectiveness of our approach. The chapter includes detailed ablation studies, error analysis, and investigation of transfer to general code generation tasks. Statistical significance testing and qualitative analysis of agent behaviors provide deeper insights into the mechanisms driving performance improvements.

\textbf{Chapter 6: Conclusions and Future Directions} synthesizes our findings, explicitly answering each research question based on empirical evidence. We discuss the broader implications for automated software engineering, identify limitations of the current approach, and outline promising directions for future research. The chapter concludes with reflections on the potential impact of "tool-mediated" reinforcement learning on the future of software development tools.