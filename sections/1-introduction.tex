\chapter{Introduction}
\label{ch:introduction}

Automated program repair has rapidly transformed from a nascent research area to one of the most promising applications of agentic \ac{AI} systems.
By ``agentic,'' we refer to systems that can autonomously interact with environments through tool use, make sequential decisions based on observations, and iteratively refine their approach based on feedback, as opposed to single-pass generation models that produce outputs without environmental interaction.
Recent advances in autonomous software engineering have achieved remarkable performance milestones, with systems like Claude~4 now exceeding 70\% success rates on SWE-Bench-Verified \cite{claude4_2025}.
This rapid progress underscores the critical importance of maintaining open research capable of understanding frontier capabilities.
A broad consensus has emerged among researchers: competitive access to training methodologies is essential to ensure that critical insights remain within the scientific community rather than concentrated in proprietary systems.
Recent open weight releases such as Kimi-K2~\cite{kimiK2_2025} and Qwen3-Coder~\cite{qwen3CoderBlog2025} follow closely behind frontier performance, but do not include full training recipes.
This reinforces the need for open, reproducible methods and analysis.

This thesis directly addresses this need by investigating how a single, minimalist agent—Nano—enables a model to acquire agentic behavior through experiential \ac{RL} feedback occurring entirely within the agent's interactive loop.
By situating all learning and adaptation within the Nano harness, we provide clear, reproducible insights into the mechanisms by which \ac{RL} cultivates autonomy and effective tool use in repository-level code repair.

An important contemporaneous effort is SWE-RL~\cite{wei2025swerladvancingllmreasoning}, which applies \ac{GRPO} with patch-similarity rewards (using Python's \texttt{difflib.SequenceMatcher}) in a static, single-turn setting: full file contexts are provided, the model outputs a patch directly, and similarity to ground truth defines the reward signal.
Our work is complementary: we also use execution-free patch-similarity rewards but combine them with \ac{GSPO}—a more robust sequence-level variant—in an agentic, multi-turn regime where the model must first acquire relevant context through tool use before proposing edits.
This distinction between static one-shot patch generation and interactive context acquisition motivates our focus on agent-integrated training within a single minimalist harness rather than prompt-time scaffolding engineering.

The necessity and transformative potential of agentic training emerge from examining the historical limitations of supervised learning for code repair and the foundational role of \acp{LLM} in enabling \ac{RL} for complex tasks.

\section{Background}
\label{sec:intro-background}

Traditional systems for automated code repair have relied predominantly on supervised learning from static datasets of bug-fix pairs.
In this paradigm, models passively observe input-output mappings between broken and corrected code, treating bug fixing as a pattern recognition task rather than engaging with the full interactive software engineering process.
While computationally efficient, this approach fundamentally misrepresents the nature of repository-level software debugging, which involves iterative hypothesis refinement, strategic exploration of entire codebases, execution of diagnostic commands, and contextual interpretation of error signals across multiple rounds of investigation.
Moreover, the information needed to fix a bug in real-world repositories is rarely contained within the bug-fix pair itself, necessitating the interactive capabilities that coding agents provide to navigate and understand the broader codebase context.

The question becomes: how do we train such agentic systems when the very complexity that necessitates their existence also makes traditional training approaches intractable?
\ac{RL} has achieved remarkable results in well-structured domains like Go, where clear success metrics exist and every action represents a valid move within the game's rules \cite{alphaGo2016}.
However, \ac{RL} fundamentally requires that learning systems can meaningfully participate in the task from the beginning.
They must be capable of generating valid actions, interpreting feedback, and understanding the basic structure of the problem space.
For complex reasoning tasks like software debugging, this prerequisite participation threshold has historically been insurmountable.

\acp{LLM} have altered this fundamental constraint by providing the world knowledge and reasoning capabilities necessary to attempt tasks of unprecedented complexity.
When equipped with appropriate interfaces, these models can navigate file systems, interpret error messages, execute diagnostic commands, and generate syntactically valid code modifications.
This capability foundation transforms previously intractable problems into domains where \ac{RL} becomes feasible.
In effect, \acp{LLM} provide the first rung on the ladder that makes applying \ac{RL} to software engineering feasible.

However, unlike traditional \ac{RL} agents that directly produce discrete actions from observations (e.g., chess moves, joystick inputs), \acp{LLM} fundamentally operate as text generators.
They cannot directly perceive environments or execute actions; they require an intermediary translation layer that converts generated text into executable actions and translates environmental responses into textual observations.

This architectural distinction necessitates a different approach to \ac{RL}.
In this paradigm, a lightweight harness interprets tool calls from the \ac{LLM}, executes corresponding actions in the environment, and returns textual observations for processing.
The entire system—\ac{LLM} plus harness—forms the \ac{RL} agent, with learning occurring through this mediated interaction cycle.
This thesis instantiates exactly one such harness (the Nano agent) and conducts all training strictly within this single-agent setting.

When equipped with appropriate harnesses, \acp{LLM} can navigate file systems, execute terminal commands, interpret error messages, and generate syntactically correct code modifications.
This capability transformation makes it practical to apply \ac{RL} to real-world software engineering tasks, as models can now generate the rich interaction trajectories required for policy gradient estimation.
Terminal-oriented agency benchmarks such as TauBench~\cite{taubench2024} provide standardized evaluation frameworks for such capabilities.

Training adopts an execution-free approach where rewards derive from patch similarity rather than test execution, enabling language-agnostic training without maintaining test infrastructure for each programming language.
Concrete observation, action, and reward definitions are specified in \cref{ch:method}.

The significance of this paradigm shift extends beyond technical feasibility.
Human software developers do not repair bugs through single-shot pattern matching; they engage in exploratory processes involving hypothesis formation, incremental information gathering, and iterative refinement.
They navigate complex codebases, examine interconnected components, execute diagnostic commands, and progressively construct mental models of system behavior.
This active learning process differs fundamentally from passive pattern recognition, suggesting that training models through interactive experience may yield qualitatively superior debugging capabilities.

This direction aligns with widely observed advances at the frontier: agentic capabilities are clearly improving, with public reports of stronger coding performance and enhanced interactive capabilities.
These developments heighten the need for open, reproducible understanding of how agentic training works, rather than reliance on closed systems.
Our work addresses this need by studying \ac{RL} with interactive coding agents in an open setting.

\section{Problem Statement}
\label{sec:intro-problem}

Tool-using coding agents already exist; what remains largely unavailable to the open scientific community is a reproducible training pipeline that develops such agentic behavior.
While proprietary systems demonstrate strong performance, their training methodologies remain opaque, creating a critical gap in open research.
This absence hinders scientific understanding of how \ac{RL} shapes agent behavior and prevents independent verification of claimed capabilities.

Addressing this gap is essential for maintaining competitive open research in a field increasingly dominated by closed frontier systems.
Without access to training recipes, the scientific community cannot critically evaluate agentic training claims, replicate findings, or build upon established methods.
The concentration of training expertise in proprietary labs risks creating fundamental knowledge asymmetries that undermine the open research ecosystem.
This concentration stems from dual barriers.
First, training is prohibitively expensive: the computational resources required for effective \ac{RL} training place such experiments beyond most academic budgets.
Second, operational complexity creates steep barriers: training pipelines involve dozens of interacting components built atop ill-understood \ac{LLM} foundations, compounding challenges in reproducing and extending prior work.
As multi-objective \ac{RL} training emerges as a promising direction, reducing both cost and complexity becomes essential for broad research participation.

We therefore develop a low-resource, execution-free \ac{RL} recipe for repository-level \ac{APR}.
The investigation addresses four research questions:

\textbf{RQ0: Does \ac{GSPO} training converge effectively with execution-free rewards?
}
\begin{itemize}
	\item Primary indicators: reward progression, variance evolution, and policy gradient signal stability over training.
	\item Analysis: reward mean and standard deviation trends.
	\item Hypothesis: execution-free patch-similarity rewards provide sufficient signal for policy convergence despite not directly optimizing for functional correctness.
\end{itemize}

\textbf{RQ1: How does \ac{GSPO} training improve Nano harness adaptation?
}
\begin{itemize}
	\item Primary indicators: tool-call success rate, action efficiency, and command usage evolution over training.
	\item Analysis: pre-training vs. post-training comparison of harness-level efficiency metrics and qualitative examination of changes in command usage patterns.
	\item Hypothesis: \ac{RL} training improves harness interaction efficiency and tool-use reliability, with observable shifts in command patterns toward more focused debugging strategies.
\end{itemize}

\textbf{RQ2: Does execution-free patch-similarity \ac{RL} training improve SWE-Bench-Verified performance?}
\begin{itemize}
	\item Primary metric: test-verified success rate on SWE-Bench-Verified (approximately 500 Python debugging tasks).
	\item Comparison: pre-training baseline vs. post-training performance using identical evaluation protocol.
	\item Hypothesis: training on static patch-similarity rewards improves functional bug-fixing success despite not directly optimizing for test passage.
\end{itemize}

\textbf{RQ3: Does execution-free \ac{RL} improve performance across programming languages without language-specific engineering?}
\begin{itemize}
	\item Primary evaluation: per-language reward improvements from first to second training epoch on the 1{,}000-task curriculum spanning nine programming languages.
	\item Analysis: comparing epoch 1 and epoch 2 rewards for the same problem instances across languages.
	\item Hypothesis: execution-free patch-similarity rewards enable unified training across programming languages without language-specific adaptations, with measurable improvements across diverse languages despite limited non-Python training data.
\end{itemize}

\section{Methodology}
\label{sec:intro-methodology}
This research employs a rigorous experimental methodology to systematically evaluate \ac{RL} for repository-level bug repair through online agent training.

We employ \ac{GSPO}~\cite{gspo2025}, a sequence-level policy optimization algorithm that extends the group-relative baseline principle with improved importance weighting for variable-length sequences.
\ac{GSPO} demonstrates superior robustness compared to earlier group-relative methods (\ac{GRPO}, DAPO), proving particularly effective for small effective batch sizes and high-variance rewards characteristic of multi-turn agent training under academic compute constraints.

Training proceeds through iterative online interaction: the agent explores repositories, generates repair episodes, and receives rewards based on static patch-similarity comparison between its repository modifications and ground-truth patches.

Primary comparisons report pre-training baseline vs.
post-training performance on SWE-Bench-Verified using the same Nano harness for both evaluations, eliminating confounds from supervised fine-tuning or prompt engineering.
Measured outcomes include repair success rates, reward progression, tool-call validity, action efficiency, and command usage patterns.

Data collection captures complete agent-environment interactions: tool invocations, repository states, reward signals computed via patch-similarity comparison, training dynamics, and aggregated evaluation metrics.
Training uses a 1,000-task curriculum (750 SWE-Gym Python tasks + 250 SWE-Bench-Multilingual tasks), with primary evaluation on SWE-Bench-Verified and multilingual generalization assessed through per-language reward progression across training epochs.

This methodology enables quantitative validation and mechanistic insights into how \ac{RL} shapes interactive debugging behavior through the Nano agent.

\section{Delimitations}
\label{sec:intro-delimitations}

This investigation operates within carefully defined boundaries that balance scientific rigor with practical constraints.
Understanding these delimitations is essential for interpreting our findings and their applicability to broader contexts.

\textbf{Task Scope}: This work addresses automated bug repair where known errors exist and the objective is patch generation.
Evaluation focuses on the Nano agent operating within its native harness, while broader code-generation benchmarks (HumanEval, MBPP) remain out of scope.
This focused investigation enables deep analysis of interactive debugging behavior while acknowledging that specification-based program synthesis constitutes orthogonal research.

\textbf{Language and Environment Constraints}: Training uses a 1,000-task curriculum (750 SWE-Gym Python issues + 250 SWE-Bench-Multilingual issues) curated to take advantage of our programming language agnostic approach.

\textbf{Compute Environment Constraints}: Experiments execute on Berzelius and Alvis Swedish national \ac{GPU} clusters using SLURM scheduling without Docker-based container orchestration support.
This infrastructure constraint shaped both Nano agent design and training pipeline architecture, favoring SLURM-compatible parallelization and process-level isolation over container-based approaches.

\textbf{Model Architecture Scope}: Our primary development and analysis focuses on Qwen3-14B—a hybrid reasoning model with strong tool-calling capabilities—with reasoning explicitly disabled to conserve context window budget in multi-turn episodes.
Limited comparisons with Qwen3-8B, Qwen3-32B, and Llama3.1-8B appear in the Appendix to illustrate capacity and architecture effects, but full model ablation studies are left for future work.

\textbf{Evaluation Protocol Boundaries}: Rewards derive from automated patch-similarity comparison rather than test suite execution, trading functional verification for computational tractability and reproducibility.
This approach aligns with established benchmarks and enables large-scale multilingual training, though it may undervalue functionally equivalent patches with syntactic divergence from ground truth.
Evaluation emphasizes single-commit bug repairs; multi-stage refactoring and architectural changes remain out of scope.

\textbf{Agent Implementation}: The Nano agent adopts a deliberately minimalist design, exposing only essential shell commands and file patching operations (two tools: \texttt{shell}, \texttt{apply\_patch}).
This simplicity enables clean experimental isolation of \ac{RL} effects, though richer tool sets might exhibit different performance-complexity trade-offs.

\textbf{Methodological Boundaries}: We briefly explored distillation from proprietary models via \ac{SFT} but abandoned this direction due to limited empirical gains and philosophical misalignment with open research objectives focused on training from environmental interaction rather than imitating closed systems.

\section{Outline}
\label{sec:intro-outline}

The remainder of this thesis progresses from theoretical foundations through empirical validation to conclusions:

\textbf{Chapter 2: Background and Related Work} reviews policy optimization algorithms for language models (\ac{PPO}, \ac{GRPO}, DAPO, \ac{GSPO}), situates \ac{APR} within repository-level evaluation paradigms, and contrasts agentless vs.
agentic approaches to motivate our execution-free, agent-integrated training methodology.

\textbf{Chapter 3: Methodology} formalizes the Nano agent (observation/action spaces, interaction loop, termination), specifies training data (SWE-Gym, SWE-Bench-Multilingual), defines the patch-similarity reward formulation, describes \ac{GSPO} policy optimization with masked loss, and outlines the evaluation protocol addressing RQ0-RQ3.

\textbf{Chapter 4: Implementation} documents the practical realization: vLLM-based serving with tool calling, live adapter synchronization via \ac{NCCL}, trainer extensions for multi-turn \ac{GSPO}, SLURM orchestration, compute-efficient infrastructure (\ac{LoRA}, DeepSpeed ZeRO, gradient checkpointing, BF16), and training protocol execution.

\textbf{Chapter 5: Results} presents experimental findings: training convergence and policy gradient signal stability (RQ0), harness adaptation metrics and behavioral changes (RQ1), SWE-Bench-Verified performance and hierarchical learning dynamics (RQ2), and per-language reward improvements across nine programming languages (RQ3).

\textbf{Chapter 6: Conclusions} synthesizes findings, discusses design philosophy and lessons learned (system complexity, reward misspecification, infrastructure challenges), examines broader implications for agentic \ac{AI}, identifies limitations, and proposes future research directions for \ac{RL}-trained coding agents.
