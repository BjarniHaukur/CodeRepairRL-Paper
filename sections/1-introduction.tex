\chapter{Introduction}
\label{ch:introduction}

Automated program repair has rapidly transformed from a nascent research area to one of the most promising applications of agentic \ac{AI} systems.
By ``agentic,'' we refer to systems that can autonomously interact with environments through tool use, make sequential decisions based on observations, and iteratively refine their approach based on feedback, as opposed to single-pass generation models that produce outputs without environmental interaction.
Recent advances in autonomous software engineering have achieved remarkable performance milestones, with systems like Claude~4 now exceeding 70\% success rates on \ac{SWE-Bench-Verified} \cite{claude4_2025}.
This rapid progress underscores the critical importance of maintaining open research that can achieve near-parity understanding with frontier capabilities.
Across diverse perspectives, there emerges a common recognition that researchers must maintain competitive access to the tools and methodologies driving these advances, ensuring that critical insights remain accessible to the broader scientific community rather than concentrated within proprietary systems.
Recent open weight releases such as Kimi-K2~\cite{kimiK2_2025} and Qwen3-Coder~\cite{qwen3coder_blog_2025} follow closely behind frontier performance, but do not include full training recipes.
This reinforces the need for open, reproducible methods and analysis.
\todoinline{Verify and update SWE-Bench(-Verified) SOTA; cite system cards/technical reports for these claims (Claude 4/3.5, Qwen3-Coder, Kimi-K2) at submission.}

An important contemporaneous effort is SWE-RL~\cite{wei2025swerladvancingllmreasoning}, which applies \ac{GRPO} with a rule-based reward based on patch similarity (Python's \texttt{difflib.SequenceMatcher}) to train models in a static, single-turn setting where the full file contexts are provided and the model directly outputs a patch that is compared to ground truth.
Our work is complementary: we also use \ac{GRPO} and a patch-similarity reward, but in an agentic, multi-step regime where the model must first acquire its own context through tool use before proposing edits.
This distinction (static one-shot patch generation versus interactive context acquisition and repair) motivates our focus on agent-integrated training rather than purely prompt-time scaffolding.

To understand why this shift to agentic training is both necessary and transformative, we must first examine the historical context and theoretical foundations that have led to this approach.

\section{Background}
\label{sec:intro-background}

Traditional systems for automated code repair have relied predominantly on supervised learning from static datasets of bug-fix pairs.
In this paradigm, models passively observe input-output mappings between broken and corrected code, treating bug fixing as a pattern recognition task rather than engaging with the full interactive software engineering process.
While computationally efficient, this approach fundamentally misrepresents the nature of repository-level software debugging, which involves iterative hypothesis refinement, strategic exploration of entire codebases, execution of diagnostic commands, and contextual interpretation of error signals across multiple rounds of investigation.
Moreover, the information needed to fix a bug in real-world repositories is rarely contained within the bug-fix pair itself, necessitating the interactive capabilities that coding agents provide to navigate and understand the broader codebase context.
\todoinline{which traditional paper to cite here}

The question becomes: how do we train such agentic systems when the very complexity that necessitates their existence also makes traditional training approaches intractable?
\ac{RL} has achieved remarkable results in well-structured domains like Go, where clear success metrics exist and every action represents a valid move within the game's rules \cite{alphaZero2018}.
However, \ac{RL} fundamentally requires that learning systems can meaningfully participate in the task from the beginning. They must be capable of generating valid actions, interpreting feedback, and understanding the basic structure of the problem space.
For complex reasoning tasks like software debugging, this prerequisite participation threshold has historically been insurmountable.

\acp{LLM} have altered this fundamental constraint by providing the world knowledge and reasoning capabilities necessary to attempt tasks of unprecedented complexity.
When equipped with appropriate interfaces, these models can navigate file systems, interpret error messages, execute diagnostic commands, and generate syntactically valid code modifications.
This capability foundation transforms previously intractable problems into domains where \ac{RL} becomes feasible.
In effect, \acp{LLM} provide the first rung on the ladder that makes applying \ac{RL} to software engineering feasible.

However, unlike traditional game playing \ac{RL} agents, which directly produce actions from observations (e.g., selecting chess moves or joystick inputs), \acp{LLM} fundamentally operate as token generators.
They cannot directly perceive environments or execute actions; instead, they require an intermediary translation layer that converts their generated tokens into executable actions and translates environmental responses back into text.

This critical distinction necessitates a fundamentally different approach to \ac{RL}.
In this paradigm, a scaffold or harness interprets tool calls from the \ac{LLM}, executes these as actions in the environment, and returns textual observations that the \ac{LLM} can process.
The entire system (\ac{LLM} plus scaffold) forms the \ac{RL} agent, with learning occurring through this mediated interaction cycle.

When equipped with appropriate scaffolds, \acp{LLM} can navigate file systems, execute terminal commands, interpret error messages, and generate syntactically correct code modifications.
This capability transformation enables, for the first time, the application of \ac{RL} to real-world software engineering tasks, as models can now generate the rich interaction trajectories required for policy gradient estimation.
\todoinline{Tie explicitly to terminal-oriented agency; add references to public statements/benchmarks on terminal skills (e.g., TerminalBench/TauBench  ).}

The significance of this paradigm shift extends beyond mere technical feasibility.
Human software developers do not repair bugs through single-shot pattern matching; rather, they engage in sophisticated exploratory processes involving hypothesis formation, incremental information gathering, and iterative solution refinement.
They navigate complex codebases, examine multiple interconnected components, execute diagnostic commands, and progressively construct mental models of system behavior.
This active learning process, fundamentally different from passive pattern recognition, suggests that training models through interactive experience could yield qualitatively superior debugging capabilities.

This direction aligns with widely observed advances at the frontier: agentic capabilities are clearly improving, with public reports of stronger coding performance and enhanced interactive capabilities.
These developments heighten the need for open, reproducible understanding of how agentic training works, rather than reliance on closed systems.
Our work addresses this need by studying \ac{RL} with interactive coding agents in an open setting.
\todoinline{Add citations to public system cards or technical reports where appropriate.}

\section{Problem}
\label{sec:intro-problem}

The central challenge addressed in this thesis concerns the fundamental disconnect between the training methodologies employed for code repair models and the interactive, exploratory nature of real-world debugging.
Contemporary approaches to automated program repair exhibit a critical limitation: models trained on static datasets of bug-fix pairs are subsequently expected to perform dynamic, multi-step reasoning in complex software environments.
This paradigm mismatch constrains their effectiveness and generalization capabilities.

We identify two specific limitations that characterize current approaches:

\textbf{Exploration Capability Deficit}: Models trained exclusively on static data lack the ability to develop effective exploration strategies.
Unlike human developers who actively navigate codebases, execute diagnostic commands, and iteratively gather contextual information, these models never experience the exploratory aspects of debugging during training.
This results in agents that, when deployed, exhibit suboptimal information-gathering behaviors and fail to leverage available tools effectively.

\textbf{Temporal Abstraction Mismatch}: The predominant training paradigm assumes bug repair can be modeled as a single-step transformation from buggy code to correct code.
However, empirical observation of developer workflows reveals that debugging is inherently a multi-step process involving hypothesis formation, incremental testing, and solution refinement.
Models trained for single-shot generation lack the temporal reasoning capabilities necessary for effective iterative problem-solving.

These limitations converge to a fundamental research question: How can we develop training methodologies that enable language models to acquire genuine debugging skills through environmental experience, and what architectural choices maximize the effectiveness of such training?
This question motivates our investigation into \ac{RL} with coding agents as a paradigm for training more capable code repair systems.

To make this investigation concrete, we address the following research questions aligned to our focus on a single minimalist harness (Nano) and practical generalization:

\textbf{RQ1: Does the trained model adapt to the harness (tools and prompts)?}
\begin{itemize}
	\item Primary indicators: increasing reward, tool success rate over training, invalid tool-call rate, tool-usage entropy/diversity
	\item Hypothesis: \ac{RL} improves harness adaptation, reflected by increased tool success and reduced invalid calls without sacrificing exploration.
\end{itemize}

\textbf{RQ2: Can we train effectively with multiple base models (e.g., SmolLM3) beyond Qwen?}
\begin{itemize}
	\item Goal: validate that our training recipe generalizes beyond the Qwen3 family used for development.
	\item Design: replicate final experiments on Gemma3 and Llama3.1 to assess portability across different model architectures and tool-calling capabilities. \todoinline{Is this a sensible set of models?}
	\item This approach tests whether our findings are Qwen-specific or represent broader principles for online \ac{RL} training.
\end{itemize}

\textbf{RQ3: Does training on \ac{SWE-Gym} overfit? (Train on \ac{SWE-Gym}, evaluate on \ac{SWE-Bench} and Multi-SWE-Bench)}
\begin{itemize}
	\item Measure cross-dataset generalization and potential overfitting to training environments.
	\item \todoinline{Report \ac{SWE-Gym} train curves vs.
		      \ac{SWE-Bench}(-Verified) / Multi-SWE-Bench evaluation; include delta vs \ac{SFT} baselines.
	      }
\end{itemize}

\textbf{RQ4: Does the trained model generalize to other agentic code harnesses and tasks?}
\begin{itemize}
	\item Cross-harness: evaluate with MiniAgent (should work) and OpenHands harnesses without.
	\item Cross-task: evaluate on TauBench and TerminalBench to probe terminal-oriented generalization. \todoinline{TerminalBench is probably too hard}
\end{itemize}

With these research questions guiding our investigation, we now articulate the specific purpose and contributions of this thesis.

\section{Purpose}
\label{sec:intro-purpose}

This thesis investigates a novel training paradigm for automated program repair using \ac{RL} with coding agents.
The fundamental premise is that language models can acquire more sophisticated debugging capabilities when trained through direct interaction with software environments, rather than passive observation of static bug-fix examples.
By embedding autonomous coding agents within the \ac{RL} optimization process, we enable models to learn from the consequences of their exploratory actions and develop effective debugging strategies through experience.

The primary objective is to demonstrate that this experiential training approach yields models with superior bug-fixing capabilities compared to those trained through conventional supervised learning.
We hypothesize that the ability to explore codebases, execute commands, observe error messages, and iteratively refine solutions during training will result in agents that better capture the problem-solving patterns characteristic of human debugging.

Through rigorous empirical evaluation, this research aims to establish this online \ac{RL} approach as a viable and superior alternative to static supervised learning for training code repair systems, demonstrating that experiential training with real environments leads to more capable debugging agents.

% Despite growing evidence that leading \ac{AI} labs employ experiential training (OpenAI's o1~\cite{openAI_o1_2024}, Anthropic's Claude~\cite{anthropic2024}, and Cognition's Devin~\cite{cognition2024} all exhibit behaviors suggesting such training), no open-source implementation has been available to the research community.

% By open-sourcing this infrastructure, we aim to democratize access to advanced training techniques previously available only to well-resourced industry labs, enabling broader research into agentic \ac{AI} systems.

% The significance extends beyond code repair: online training could transform how models learn any task requiring environmental interaction, from scientific experimentation to robotic control.
% This thesis demonstrates its viability in the well-scoped domain of automated debugging, paving the way for broader applications.

% Current research in this area is primarily conducted by industry labs (OpenAI, Anthropic, Cognition Labs) with limited open-source replication, creating a significant knowledge gap in the academic community.

\section{Goal}
\label{sec:intro-goal}

% This thesis makes several significant contributions to the field of automated program repair and \ac{RL} for code generation:

% \textbf{Novel Training Paradigm}: We introduce and implement \ac{RL} for code repair, wherein autonomous agents interact with real software environments during training.
% \todoinline{Recommend defining this approach once and dropping the term}
% This approach represents a fundamental departure from static supervised learning, enabling models to learn debugging strategies through direct experience.
% Our implementation demonstrates the feasibility of this paradigm at scale, handling the computational and engineering challenges of simultaneous training and environment interaction.

% \textbf{Nano Agent Implementation}: We develop a minimalist "Nano agent" that utilizes only essential terminal commands and file operations, demonstrating that sophisticated debugging behaviors can emerge from simple interfaces when combined with \ac{RL}.
% This clean-slate approach validates that effective code repair capabilities arise from learning rather than engineered complexity.

% \textbf{Technical Infrastructure}: We develop a comprehensive training infrastructure that enables efficient \ac{RL} with coding agents.
% This includes a custom integration layer between vLLM inference servers and \ac{RL} trainers, real-time weight synchronization via \ac{NCCL} for live policy updates, and optimizations enabling training of models up to 32B parameters with limited computational resources.
% The infrastructure supports both Group Relative Policy Optimization (\ac{GRPO})~\cite{shao2024deepseekmathpushinglimitsmathematical} and can be extended to other policy gradient methods.

% \textbf{Empirical Validation}: Through extensive experiments on the \ac{SWE-Gym} training environment and \ac{SWE-Bench-Verified} evaluation benchmark, we demonstrate that this \ac{RL} approach produces monotonic improvements in bug-fixing performance.
% This result, achieved in an open-source setting, represents significant progress toward practical autonomous debugging systems and validates the potential of experiential training approaches.

% \textbf{Open Science Contribution}: All code, training configurations, and evaluation protocols are released as open-source contributions to the research community.
% This includes the Nano agent implementation and the \ac{RL} training pipeline.

The overarching goal of this thesis is to develop and empirically validate an online \ac{RL} training paradigm for automated program repair.
Through rigorous experimentation with our minimalist Nano agent, we demonstrate that models can learn effective debugging strategies through environmental interaction rather than passive observation of bug-fix pairs.
Our comprehensive evaluation addresses harness adaptation, cross-model generalization, dataset transfer, and task generalization as outlined in our research questions above.

\section{Benefits, Ethics and Sustainability}
\label{sec:intro-benefits}

This work contributes to open research in a field dominated by closed frontier labs by releasing methods, code, and evaluation protocols in an open format.
All implementation details needed to reproduce results are documented, including configuration files, prompts, and harness interfaces.
We use standard datasets with clear licensing and provide exact commit hashes for benchmarks and dependencies.
To enable critical scrutiny, we specify evaluation scripts and random seeds, and we will archive artifacts to ensure long-term accessibility.
By reducing barriers to entry and facilitating independent replication, the project strengthens transparency and community-driven progress in automated program repair.

Ethical considerations arise from the potential misuse of agentic systems and the provenance of training data.
We restrict experiments to permissively licensed repositories and avoid exposing sensitive credentials or private data within environments.
We emphasize safe tooling (read/write-limited file access, sandboxed execution) and encourage downstream \todoinline{I believe we can better isolate each agent with some apptainer flags I found, decide if the final iteration of experiments will use those}

We closely monitor the amount of compute per experiment and report tokens processed and \ac{GPU}-hours to support reproducibility and minimize unnecessary energy use.
We adopt efficiency practices such as mixed-precision training, reuse of checkpoints and shared inference servers via training-inference duality, and targeted ablations to avoid redundant runs.

\section{Methodology}
\label{sec:intro-methodology}
This research employs a rigorous experimental methodology designed to systematically evaluate \ac{RL} for repository-level bug fixing.
% Our approach combines theoretical insights from \ac{RL} with practical engineering solutions to create a comprehensive experimental framework.

We employ \ac{GRPO}, a variant of \ac{PPO}~\cite{schulman2017proximalpolicyoptimizationalgorithms} that forgoes value function estimation in favor of group-relative reward baselines.
During this stage, agents actively interact with software repositories, attempting repairs and receiving rewards based on patch quality.

Our experimental design implements careful controls to ensure valid comparisons between different training paradigms.
We maintain identical model architectures (Qwen3 family models chosen for their superior tool-calling abilities), training data sources, and computational budgets across all experimental conditions.
The primary independent variable is the training methodology (supervised learning versus \ac{RL}).
Dependent variables include repair success rates, patch quality metrics, and generalization performance.

Data collection encompasses multiple complementary sources: detailed logs of agent-environment interactions capturing exploration patterns and decision-making processes, reward signals computed through automated comparison with ground-truth patches, and comprehensive evaluation metrics across diverse benchmarks.
We employ both the \ac{SWE-Gym} environment for training, which provides thousands of real-world Python bug-fixing tasks, and \ac{SWE-Bench} for evaluation, ensuring robust assessment on human-verified bug fixes.

\todoinline{SWE-Bench?
Verified?
R2E?
}

Statistical analysis includes significance testing for performance differences, learning curve analysis to understand training dynamics, and qualitative examination of agent behaviors to identify emergent strategies.
This multi-faceted approach enables both quantitative validation of our hypotheses and deeper insights into the mechanisms driving performance improvements.

\section{Stakeholders}
\label{sec:intro-stakeholders}

Primary stakeholders include \ac{ML} and software engineering researchers interested in \ac{APR}, open-source maintainers whose repositories inform benchmarks and evaluation, and practitioners seeking reliable automated debugging tools.
Secondary stakeholders include infrastructure providers supporting large-scale training and serving, and the broader \ac{AI} community working on agentic systems.
We design our methods and releases to maximize utility for these groups while balancing privacy, security, and sustainability concerns.

\section{Delimitations}
\label{sec:intro-delimitations}

This investigation operates within carefully defined boundaries that balance scientific rigor with practical constraints.
Understanding these delimitations is essential for interpreting our findings and their applicability to broader contexts.

\textbf{Task Scope}: We focus specifically on automated bug repair tasks where a known error exists in the codebase and the goal is to generate a corrective patch.
While we conduct limited evaluation on general code generation benchmarks to assess skill transfer, our primary emphasis remains on debugging and repair scenarios.
This focus allows deep investigation of the debugging process while acknowledging that code generation from specifications represents a related but distinct challenge.

\textbf{Language and Environment Constraints}: Our experiments focus exclusively on Python repositories due to the availability of high-quality benchmarks and the language's popularity in open-source development.
We specifically exclude bugs requiring complex runtime environments, external service interactions, or hardware-specific behaviors, as these cannot be reliably simulated in our containerized training environment.

\textbf{Compute Environment Constraints}: Experiments were conducted on the Berzelius and Alvis Swedish national \ac{GPU} clusters, which use SLURM scheduling and do not support horizontal scalability via Docker-based container orchestration.
This constraint informed the design of both the Nano agent and the training pipeline, leading us to adopt parallelization and isolation mechanisms compatible with SLURM scheduling.

\textbf{Model Architecture Scope}: Our primary development and analysis focuses on the Qwen3 model family (reasoning turned off), selected for their exceptional tool-calling capabilities and open availability.
We deliberately avoid reasoning models as they generate excessively verbose outputs that rapidly exhaust token budgets in our online training environment.
To validate generalizability, we replicate key experiments on Gemma3 and Llama3.1.

\textbf{Evaluation Protocol Boundaries}: Our reward function relies on automated patch comparison rather than full test suite execution, trading perfect functional correctness assessment for computational tractability.
While this approach aligns with established benchmarks and enables large-scale experiments, it may occasionally miss functionally equivalent but syntactically different solutions.
Additionally, our evaluation emphasizes single-commit bug fixes rather than complex multi-stage refactoring or architectural changes.

\textbf{Agent Implementation}: Our Nano agent represents a deliberately minimalist approach to agent design, providing only essential terminal commands and file operations.
While this simplicity enables clean experimental validation of the \ac{RL} approach, more sophisticated tool sets could potentially yield different performance characteristics.

\textbf{Methodological Boundaries}: We briefly explored distillation via \ac{SFT}, but did not pursue it further due to limited empirical benefit and because reliance on proprietary teacher models would run counter to the thesis objective of advancing open \ac{RL} training for agentic coding behaviour.

\section{Outline}
\label{sec:intro-outline}

The remainder of this thesis is structured to provide a comprehensive treatment of \ac{RL} for automated program repair, progressing from theoretical foundations through empirical validation.

\textbf{Chapter 2: Background and Theoretical Foundations} establishes the theoretical context for our work, reviewing \ac{RL} algorithms for language models with particular emphasis on \ac{GRPO}.
We examine the evolution of automated program repair approaches, from rule-based systems through modern neural methods, and analyze the emergence of tool-augmented language models.
The chapter introduces key concepts including agent scaffolding, reward design for code repair, and the training-inference duality that characterizes our approach.

\textbf{Chapter 3: Methodology and System Design} presents our technical approach in detail, beginning with the scaffolded training paradigm and its implementation.
We describe the architecture of our Nano agent, explaining design decisions and rationale for the minimalist approach.
The chapter details our two-stage training pipeline, reward formulation, and the engineering innovations that enable efficient large-scale training.
Particular attention is given to the integration of vLLM inference servers with \ac{RL} trainers and the \ac{NCCL}-based weight synchronization system.

\textbf{Chapter 4: Related Work} contextualizes our contributions within the broader landscape of code generation and repair research.
We analyze prior work on \ac{RL} for programming tasks, agent-based software engineering tools, and the evolution of benchmarks for code repair evaluation.
This chapter highlights how our approach synthesizes insights from multiple research threads while addressing limitations in existing methods.

\textbf{Chapter 5: Experimental Results and Analysis} presents comprehensive empirical evaluation addressing our research questions.
We report performance metrics on \ac{SWE-Bench-Verified}, analyze learning curves and training dynamics, and examine the effectiveness of our approach.
The chapter includes detailed ablation studies, error analysis, and investigation of transfer to general code generation tasks.
Statistical significance testing and qualitative analysis of agent behaviors provide deeper insights into the mechanisms driving performance improvements.

\textbf{Chapter 6: Conclusions and Future Directions} synthesizes our findings, explicitly answering each research question based on empirical evidence.
We discuss the broader implications for automated software engineering, identify limitations of the current approach, and outline promising directions for future research.
The chapter concludes with reflections on the potential impact of scaffolded \ac{RL} on the future of software development tools.
