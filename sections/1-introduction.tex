\chapter{Introduction}

Automated program repair represents one of the most formidable challenges in software engineering, with profound implications for development productivity and software reliability. Despite remarkable advances in large language models (LLMs), state-of-the-art systems achieve less than 20\% success rates on rigorous benchmarks such as SWE-Bench \cite{sweBench2024}, highlighting a significant gap between current capabilities and practical requirements. This limitation persists despite the availability of extensive training data and increasingly sophisticated model architectures, suggesting that fundamental methodological innovations are necessary to achieve meaningful progress.

The traditional paradigm for training code repair models relies on supervised learning from static datasets of bug-fix pairs, wherein models passively observe input-output mappings without experiencing the interactive, exploratory process that characterizes human debugging. This approach, while computationally efficient, fails to capture the essential dynamics of software debugging: the iterative refinement of hypotheses, strategic exploration of codebases, and contextual interpretation of error signals. Consequently, models trained through passive observation struggle to generalize beyond pattern matching to genuine problem-solving.

The recent convergence of large language models and reinforcement learning presents an unprecedented opportunity to transcend these limitations. Historically, reinforcement learning remained inapplicable to complex symbolic reasoning tasks due to a fundamental bootstrapping problem: agents could not generate meaningful actions in domains requiring substantial prior knowledge. The game-playing successes of systems like AlphaZero \cite{alphaZero2018}, while impressive, operated in constrained environments where every legal action was well-defined and immediately evaluable. In contrast, tasks such as programming demand extensive conceptual understanding merely to produce syntactically valid attempts.

Large language models have fundamentally altered this landscape by providing the prerequisite knowledge and capabilities necessary for meaningful participation in complex domains. When equipped with appropriate interfaces, LLMs can navigate file systems, execute terminal commands, interpret error messages, and generate syntactically correct code modifications. This capability transformation enables, for the first time, the application of reinforcement learning to real-world software engineering tasks, as models can now generate the rich interaction trajectories required for policy gradient estimation.

The significance of this paradigm shift extends beyond mere technical feasibility. Human software developers do not repair bugs through single-shot pattern matching; rather, they engage in sophisticated exploratory processes involving hypothesis formation, incremental information gathering, and iterative solution refinement. They navigate complex codebases, examine multiple interconnected components, execute diagnostic commands, and progressively construct mental models of system behavior. This active learning process, fundamentally different from passive pattern recognition, suggests that training models through interactive experience could yield qualitatively superior debugging capabilities.

Recent investigations into agent-based approaches have demonstrated the potential of tool-augmented LLMs for software engineering tasks \cite{openHands2024, aider2024}. However, these systems typically employ a two-stage approach: first training models on static datasets through supervised learning, then deploying them as agents with access to various tools and interfaces. This training-deployment mismatch potentially limits their effectiveness, as models never learn to optimize their tool usage or develop exploration strategies during training. The disconnect between the passive training regime and the active deployment environment represents a fundamental limitation that our work addresses.

\section{Problem Statement}

The central challenge addressed in this thesis concerns the fundamental disconnect between the training methodologies employed for code repair models and the interactive, exploratory nature of real-world debugging. Contemporary approaches to automated program repair exhibit a critical limitation: models trained on static datasets of bug-fix pairs are subsequently expected to perform dynamic, multi-step reasoning in complex software environments. This paradigm mismatch constrains their effectiveness and generalization capabilities.

We identify three specific limitations that characterize current approaches:

\textbf{Exploration Capability Deficit}: Models trained exclusively on static data lack the ability to develop effective exploration strategies. Unlike human developers who actively navigate codebases, execute diagnostic commands, and iteratively gather contextual information, these models never experience the exploratory aspects of debugging during training. This results in agents that, when deployed, exhibit suboptimal information-gathering behaviors and fail to leverage available tools effectively.

\textbf{Temporal Abstraction Mismatch}: The predominant training paradigm assumes bug repair can be modeled as a single-step transformation from buggy code to correct code. However, empirical observation of developer workflows reveals that debugging is inherently a multi-step process involving hypothesis formation, incremental testing, and solution refinement. Models trained for single-shot generation lack the temporal reasoning capabilities necessary for effective iterative problem-solving.

\textbf{Scaffold Design Uncertainty}: When LLMs are augmented with tools and interfaces (collectively termed scaffolds) for code interaction, the relationship between scaffold complexity and learning effectiveness remains poorly understood. The spectrum ranges from minimalist interfaces providing basic file operations to sophisticated frameworks offering repository analysis, semantic search, and guided reasoning. The optimal level of scaffold complexity for reinforcement learning-based training has not been systematically investigated.

These limitations converge to a fundamental research question: How can we develop training methodologies that enable language models to acquire genuine debugging skills through interactive experience, and what architectural choices maximize the effectiveness of such training? This question motivates our investigation into agent-in-the-loop reinforcement learning as a paradigm for training more capable code repair systems.

\section{Research Objectives}

This thesis investigates a novel training paradigm termed "agent-in-the-loop reinforcement learning" for automated program repair. The fundamental premise is that language models can acquire more sophisticated debugging capabilities when trained through direct interaction with software environments, rather than passive observation of static bug-fix examples. By embedding autonomous coding agents within the reinforcement learning optimization process, we enable models to learn from the consequences of their exploratory actions and develop effective debugging strategies through experience.

The primary objective is to demonstrate that this interactive training approach yields models with superior bug-fixing capabilities compared to those trained through conventional supervised learning. We hypothesize that the ability to explore codebases, execute commands, observe error messages, and iteratively refine solutions during training will result in agents that better capture the problem-solving patterns characteristic of human debugging.

A secondary objective concerns the architectural design of agent scaffolds and their impact on learning effectiveness. We systematically compare minimalist scaffolds—providing only essential terminal operations and file manipulation capabilities—against sophisticated frameworks incorporating repository analysis, semantic search, and structured reasoning support. This comparison addresses a fundamental question in AI system design: whether simple, general-purpose interfaces or carefully engineered, domain-specific tools lead to more effective learning when integrated into the training process.

Through rigorous empirical evaluation, this research aims to establish agent-in-the-loop reinforcement learning as a viable and superior alternative to static supervised learning for training code repair systems, while providing actionable insights into scaffold design principles that maximize learning effectiveness.

\section{Contributions}

This thesis makes several significant contributions to the field of automated program repair and reinforcement learning for code generation:

\textbf{Novel Training Paradigm}: We introduce and implement agent-in-the-loop reinforcement learning for code repair, wherein autonomous agents interact with real software environments during training. This approach represents a fundamental departure from static supervised learning, enabling models to learn debugging strategies through direct experience. Our implementation demonstrates the feasibility of this paradigm at scale, handling the computational and engineering challenges of simultaneous training and environment interaction.

\textbf{Comparative Scaffold Analysis}: We conduct the first systematic comparison of scaffold complexity in the context of reinforcement learning for code repair. By implementing and evaluating both a minimalist "nano-agent" utilizing only essential terminal commands and a sophisticated scaffold incorporating advanced code analysis tools, we provide empirical evidence regarding the relationship between environmental complexity and learning effectiveness. This analysis offers principled guidance for future agent design decisions.

\textbf{Technical Infrastructure}: We develop a comprehensive training infrastructure that enables efficient reinforcement learning with interactive agents. This includes a custom integration layer between vLLM inference servers and reinforcement learning trainers, real-time weight synchronization via NCCL for live policy updates, and optimizations enabling training of models up to 32B parameters with limited computational resources. The infrastructure supports both Group Relative Policy Optimization (GRPO) and can be extended to other policy gradient methods.

\textbf{Empirical Validation}: Through extensive experiments on the SWE-Gym training environment and SWE-Bench-Verified evaluation benchmark, we demonstrate that agent-in-the-loop reinforcement learning produces monotonic improvements in bug-fixing performance. This result, achieved in an open-source setting, represents significant progress toward practical autonomous debugging systems and validates the potential of interactive training approaches.

\textbf{Open Science Contribution}: All code, training configurations, and evaluation protocols are released as open-source contributions to the research community. This includes the nano-agent implementation, the reinforcement learning training pipeline, and the infrastructure for integrating arbitrary agent scaffolds into the training process. By providing these resources, we lower the barrier to entry for future research in this domain.

\section{Research Questions}

This investigation is structured around three carefully formulated research questions that probe different aspects of agent-in-the-loop reinforcement learning for code repair:

\textbf{RQ1: To what extent does agent-in-the-loop reinforcement learning improve automated program repair performance compared to models trained through conventional supervised learning?}

This foundational question examines the core hypothesis that interactive training through reinforcement learning yields superior debugging capabilities. We investigate not only whether improvements occur, but also their magnitude, consistency across different bug types, and the learning dynamics that produce them. By comparing against both pretrained models and those fine-tuned through supervised learning on identical data, we isolate the specific contribution of the reinforcement learning paradigm. We hypothesize that the ability to learn from environmental feedback and develop exploration strategies will result in statistically significant performance improvements on held-out bug-fixing tasks.

\textbf{RQ2: What is the relationship between scaffold complexity and learning effectiveness in reinforcement learning-based code repair?}

Drawing inspiration from Sutton's "bitter lesson" \cite{sutton2019bitter}, which argues that general-purpose computation ultimately outperforms hand-crafted features, this question investigates whether minimalist or feature-rich scaffolds lead to better learning outcomes. We compare a deliberately minimal scaffold providing only basic shell operations and file manipulation against a sophisticated framework incorporating repository mapping, semantic code search, and structured reasoning support. Beyond simple performance comparison, we analyze how scaffold complexity affects sample efficiency, training stability, and the types of debugging strategies learned. This investigation provides critical insights for designing future agent architectures.

\textbf{RQ3: To what degree do debugging capabilities acquired through agent-in-the-loop training transfer to novel contexts, including different programming languages and broader code generation tasks?}

This question addresses the fundamental concern of whether our training approach produces genuine problem-solving improvements or merely overfits to the specific training environment. We evaluate transfer learning across multiple dimensions: cross-language generalization to Java debugging tasks, performance on general code generation benchmarks such as HumanEval, and robustness to variations in repository structure and coding conventions. By assessing performance in these diverse contexts, we determine whether the training paradigm develops transferable reasoning capabilities or produces narrow, environment-specific adaptations.

\section{Methodology Overview}

This research employs a rigorous experimental methodology designed to systematically evaluate agent-in-the-loop reinforcement learning for automated program repair. Our approach combines theoretical insights from reinforcement learning with practical engineering solutions to create a comprehensive experimental framework.

The methodology centers on a two-stage training pipeline that progressively refines model capabilities. The first stage employs supervised fine-tuning (SFT) on curated datasets of high-quality bug fixes, establishing a foundation of code understanding and basic repair patterns. This stage ensures that models begin reinforcement learning with sufficient capability to generate meaningful actions within the environment. The second stage implements Group Relative Policy Optimization (GRPO), a variant of proximal policy optimization that forgoes value function estimation in favor of group-relative reward baselines. During this stage, agents actively interact with software repositories, attempting repairs and receiving rewards based on patch quality.

Our experimental design implements careful controls to ensure valid comparisons between different training paradigms and scaffold configurations. We maintain identical model architectures (Qwen3 family models chosen for their superior tool-calling abilities), training data sources, and computational budgets across all experimental conditions. The primary independent variables are the training methodology (supervised learning versus reinforcement learning) and scaffold complexity (minimalist versus feature-rich). Dependent variables include repair success rates, patch quality metrics, and generalization performance.

Data collection encompasses multiple complementary sources: detailed logs of agent-environment interactions capturing exploration patterns and decision-making processes, reward signals computed through automated comparison with ground-truth patches, and comprehensive evaluation metrics across diverse benchmarks. We employ both the SWE-Gym environment for training, which provides thousands of real-world Python bug-fixing tasks, and SWE-Bench-Verified for evaluation, ensuring robust assessment on human-verified bug fixes.

Statistical analysis includes significance testing for performance differences, learning curve analysis to understand training dynamics, and qualitative examination of agent behaviors to identify emergent strategies. This multi-faceted approach enables both quantitative validation of our hypotheses and deeper insights into the mechanisms driving performance improvements.

\section{Scope and Limitations}

This investigation operates within carefully defined boundaries that balance scientific rigor with practical constraints. Understanding these delimitations is essential for interpreting our findings and their applicability to broader contexts.

\textbf{Task Scope}: We focus specifically on automated bug repair tasks where a known error exists in the codebase and the goal is to generate a corrective patch. While we conduct limited evaluation on general code generation benchmarks to assess skill transfer, our primary emphasis remains on debugging and repair scenarios. This focus allows deep investigation of the debugging process while acknowledging that code generation from specifications represents a related but distinct challenge.

\textbf{Language and Environment Constraints}: Our primary experiments utilize Python repositories due to the availability of high-quality benchmarks and the language's popularity in open-source development. Cross-language generalization experiments include Java to assess transfer to statically typed languages, but we do not comprehensively evaluate all programming paradigms. We specifically exclude bugs requiring complex runtime environments, external service interactions, or hardware-specific behaviors, as these cannot be reliably simulated in our containerized training environment.

\textbf{Model Architecture Limitations}: While our approach is theoretically model-agnostic, computational constraints necessitate focusing on the Qwen3 model family (8B and 32B variants). These models were selected for their exceptional tool-calling capabilities and open availability, but our findings may not directly generalize to models with different architectural characteristics or training objectives. The reinforcement learning approach demands substantial computational resources for both training and inference, limiting the scope of hyperparameter exploration and model scaling experiments.

\textbf{Evaluation Protocol Boundaries}: Our reward function relies on automated patch comparison rather than full test suite execution, trading perfect functional correctness assessment for computational tractability. While this approach aligns with established benchmarks and enables large-scale experiments, it may occasionally miss functionally equivalent but syntactically different solutions. Additionally, our evaluation emphasizes single-commit bug fixes rather than complex multi-stage refactoring or architectural changes.

\textbf{Scaffold Implementation Constraints}: The two scaffold variants studied represent specific points in a continuous design space rather than exhaustive coverage of all possible configurations. The minimalist scaffold deliberately excludes many tools that could potentially assist debugging, while the feature-rich scaffold represents one particular implementation of advanced capabilities. Other scaffold designs might yield different learning dynamics and performance characteristics.

\section{Thesis Organization}

The remainder of this thesis is structured to provide a comprehensive treatment of agent-in-the-loop reinforcement learning for automated program repair, progressing from theoretical foundations through empirical validation.

\textbf{Chapter 2: Background and Theoretical Foundations} establishes the theoretical context for our work, reviewing reinforcement learning algorithms for language models with particular emphasis on Group Relative Policy Optimization. We examine the evolution of automated program repair approaches, from rule-based systems through modern neural methods, and analyze the emergence of tool-augmented language models. The chapter introduces key concepts including agent scaffolding, reward design for code repair, and the training-inference duality that characterizes our approach.

\textbf{Chapter 3: Methodology and System Design} presents our technical approach in detail, beginning with the agent-in-the-loop training paradigm and its implementation. We describe the architecture of both minimalist and feature-rich scaffolds, explaining design decisions and trade-offs. The chapter details our two-stage training pipeline, reward formulation, and the engineering innovations that enable efficient large-scale training. Particular attention is given to the integration of vLLM inference servers with reinforcement learning trainers and the NCCL-based weight synchronization system.

\textbf{Chapter 4: Related Work} contextualizes our contributions within the broader landscape of code generation and repair research. We analyze prior work on reinforcement learning for programming tasks, agent-based software engineering tools, and the evolution of benchmarks for code repair evaluation. This chapter highlights how our approach synthesizes insights from multiple research threads while addressing limitations in existing methods.

\textbf{Chapter 5: Experimental Results and Analysis} presents comprehensive empirical evaluation addressing our three research questions. We report performance metrics on SWE-Bench-Verified, analyze learning curves and training dynamics, and examine the differential effects of scaffold complexity. The chapter includes detailed ablation studies, error analysis, and investigation of transfer learning to Java debugging and general code generation tasks. Statistical significance testing and qualitative analysis of agent behaviors provide deeper insights into the mechanisms driving performance improvements.

\textbf{Chapter 6: Conclusions and Future Directions} synthesizes our findings, explicitly answering each research question based on empirical evidence. We discuss the broader implications for automated software engineering, identify limitations of the current approach, and outline promising directions for future research. The chapter concludes with reflections on the potential impact of agent-in-the-loop reinforcement learning on the future of software development tools.