\chapter{Introduction}
\label{ch:introduction}

Automated program repair has rapidly transformed from a nascent research area to one of the most promising applications of agentic \ac{AI} systems.
By ``agentic,'' we refer to systems that can autonomously interact with environments through tool use, make sequential decisions based on observations, and iteratively refine their approach based on feedback, as opposed to single-pass generation models that produce outputs without environmental interaction.
Recent advances in autonomous software engineering have achieved remarkable performance milestones, with systems like Claude~4 now exceeding 70\% success rates on SWE-Bench-Verified \cite{claude4_2025}.
This rapid progress underscores the critical importance of maintaining open research capable of understanding frontier capabilities.
A broad consensus has emerged among researchers: competitive access to training methodologies is essential to ensure that critical insights remain within the scientific community rather than concentrated in proprietary systems.
Recent open weight releases such as Kimi-K2~\cite{kimiK2_2025} and Qwen3-Coder~\cite{qwen3CoderBlog2025} follow closely behind frontier performance, but do not include full training recipes.
This reinforces the need for open, reproducible methods and analysis.

This thesis directly addresses this need by investigating how a single, minimalist agent called Nano enables a model to acquire agentic behavior through experiential \ac{RL} feedback occurring entirely within the agent's interactive loop.
By situating all learning and adaptation within the Nano agent, we provide clear, reproducible insights into the mechanisms by which \ac{RL} cultivates autonomy and effective tool use in repository-level code repair.

An important contemporaneous effort is SWE-RL~\cite{wei2025swerladvancingllmreasoning}, which applies \ac{GRPO} with patch-similarity rewards (using Python's \texttt{difflib.SequenceMatcher}) in a static, single-turn setting where full file contexts are provided, the model outputs a patch directly, and similarity to ground truth defines the reward signal.
Our work is complementary.
We also use execution-free patch-similarity rewards but combine them with \ac{GSPO}, a more robust sequence-level variant, in an agentic, multi-turn regime where the model must first acquire relevant context through tool use before proposing edits.
This distinction between static one-shot patch generation and interactive context acquisition motivates our focus on agent-integrated training within a single minimalist scaffold rather than prompt-time scaffold engineering.

The necessity and transformative potential of agentic training emerge from examining the historical limitations of supervised learning for code repair and the foundational role of \acp{LLM} in enabling \ac{RL} for complex tasks.

\section{Background}
\label{sec:intro-background}

Traditional systems for automated code repair have relied predominantly on supervised learning from static datasets of bug-fix pairs.
In this paradigm, models passively observe input-output mappings between broken and corrected code, treating bug fixing as a pattern recognition task rather than engaging with the full interactive software engineering process.
While computationally efficient, this approach fundamentally misrepresents the nature of repository-level software debugging, which involves iterative hypothesis refinement, strategic exploration of entire codebases, execution of diagnostic commands, and contextual interpretation of error signals across multiple rounds of investigation.
Moreover, the information needed to fix a bug in real-world repositories is rarely contained within the bug-fix pair itself, necessitating the interactive capabilities that coding agents provide to navigate and understand the broader codebase context.

The question becomes: how do we train such agentic systems when the very complexity that necessitates their existence also makes traditional training approaches intractable?
\ac{RL} has achieved remarkable results in well-structured domains like Go, where clear success metrics exist and every action represents a valid move within the game's rules \cite{alphaGo2016}.
However, \ac{RL} fundamentally requires that learning systems can meaningfully participate in the task from the beginning.
They must be capable of generating valid actions, interpreting feedback, and understanding the basic structure of the problem space.
For complex reasoning tasks like software debugging, this prerequisite participation threshold has historically been insurmountable.

\acp{LLM} have altered this fundamental constraint by providing the world knowledge and reasoning capabilities necessary to attempt tasks of unprecedented complexity.
When equipped with appropriate interfaces, these models can navigate file systems, interpret error messages, execute diagnostic commands, and generate syntactically valid code modifications.
This capability foundation transforms previously intractable problems into domains where \ac{RL} becomes feasible.
In effect, \acp{LLM} provide the first rung on the ladder that makes applying \ac{RL} to software engineering feasible.

However, unlike traditional \ac{RL} agents that directly produce discrete actions from observations (e.g., chess moves, joystick inputs), \acp{LLM} fundamentally operate as text generators.
They cannot directly perceive environments or execute actions; they require an intermediary translation layer that converts generated text into executable actions and translates environmental responses into textual observations.

This architectural distinction necessitates a different approach to \ac{RL}.
In this paradigm, a lightweight agent scaffold interprets tool calls from the \ac{LLM}, executes corresponding actions in the environment, and returns textual observations for processing.
The entire system combining the \ac{LLM} and scaffold forms the \ac{RL} agent, with learning occurring through this mediated interaction cycle.
This thesis instantiates exactly one such scaffold (Nano agent) and conducts all training strictly within this single-agent setting.

When equipped with appropriate scaffolds, \acp{LLM} can navigate file systems, execute terminal commands, interpret error messages, and generate syntactically correct code modifications.
This capability transformation makes it practical to apply \ac{RL} to real-world software engineering tasks, as models can now generate the rich interaction trajectories required for policy gradient estimation.
Terminal-oriented agency benchmarks such as TauBench~\cite{taubench2024} provide standardized evaluation frameworks for such capabilities.

Training adopts an execution-free approach where rewards derive from patch similarity rather than test execution, enabling language-agnostic training without maintaining test infrastructure for each programming language.
Concrete observation, action, and reward definitions are specified in \cref{ch:method}.

The significance of this paradigm shift extends beyond technical feasibility.
Human software developers do not repair bugs through single-shot pattern matching; they engage in exploratory processes involving hypothesis formation, incremental information gathering, and iterative refinement.
They navigate complex codebases, examine interconnected components, execute diagnostic commands, and progressively construct mental models of system behavior.
This active learning process differs fundamentally from passive pattern recognition, suggesting that training models through interactive experience may yield qualitatively superior debugging capabilities.

This direction aligns with widely observed advances at the frontier: agentic capabilities are clearly improving, with public reports of stronger coding performance and enhanced interactive capabilities.
These developments heighten the need for open, reproducible understanding of how agentic training works, rather than reliance on closed systems.
Our work addresses this need by studying \ac{RL} with interactive coding agents in an open setting.

\section{Problem Statement}
\label{sec:intro-problem}

The central problem is twofold: understanding how online \ac{RL} for multi-turn tool-using agents is done and making it feasible within academic compute constraints.
Frontier labs have likely addressed these challenges.
Systems like Anthropic's Claude Code and OpenAI's Codex demonstrate capabilities suggesting specialized agent training, but their methods remain proprietary.
The investment in purpose-built agent scaffolds indicates successful training approaches exist, yet without disclosed methodologies, the scientific community cannot replicate these results or understand how they were achieved.

Even when methodologies are partially disclosed, computational and operational demands place such training beyond most academic budgets, as pipelines involve dozens of interacting components built atop complex \ac{LLM} foundations.
This concentration of both knowledge and resources in proprietary labs creates fundamental asymmetries that undermine the open research ecosystem: without access to tractable training recipes, the scientific community cannot critically evaluate these approaches, replicate findings, or build upon established methods.
As online \ac{RL} for interactive agents emerges as a promising direction, bridging both the knowledge gap and the resource gap becomes essential for broad research participation.

We therefore develop a low-resource, execution-free \ac{RL} recipe for repository-level \ac{APR}.
The investigation addresses three research questions:

\textbf{RQ1: Does execution-free \ac{GSPO} training converge across diverse programming languages?
}
\begin{itemize}
	\item Primary indicators: overall reward progression and variance evolution across the 1{,}000-task curriculum (750 Python + 250 multilingual tasks); per-language reward improvements from epoch 1 to epoch 2 on identical problem instances across nine programming languages.
	\item Analysis: reward mean and standard deviation trends for convergence validation; epoch-over-epoch comparison per language to assess language-agnostic learning.
	\item Hypothesis: execution-free patch-similarity rewards provide sufficient signal for policy convergence despite not directly optimizing for functional correctness, and this convergence generalizes across diverse programming languages without language-specific engineering.
\end{itemize}

\textbf{RQ2: How does \ac{GSPO} training improve Nano agent scaffold adaptation?}
\begin{itemize}
	\item Primary indicators: tool-call success rate, action efficiency, and command usage evolution over training.
	\item Analysis: pre-training vs. post-training comparison of scaffold-level efficiency metrics and qualitative examination of changes in command usage patterns.
	\item Hypothesis: \ac{RL} training improves scaffold interaction efficiency and tool-use reliability, with observable shifts in command patterns toward more effective tool use.
\end{itemize}

\textbf{RQ3: Does execution-free patch-similarity \ac{RL} training improve SWE-Bench-Verified performance?}
\begin{itemize}
	\item Primary metric: test-verified success rate on SWE-Bench-Verified (approximately 500 Python debugging tasks).
	\item Comparison: pre-training baseline vs. post-training performance using identical evaluation protocol.
	\item Hypothesis: training on static patch-similarity rewards improves functional bug-fixing success despite not directly optimizing for test passage.
\end{itemize}

\section{Contributions}
\label{sec:intro-methodology}
This research makes several methodological and technical contributions to enable \ac{RL} training for coding agents.

We employ \ac{GSPO}~\cite{gspo2025}, a sequence-level policy optimization algorithm that extends the group-relative baseline principle with improved importance weighting for variable-length sequences.
\ac{GSPO} demonstrates superior robustness compared to earlier group-relative methods (\ac{GRPO}, DAPO), proving particularly effective for small effective batch sizes and high-variance rewards characteristic of multi-turn agent training under academic compute constraints.

Training proceeds through iterative online interaction: the agent explores repositories, generates repair episodes, and receives rewards based on static patch-similarity comparison between its repository modifications and ground-truth patches.

Primary comparisons report pre-training baseline vs.
post-training performance on SWE-Bench-Verified using the same Nano agent scaffold for both evaluations, eliminating confounds from supervised fine-tuning or prompt engineering.
Measured outcomes include repair success rates, reward progression, tool-call validity, action efficiency, and command usage patterns.

Data collection captures complete agent-environment interactions: tool invocations, repository states, reward signals computed via patch-similarity comparison, training dynamics, and aggregated evaluation metrics.
Training uses a 1,000-task curriculum (750 SWE-Gym Python tasks + 250 SWE-Bench-Multilingual tasks), with primary evaluation on SWE-Bench-Verified and multilingual generalization assessed through per-language reward progression across training epochs.

This methodology enables quantitative validation and mechanistic insights into how \ac{RL} shapes interactive debugging behavior through the Nano agent.

\section{Delimitations}
\label{sec:intro-delimitations}

This investigation operates within carefully defined boundaries that balance scientific rigor with practical constraints.
Understanding these delimitations is essential for interpreting our findings and their applicability to broader contexts.

\textbf{Task Scope}: This work addresses automated bug repair where known errors exist and the objective is patch generation.
Evaluation focuses on the Nano agent operating within its native scaffold, while broader code-generation benchmarks (HumanEval, MBPP) remain out of scope.
This focused investigation enables deep analysis of interactive debugging behavior while acknowledging that specification-based program synthesis constitutes orthogonal research.

\textbf{Language and Environment Constraints}: Training uses a 1,000-task curriculum (750 SWE-Gym Python issues + 250 SWE-Bench-Multilingual issues) curated to take advantage of our programming language agnostic approach.

\textbf{Compute Environment Constraints}: Experiments execute on Berzelius and Alvis Swedish national \ac{GPU} clusters using SLURM scheduling without Docker-based container orchestration support.
This infrastructure constraint shaped both Nano agent design and training pipeline architecture, favoring SLURM-compatible parallelization and process-level isolation over container-based approaches.

\textbf{Model Architecture Scope}: Our primary development and analysis focuses on Qwen3-14B—a hybrid reasoning model with strong tool-calling capabilities—with reasoning explicitly disabled to conserve context window budget in multi-turn episodes.
Limited comparisons with Qwen3-8B, Qwen3-32B, and Llama3.1-8B appear in the Appendix to illustrate capacity and architecture effects, but full model ablation studies are left for future work.

\textbf{Evaluation Protocol Boundaries}: Rewards derive from automated patch-similarity comparison rather than test suite execution, trading functional verification for computational tractability and reproducibility.
This approach aligns with established benchmarks and enables large-scale multilingual training, though it may undervalue functionally equivalent patches with syntactic divergence from ground truth.
Evaluation emphasizes single-commit bug repairs; multi-stage refactoring and architectural changes remain out of scope.

\textbf{Agent Implementation}: The Nano agent adopts a deliberately minimalist design, exposing only essential shell commands and file patching operations (two tools: \texttt{shell}, \texttt{apply\_patch}).
This simplicity enables clean experimental isolation of \ac{RL} effects, though richer tool sets might exhibit different performance-complexity trade-offs.

\textbf{Methodological Boundaries}: We briefly explored distillation from proprietary models via \ac{SFT} but abandoned this direction due to limited empirical gains and philosophical misalignment with open research objectives focused on training from environmental interaction rather than imitating closed systems.

\section{Outline}
\label{sec:intro-outline}

The remainder of this thesis progresses from theoretical foundations through empirical validation to conclusions:

\textbf{Chapter 2: Background and Related Work} reviews policy optimization algorithms for language models (\ac{PPO}, \ac{GRPO}, DAPO, \ac{GSPO}), situates \ac{APR} within repository-level evaluation paradigms, and contrasts agentless vs.
agentic approaches to motivate our execution-free, agent-integrated training methodology.

\textbf{Chapter 3: Method} formalizes the Nano agent (observation/action spaces, interaction loop, termination), specifies training data (SWE-Gym, SWE-Bench-Multilingual), defines the patch-similarity reward formulation, describes \ac{GSPO} policy optimization with masked loss, and outlines the evaluation protocol addressing RQ1-RQ3.

\textbf{Chapter 4: Infrastructure} documents the practical realization: vLLM-based serving with tool calling, live adapter synchronization via \ac{NCCL}, trainer extensions for multi-turn \ac{GSPO}, SLURM orchestration, compute-efficient infrastructure (\ac{LoRA}, DeepSpeed ZeRO, gradient checkpointing, BF16), and training protocol execution.

\textbf{Chapter 5: Results} presents experimental findings: training convergence across languages with policy gradient signal stability (RQ1), scaffold adaptation metrics and behavioral changes (RQ2), and SWE-Bench-Verified performance (RQ3).
The chapter concludes with a discussion interpreting the observed patterns and their implications.

\textbf{Chapter 6: Conclusions} summarizes contributions, discusses design philosophy and lessons learned (system complexity, reward misspecification, infrastructure challenges), examines broader implications for agentic \ac{AI}, identifies limitations, and proposes future research directions for \ac{RL}-trained coding agents.
