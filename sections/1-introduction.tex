\chapter{Introduction}
\label{ch:introduction}

Automated program repair has rapidly transformed from a nascent research area to one of the most promising applications of agentic \ac{AI} systems.
By ``agentic,'' we refer to systems that can autonomously interact with environments through tool use, make sequential decisions based on observations, and iteratively refine their approach based on feedback, as opposed to single-pass generation models that produce outputs without environmental interaction.
Recent advances in autonomous software engineering have achieved remarkable performance milestones, with systems like Claude~4 now exceeding 70\% success rates on SWE-Bench-Verified \cite{claude4_2025}.
This rapid progress underscores the critical importance of maintaining open research that can achieve near-parity understanding with frontier capabilities.
Across diverse perspectives, there emerges a common recognition that researchers must maintain competitive access to the tools and methodologies driving these advances, ensuring that critical insights remain accessible to the broader scientific community rather than concentrated within proprietary systems.
Recent open weight releases such as Kimi-K2~\cite{kimiK2_2025} and Qwen3-Coder~\cite{qwen3CoderBlog2025} follow closely behind frontier performance, but do not include full training recipes.
This reinforces the need for open, reproducible methods and analysis.
\todoinline{Verify and update SWE-Bench(-Verified) SOTA; cite system cards/technical reports for these claims (Claude 4/3.5, Qwen3-Coder, Kimi-K2) at submission.}

This thesis directly addresses this need by investigating how a single, minimalist agent—Nano—enables a model to acquire agentic behavior through experiential \ac{RL} feedback occurring entirely within the agent's interactive loop.
By situating all learning and adaptation within the Nano harness, we provide clear, reproducible insights into the mechanisms by which \ac{RL} cultivates autonomy and effective tool use in repository-level code repair.

An important contemporaneous effort is SWE-RL~\cite{wei2025swerladvancingllmreasoning}, which applies \ac{GRPO} with a rule-based reward based on patch similarity (Python's \texttt{difflib.SequenceMatcher}) to train models in a static, single-turn setting where the full file contexts are provided and the model directly outputs a patch that is compared to ground truth.
Our work is complementary: we also use \ac{GRPO} and a patch-similarity reward, but in an agentic, multi-step regime where the model must first acquire its own context through tool use before proposing edits.
This distinction (static one-shot patch generation versus interactive context acquisition and repair) motivates our focus on agent-integrated training with a single minimalist harness (the Nano agent), rather than on prompt-time scaffolding.

To understand why this shift to agentic training is both necessary and transformative, we must first examine the historical context and theoretical foundations that have led to this approach.

\section{Background}
\label{sec:intro-background}

Traditional systems for automated code repair have relied predominantly on supervised learning from static datasets of bug-fix pairs.
In this paradigm, models passively observe input-output mappings between broken and corrected code, treating bug fixing as a pattern recognition task rather than engaging with the full interactive software engineering process.
While computationally efficient, this approach fundamentally misrepresents the nature of repository-level software debugging, which involves iterative hypothesis refinement, strategic exploration of entire codebases, execution of diagnostic commands, and contextual interpretation of error signals across multiple rounds of investigation.
Moreover, the information needed to fix a bug in real-world repositories is rarely contained within the bug-fix pair itself, necessitating the interactive capabilities that coding agents provide to navigate and understand the broader codebase context.
\todoinline{which traditional paper to cite here}

The question becomes: how do we train such agentic systems when the very complexity that necessitates their existence also makes traditional training approaches intractable?
\ac{RL} has achieved remarkable results in well-structured domains like Go, where clear success metrics exist and every action represents a valid move within the game's rules \cite{alphaGo2016}.
However, \ac{RL} fundamentally requires that learning systems can meaningfully participate in the task from the beginning.
\todoinline{OR that we can heuristically reward them.
Not feasible for SWE.
}
They must be capable of generating valid actions, interpreting feedback, and understanding the basic structure of the problem space.
For complex reasoning tasks like software debugging, this prerequisite participation threshold has historically been insurmountable.

\acp{LLM} have altered this fundamental constraint by providing the world knowledge and reasoning capabilities necessary to attempt tasks of unprecedented complexity.
When equipped with appropriate interfaces, these models can navigate file systems, interpret error messages, execute diagnostic commands, and generate syntactically valid code modifications.
This capability foundation transforms previously intractable problems into domains where \ac{RL} becomes feasible.
In effect, \acp{LLM} provide the first rung on the ladder that makes applying \ac{RL} to software engineering feasible.

However, unlike traditional game playing \ac{RL} agents, which directly produce actions from observations (e.g., selecting chess moves or joystick inputs), \acp{LLM} fundamentally operate as token generators.
They cannot directly perceive environments or execute actions; instead, they require an intermediary translation layer that converts their generated tokens into executable actions and translates environmental responses back into text.

This critical distinction necessitates a fundamentally different approach to \ac{RL}.
In this paradigm, a lightweight harness interprets tool calls from the \ac{LLM}, executes these as actions in the environment, and returns textual observations that the \ac{LLM} can process.
The entire system (\ac{LLM} plus harness) forms the \ac{RL} agent, with learning occurring through this mediated interaction cycle.
In this thesis we instantiate exactly one such harness—the Nano agent—and conduct all training strictly within this single-agent setting.

When equipped with appropriate harnesses, \acp{LLM} can navigate file systems, execute terminal commands, interpret error messages, and generate syntactically correct code modifications.
This capability transformation makes it practical to apply \ac{RL} to real-world software engineering tasks, as models can now generate the rich interaction trajectories required for policy gradient estimation.
\todoinline{Add citations for terminal-oriented agency benchmarks (e.g., TauBench) and point readers to the cross-scaffold evaluation in Chapter~\ref{sec:rq3-scaffold}.}

In this thesis, training adopts an execution-free agentic setup: the agent does not run builds or tests during training.
Instead, it receives static, repository-local feedback obtained exclusively through shell commands, without executing tests or builds.
This trades direct functional signals for infrastructure simplicity and determinism, enabling scale and multilingual coverage while keeping optimization aligned with interaction quality (valid tool use, efficiency) and end-task success.
Concrete observation, action, and reward definitions are specified in \cref{ch:method}.

The significance of this paradigm shift extends beyond mere technical feasibility.
Human software developers do not repair bugs through single-shot pattern matching; rather, they engage in sophisticated exploratory processes involving hypothesis formation, incremental information gathering, and iterative solution refinement.
They navigate complex codebases, examine multiple interconnected components, execute diagnostic commands, and progressively construct mental models of system behavior.
This active learning process, fundamentally different from passive pattern recognition, suggests that training models through interactive experience could yield qualitatively superior debugging capabilities.

%The current landscape of automated program repair reflects an artificial fragmentation: separate models for Python, distinct approaches for Java, isolated systems for JavaScript.
%This language-specific isolation stems not from fundamental differences in debugging strategies but from the practical constraints of execution-based evaluation.
%Test-based rewards require language-specific test runners, build systems, package managers, and dependency resolution, creating insurmountable engineering barriers to unified training.
%Our execution-free approach dissolves these barriers, enabling the first practical path toward universal debugging agents that operate across language boundaries.

\todoinline{Consider expanding: Our execution-free problem setup has significantly lower engineering friction compared to test-based approaches.
No need for language-specific test runners, build systems, dependency management, or containerized execution environments.
This enables seamless multilingual training that would be prohibitively complex with execution-based rewards.
}

This direction aligns with widely observed advances at the frontier: agentic capabilities are clearly improving, with public reports of stronger coding performance and enhanced interactive capabilities.
These developments heighten the need for open, reproducible understanding of how agentic training works, rather than reliance on closed systems.
Our work addresses this need by studying \ac{RL} with interactive coding agents in an open setting.
\todoinline{Add citations to public system cards or technical reports where appropriate.}

\section{Problem}
\label{sec:intro-problem}

Tool-using coding agents already exist; what is largely missing in the open scientific community is an open, reproducible training pipeline to surface that agentic behavior.
Concretely, we target a minimalist, execution-free \ac{RL} recipe for repository-level \ac{APR} that (i) optimizes terminal interaction quality in live repositories using static, execution-free feedback, (ii) scales beyond a single language without bespoke test infrastructure or heavy dataset engineering, and (iii) demonstrates generalization from training suites to external evaluations and across base models.

We therefore frame execution-free \ac{RL} as a pragmatic compromise: by trading direct functional signals for deterministic, static feedback, it becomes feasible to run large-scale and multilingual experiments with tight control over variance and reproducibility.
The scientific question is whether such a recipe can adapt a single minimalist agent to repositories, reduce invalid actions, and improve action efficiency while transferring across datasets and model families.

\section{Purpose}
\label{sec:intro-purpose}

This thesis develops and evaluates an open, execution-free \ac{RL} training recipe for a single minimalist coding agent (Nano).
Our emphasis is on training practice and evidence, not on introducing a new scaffold or algorithm: we document a transparent procedure and analyze its effects on interaction behavior.
We show that static, repository-local rewards can improve agent interaction without executing tests or builds, and that the same recipe transfers across base models and from training suites to external benchmarks.
We prioritize methodological clarity and reproducibility: all configurations and harness interfaces are specified, and the approach is designed to scale to multilingual settings without bespoke per-language engineering.

\todoinline{Tighten scope/claims to match finalized results; add pointers to Sections/Figures once numbers are fixed.}

% Despite growing evidence that leading \ac{AI} labs employ experiential training—OpenAI's o1~\cite{openAI_o1_2024}, Anthropic's Claude~\cite{anthropic2024}, and Cognition's Devin~\cite{cognition2024} all exhibit behaviors suggesting such training—no open-source implementation has been available to the research community.

% By open-sourcing this infrastructure, we aim to democratize access to advanced training techniques previously available only to well-resourced industry labs, enabling broader research into agentic \ac{AI} systems.

% The significance extends beyond code repair: online training could transform how models learn any task requiring environmental interaction, from scientific experimentation to robotic control.
% This thesis demonstrates its viability in the well-scoped domain of automated debugging, paving the way for broader applications.

% Current research in this area is primarily conducted by industry labs (OpenAI, Anthropic, Cognition Labs) with limited open-source replication, creating a significant knowledge gap in the academic community.

\section{Goal}
\label{sec:intro-goal}

% This thesis makes several significant contributions to the field of automated program repair and \ac{RL} for code generation:

% \textbf{Novel Training Paradigm}: We introduce and implement \ac{RL} for code repair, wherein autonomous agents interact with real software environments during training.
% \todoinline{Recommend defining this approach once and dropping the term}
% This approach represents a fundamental departure from static supervised learning, enabling models to learn debugging strategies through direct experience.
% Our implementation demonstrates the feasibility of this paradigm at scale, handling the computational and engineering challenges of simultaneous training and environment interaction.

% \textbf{Nano Agent Implementation}: We develop a minimalist "Nano agent" that utilizes only essential terminal commands and file operations, demonstrating that sophisticated debugging behaviors can emerge from simple interfaces when combined with \ac{RL}.
% This clean-slate approach validates that effective code repair capabilities arise from learning rather than engineered complexity.

% \textbf{Technical Infrastructure}: We develop a comprehensive training infrastructure that enables efficient \ac{RL} with coding agents.
% This includes a custom integration layer between vLLM inference servers and \ac{RL} trainers, real-time weight synchronization via \ac{NCCL} for live policy updates, and optimizations enabling training of models up to 32B parameters with limited computational resources.
% The infrastructure supports both Group Relative Policy Optimization (\ac{GRPO})~\cite{grpo2024} and can be extended to other policy gradient methods.

% \textbf{Empirical Validation}: Through extensive experiments on the \ac{SWE-Gym} training environment and \ac{SWE-Bench-Verified} evaluation benchmark, we demonstrate that this \ac{RL} approach produces monotonic improvements in bug-fixing performance.
% This result, achieved in an open-source setting, represents significant progress toward practical autonomous debugging systems and validates the potential of experiential training approaches.

% \textbf{Open Science Contribution}: All code, training configurations, and evaluation protocols are released as open-source contributions to the research community.
% This includes the Nano agent implementation and the \ac{RL} training pipeline.

The overarching goal of this thesis is to develop and empirically validate an online \ac{RL} training paradigm for automated program repair.
%Through rigorous experimentation with our minimalist Nano agent, we demonstrate that models can learn effective debugging strategies through environmental interaction rather than passive observation of bug-fix pairs.
%Our comprehensive evaluation addresses harness adaptation, cross-model generalization, dataset transfer, and task generalization as outlined in our research questions above.

To make this investigation concrete, we organize the work around three learnings (RQ1-RQ3) that arise from building an open-source agentic model in a single minimalist harness (Nano).
\todoinline{Verify RQ numbering and titles are consistent with Chapters 3 and 5 before submission.}

\textbf{RQ1: How does GSPO training reshape Nano harness adaptation and SWE-Bench-Verified success across model sizes?}
\begin{itemize}
	\item Primary indicators: success rate uplift, tool success, invalid-call reduction, and action efficiency reported per Qwen3 size.
	\item Hypothesis: consistent behavioural improvements emerge across capacities, with qualitative differences captured in the model-size training curves.\todoinline{Ensure each size has comparable logging for tool metrics.}
\end{itemize}

\textbf{RQ2: Does the 750/250 multilingual curriculum improve held-out performance across languages?}
\begin{itemize}
	\item Core contribution: quantify reward deltas per language on the 50-task SWE-Bench-Multilingual holdout.\todoinline{Confirm language buckets and sample counts after final data export.}
	\item Evaluation: pre/post reward comparison with bootstrap confidence intervals and per-language breakdowns.
	\item Hypothesis: execution-free rewards deliver non-trivial gains beyond English-centric training.
\end{itemize}

\textbf{RQ3: To what extent do behaviours learned on Nano transfer to alternative scaffolds?}
\begin{itemize}
	\item Core contribution: evaluate zero-shot and limited-shot transfer on Mini-SWE-Agent-, Aider-, and OpenHands-style harnesses without retraining.\todoinline{Specify exact harness configurations once experiments finish.}
	\item Evaluation: measure success rate, tool validity, and scaffold-adherence metrics relative to Nano baselines.
	\item Hypothesis: training within a minimalist scaffold yields transferable routines that survive interface shifts.
\end{itemize}

With these research questions guiding our investigation, we now articulate the specific purpose and contributions of this thesis.

\section{Benefits, Ethics and Sustainability}
\label{sec:intro-benefits}

This work contributes to open research in a field dominated by closed frontier labs by releasing methods, code, and evaluation protocols in an open format.
All implementation details needed to reproduce results are documented, including configuration files, prompts, and harness interfaces.
We use standard datasets with clear licensing and provide exact commit hashes for benchmarks and dependencies.
To enable critical scrutiny, we specify evaluation scripts and random seeds, and we will archive artifacts to ensure long-term accessibility.
By reducing barriers to entry and facilitating independent replication, the project strengthens transparency and community-driven progress in automated program repair.

Ethical considerations arise from the potential misuse of agentic systems and the provenance of training data.
We restrict experiments to permissively licensed repositories and avoid exposing sensitive credentials or private data within environments.
We emphasize safe tooling (read/write-limited file access, sandboxed execution) and encourage downstream \todoinline{I believe we can better isolate each agent with some apptainer flags I found, decide if the final iteration of experiments will use those}

We closely monitor the amount of compute per experiment and report tokens processed and \ac{GPU}-hours to support reproducibility and minimize unnecessary energy use.
We adopt efficiency practices such as mixed-precision training, reuse of checkpoints and shared inference servers via training-inference duality, and targeted ablations to avoid redundant runs.

\section{Methodology}
\label{sec:intro-methodology}
This research employs a rigorous experimental methodology designed to systematically evaluate \ac{RL} for repository-level bug fixing.
% Our approach combines theoretical insights from \ac{RL} with practical engineering solutions to create a comprehensive experimental framework.

We employ \ac{GSPO}, a sequence-level variant of \ac{GRPO}~\cite{grpo2024} that pairs group-relative baselines with a KL regularizer to stabilize updates under the small effective batch sizes available on academic compute allotments.

During this stage, agents actively interact with software repositories, attempting repairs and receiving rewards based on patch similarity.

Our experimental design keeps training conditions constant across model sizes: each Qwen3 checkpoint (8B/14B/30B) and the Llama3.1-8B reference share identical data streams, optimization hyperparameters, and compute budgets.
The primary comparison reports pre/post SWE-Bench-Verified success rates after GSPO training; we do not rely on supervised fine-tuning baselines.
Dependent variables include repair success rate, reward trajectory, and harness-efficiency metrics.

Data collection encompasses detailed logs of agent-environment interactions capturing exploration patterns, reward signals computed via canonical diff comparison to ground-truth patches, and aggregated metrics on SWE-Bench-Verified plus a 50-sample SWE-Bench-Multilingual holdout for multilingual probing.
We train on a 1,000-task curriculum composed of 750 SWE-Gym Python bugs and 250 SWE-Bench-Multilingual samples, evaluate primary performance on SWE-Bench-Verified, probe multilingual transfer on the 50-task holdout, and test scaffold transfer on Mini-SWE-Agent- and Aider-style harnesses without additional fine-tuning.
\todoinline{Double-check the exact scaffold suite before freezing the draft.}

Statistical analysis includes significance testing for performance differences, learning curve analysis to understand training dynamics, and qualitative examination of agent behaviors to identify emergent strategies.

In coordination with the forthcoming conference submission on scaffold transfer, we reuse the protocol drafted there: Nano-trained checkpoints are replayed on Mini-SWE-Agent-, Aider-, and OpenHands-style harnesses to probe zero-shot and few-shot transfer characteristics.
\todoinline{Add citation once the conference paper is public and ensure alignment with Chapter~\ref{sec:rq3-scaffold}.}
This multi-faceted approach enables both quantitative validation of our hypotheses and deeper insights into the mechanisms driving performance improvements.

\section{Stakeholders}
\label{sec:intro-stakeholders}

Primary stakeholders include \ac{ML} and software engineering researchers interested in \ac{APR}, open-source maintainers whose repositories inform benchmarks and evaluation, and practitioners seeking reliable automated debugging tools.
Secondary stakeholders include infrastructure providers supporting large-scale training and serving, and the broader \ac{AI} community working on agentic systems.
We design our methods and releases to maximize utility for these groups while balancing privacy, security, and sustainability concerns.

\section{Delimitations}
\label{sec:intro-delimitations}

This investigation operates within carefully defined boundaries that balance scientific rigor with practical constraints.
Understanding these delimitations is essential for interpreting our findings and their applicability to broader contexts.

\textbf{Task Scope}: We focus specifically on automated bug repair tasks where a known error exists in the codebase and the goal is to generate a corrective patch.
We extend evaluation beyond Nano only to assess cross-scaffold transfer on Mini-SWE-Agent-, Aider-, and OpenHands-style harnesses (Chapter~\ref{sec:rq3-scaffold}), but do not explore broader code-generation benchmarks.
This focus enables deep investigation of debugging behaviour while acknowledging that specification-based synthesis remains outside scope.
\todoinline{Keep the task scope sentence aligned with the final evaluation lineup.}

\textbf{Language and Environment Constraints}: Training uses a 1,000-task curriculum (750 SWE-Gym Python issues + 250 SWE-Bench-Multilingual issues) curated to take advantage of our programming language agnostic approach.

\textbf{Compute Environment Constraints}: Experiments were conducted on the Berzelius and Alvis Swedish national \ac{GPU} clusters, which use SLURM scheduling and do not support horizontal scalability via Docker-based container orchestration.
This constraint informed the design of both the Nano agent and the training pipeline, leading us to adopt parallelization and isolation mechanisms compatible with SLURM scheduling.

\textbf{Model Architecture Scope}: Our primary development and analysis focuses on the Qwen3 model family (with reasoning disabled) because of their tool-calling reliability and open availability.
We deliberately avoid reasoning modes as they generate verbose outputs that exhaust token budgets in online training settings.
We also evaluate our approach on Llama3.1-8B alongside the Qwen3 series.

\textbf{Evaluation Protocol Boundaries}: Our reward function relies on automated patch comparison rather than full test suite execution, trading perfect functional correctness assessment for computational tractability.
While this approach aligns with established benchmarks and enables large-scale experiments, it may occasionally miss functionally equivalent but syntactically different solutions.
Additionally, our evaluation emphasizes single-commit bug fixes rather than complex multi-stage refactoring or architectural changes.

\textbf{Agent Implementation}: Our Nano agent represents a deliberately minimalist approach to agent design, providing only essential terminal commands and file operations.
While this simplicity enables clean experimental validation of the \ac{RL} approach, more sophisticated tool sets could potentially yield different performance characteristics.

\textbf{Methodological Boundaries}: We briefly explored distillation via \ac{SFT}, but did not pursue it further due to limited empirical benefit and because reliance on proprietary teacher models would run counter to the thesis objective of advancing open \ac{RL} training for agentic coding behaviour.

\section{Outline}
\label{sec:intro-outline}

The remainder of this thesis is structured to provide a comprehensive treatment of \ac{RL} for automated program repair, progressing from theoretical foundations through empirical validation.

\todoinline{fix these names}

\textbf{Chapter 2: Theoretical Background and Related Work} establishes the theoretical context for our work, reviewing \ac{RL} algorithms for language models (e.g., \ac{GRPO}/\acs{GSPO}) and situating \ac{APR} within repository-grounded, agentic evaluation.
It introduces the Nano agent in relation to broader agent ecosystems and motivates execution-free rewards.

\textbf{Chapter 3: Methodology and System Design} presents our technical approach in detail, beginning with the Nano agent interface and its implementation.
We describe design decisions supporting the minimalist approach, the training pipeline, reward formulation, and engineering choices that enable efficient large-scale training and evaluation.

\textbf{Chapter 4: The work} what was done.
\todoinline{write this shit}

\textbf{Chapter 5: Experimental Results and Analysis} presents comprehensive empirical evaluation addressing our research questions.
We report performance metrics on SWE-Bench-Verified, analyze learning curves and training dynamics, and examine the effectiveness of our approach.
The chapter includes detailed ablation studies, error analysis, and investigation of transfer to general code generation tasks.
Statistical significance testing and qualitative analysis of agent behaviors provide deeper insights into the mechanisms driving performance improvements.

\textbf{Chapter 6: Conclusions and Future Directions} synthesizes our findings, explicitly answering each research question based on empirical evidence.
We discuss the broader implications for automated software engineering, identify limitations of the current approach, and outline promising directions for future research.
The chapter concludes with reflections on the potential impact of applying \ac{RL} to \acp{LLM} and the future of software development tools.
